{"meta":{"title":"逸辰","subtitle":"","description":"","author":"逸辰","url":"https://yjy9899.github.io","root":"/"},"pages":[{"title":"categories","date":"2023-07-15T12:32:30.000Z","updated":"2023-07-15T12:32:57.516Z","comments":true,"path":"categories/index.html","permalink":"https://yichenfirst.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-07-15T12:18:01.000Z","updated":"2023-07-15T12:24:34.394Z","comments":true,"path":"tags/index.html","permalink":"https://yichenfirst.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式事务","slug":"分布式/分布式事务","date":"2023-09-01T08:53:49.259Z","updated":"2023-09-01T08:55:53.143Z","comments":true,"path":"2023/09/01/分布式/分布式事务/","link":"","permalink":"https://yichenfirst.github.io/2023/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"分布式事务分布式事务是解决在分布式系统中事务的一致性问题。 问题描述在一个电商系统中，订单模块和库存模块式分别部署的，用户下单后系统会创建订单并扣减库存。 在下单过程中，如果网络故障或系统故障，就会出现创建订单失败但库存扣减成功或订单创建成功库存扣减失败，这样就会破坏数据一致性。 而出现这个问题主要原因是数据库的事务只能保证当前机器的数据一致性，而在上面的例子中，订单服务与库存服务的数据库不相同，所以数据库的事务就无法保证这个下单场景的数据一致性。 二阶段提交二阶段提交是分布式事务的一种解决方案。 二阶段提交通过一个协调者来控制事务的提交。当一个服务的本地事务执行完毕（还是未提交状态）后，通知协调者。当所有的服务都执行完毕后，协调者回向所有服务发送Commit命令，服务收到命令后提交本地事务。 具体过程如下： prepare阶段：当需要执行分布式事务是，协调者回向所有服务发送prepare消息，服务开始执行本地事务，当执行完毕后（此时事务没有提交）给协调者发送ack消息 commit阶段：协调者收到所有服务的ack消息后，会发送commit消息，服务收到此消息后就会提交本地事务。 存在的问题： 同步阻塞：如服务A占有资源M，服务B也需要资源M，但是资源A一直没提交，B就一直无法获取资源M，导致阻塞 单点故障：依赖协调者，协调者挂了就会瘫痪 数据不一致：在prepare节点发送ack后，网络出现故障，没有收到commit消息，而且服务都正常提交，此时就会出现数据不一致的问题。 三阶段提交三阶段提交就是在二阶段提交的中间多出一个阶段。 具体过程如下： canCommit：协调者向服务发送canCommit消息，服务开始执行本地事务，完毕后给协调者发送ack消息 preCommit：协调者收到所有ack后，发送preCommit，服务再次确认。此时，服务已经知道其他服务的事务已经执行完毕，正在等待提交。 doCommit：发送doCommit消息，服务提交本地事务 优点： 引入超时机制，减少阻塞：如果超时，默认就会中断服务，但是如何是在第三阶段超时，服务会自动提交事务，因为在第二阶段服务已经知道其他服务的事务已经执行完毕。而二阶段无法实现。 可以搭建协调者集群：因为操作保证幂等性","categories":[{"name":"分布式","slug":"分布式","permalink":"https://yichenfirst.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[]},{"title":"Java线程模型","slug":"java/Java线程模型","date":"2023-07-28T13:59:18.661Z","updated":"2023-07-28T14:00:31.011Z","comments":true,"path":"2023/07/28/java/Java线程模型/","link":"","permalink":"https://yichenfirst.github.io/2023/07/28/java/Java%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Java线程模型在java中，我们平时说的线程指的是用户线程，位于用户空间。与用户线程相对的是内核线程，位于内核空间。 线程模型说的就是用户线程与内核线程的对应关系，常见的模型有一对一模型，一对多模型和多对多模型。 一对一模型 一对一模型（又叫内核级线程模型），顾名思义内核线程与用户线程一一对应，内核负责线程的调度。一对一模型在用户线程之间切换会涉及用户空间与内核空间的切换。 优点： 实现简单 缺点： 用户线程之间切换会导致用户空间和内核空间的切换，效率较低 用户线程与内核线程意义对应，所以创建用户线程时也会对应创建内核线程，会影响系统性能 Java中使用的就是一对一的线程模型 多对一模型 多对一线程模型，又叫作用户级线程模型，即多个用户线程对应到同一个内核线程上，线程的创建、调度、同步的所有细节全部由进程的用户空间线程库来处理。 优点： 用户线程的很多操作对内核来说都是透明的，不需要用户空间和内核空间的频繁切换，使线程的创建、调度、同步等非常快。 缺点： 由于多个用户线程对应到同一个内核线程，如果其中一个用户线程阻塞，那么其他用户线程也无法执行 内核并不知道用户空间有哪些线程，无法像内核线程一样实现完整的调度、优先级等； Python中的线程模型就是多对一模型。 多对多线程模型 多对多模型，又叫作两级线程模型，该种线程模型吸取前两种的优点切尽量规避了他们的缺点。 多对多模型，用户线程与内核线程是m:n(一般m&gt;=n)。 首先区别于多对一模型，多对对模型好中的一个进程可以与多个内核进程关联，于是用户进程可以绑定不同的内核线程，与一对一模型类似。 其次，区别于一对一模型，它的进程里的所有用户线程并不与内核线程一一绑定，而是可以动态绑定内核线程， 当某个内核线程因为其绑定的用户线程的阻塞操作被内核调度让出CPU时，其关联的进程中其余用户线程可以重新与其他内核线程绑定运行。 所以，多对多模型既不是多对一模型那种完全靠自己调度的也不是一对一模型完全靠操作系统调度的，而是中间态（自身调度与系统调度协同工作），因为这种模型的高度复杂性，操作系统内核开发者一般不会使用，所以更多时候是作为第三方库的形式出现。 优点： 兼具多对一模型的轻量； 由于对应了多个内核线程，则一个用户线程阻塞时，其他用户线程仍然可以执行； 由于对应了多个内核线程，则可以实现较完整的调度、优先级等； 缺点： 实现复杂 Go语言中的goroutine调度器就是采用的这种实现方案，在Go语言中一个进程可以启动成千上万个goroutine，这也是其出道以来就自带“高并发”光环的重要原因。","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"AQS详解","slug":"java/AQS","date":"2023-05-18T16:00:00.000Z","updated":"2023-07-28T14:00:31.005Z","comments":true,"path":"2023/05/19/java/AQS/","link":"","permalink":"https://yichenfirst.github.io/2023/05/19/java/AQS/","excerpt":"","text":"AQSAQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构建出应用广泛的大量的同步器，比如ReentrantLock，Semaphore，其他的如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等。 AQS核心思想AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列(虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系)。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点(Node)来实现锁的分配。 AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 1private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过procted类型的getState，setState，compareAndSetState进行操作 123456789101112//返回同步状态的当前值protected final int getState() { return state;} // 设置同步状态的值protected final void setState(int newState) { state = newState;}//原子地(CAS操作)将同步状态值设置为给定值update如果当前同步状态的值等于expect(期望值)protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} AQS 对资源的共享方式AQS定义两种资源共享方式 Exclusive(独占)：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share(共享)：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护(如获取资源失败入队/唤醒出队等)，AQS已经在上层已经帮我们实现好了。 AQS底层使用了模板方法模式AQS使用了模板方法模式，自定义同步器时需要重写下面几个AQS提供的模板方法: 12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS类中的其他方法都是final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0(即释放锁)为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的(state会累加)，这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 AbstractQueuedSynchronizer数据结构AbstractQueuedSynchronizer类底层的数据结构是使用CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列(虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系)。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点(Node)来实现锁的分配。其中Sync queue，即同步队列，是双向链表，包括head结点和tail结点，head结点主要用作后续的调度。而Condition queue不是必须的，其是一个单向链表，只有当使用Condition时，才会存在此单向链表。并且可能会有多个Condition queue。","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"new与make关键字的区别","slug":"go/new与make关键字的区别","date":"2023-05-02T16:00:00.000Z","updated":"2023-07-23T12:35:36.003Z","comments":true,"path":"2023/05/03/go/new与make关键字的区别/","link":"","permalink":"https://yichenfirst.github.io/2023/05/03/go/new%E4%B8%8Emake%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"newnew是一个内置函数，它的作用是分配一块内存，并返回指向该内存的指针。 函数定义如下： 1234// The new built-in function allocates memory. The first argument is a type,// not a value, and the value returned is a pointer to a newly// allocated zero value of that type.func new(Type) *Type 从上面的代码可以看出，new 函数只接受一个参数，这个参数是一个类型，并且返回一个指向该类型内存地址的指针。 同时 new 函数会把分配的内存置为零，也就是类型的零值。 123p := new(int)fmt.Printf(\"p address %#v\\n\", p) // (*int)(0xc000016060)fmt.Printf(\"p %#v\", *p) // 0 new(int) 将分配的空间初始化为 int 的零值，也就是 0，并返回 int 的指针，这和直接声明指针并初始化的效果是相同的。 当然，new 函数不仅能够为系统默认的数据类型分配空间，自定义类型也可以使用 new 函数来分配空间，如下所示： 12345678type person struct{ name string age int}var p *personp = new(person)p.name = \"yichen\"fmt.Printf(p) // &amp;{yichen 0} 这就是 new 函数，它返回的永远是类型的指针，指针指向分配类型的内存地址。需要注意的是，new 函数只会分配内存空间，但并不会初始化该内存空间。 makemake 也是用于内存分配的，但是和 new 不同，它只用于 slice、map 和 chan 的内存创建，而且它返回的类型就是这三个类型本身，而不是他们的指针类型。因为这三种类型本身就是引用类型，所以就没有必要返回他们的指针了。 函数定义如下： 1234567891011121314151617// The make built-in function allocates and initializes an object of type// slice, map, or chan (only). Like new, the first argument is a type, not a// value. Unlike new, make's return type is the same as the type of its// argument, not a pointer to it. The specification of the result depends on// the type:// Slice: The size specifies the length. The capacity of the slice is// equal to its length. A second integer argument may be provided to// specify a different capacity; it must be no smaller than the// length, so make([]int, 0, 10) allocates a slice of length 0 and// capacity 10.// Map: An empty map is allocated with enough space to hold the// specified number of elements. The size may be omitted, in which case// a small starting size is allocated.// Channel: The channel's buffer is initialized with the specified// buffer capacity. If zero, or the size is omitted, the channel is// unbuffered.func make(t Type, size ...IntegerType) Type 通过上面的代码可以看出 make 函数的 t 参数必须是 slice、map 和 chan 中的一个，并且返回值也是类型本身 下面使用slice举例： 1234567891011var s1 []intif s1 == nil { fmt.Printf(\"s1 is nil --&gt; %#v \\n \", s1) // []int(nil)}s2 := make([]int, 3)if s2 == nil { fmt.Printf(\"s2 is nil --&gt; %#v \\n \", s2)} else { fmt.Printf(\"s2 is not nill --&gt; %#v \\n \", s2)// []int{0, 0, 0}} slice 的零值是 nil，但使用 make 初始化之后，slice 内容被类型 int 的零值填充，如：[]int{0, 0, 0}。 总结通过以上分析，总结一下 new 和 make 主要区别如下： make 只能用来分配及初始化类型为 slice、map 和 chan 的数据。new 可以分配任意类型的数据； new 分配返回的是指针，即类型 *Type。make 返回类型本身，即 Type； new 分配的空间被清零。make 分配空间后，会进行初始化；","categories":[{"name":"go","slug":"go","permalink":"https://yichenfirst.github.io/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://yichenfirst.github.io/tags/go/"}]},{"title":"CAS","slug":"java/CAS","date":"2023-04-14T16:00:00.000Z","updated":"2023-07-30T11:31:45.610Z","comments":true,"path":"2023/04/15/java/CAS/","link":"","permalink":"https://yichenfirst.github.io/2023/04/15/java/CAS/","excerpt":"","text":"CAS简介CAS全称是Compare And Swap，翻译过来就是比较和交换，也就是CAS中包含比较和交换这两个动作，在CPU中，为这两个动作专门提供了一个指令，就是CAH指令，有CPU来保证这两个操作的原子性，即比较和交换要么全部成功，要么全部失败。 CAS机制中使用了三个操作数，内存地址，预期值，要修改的值。 在执行CAS指令时，首先会比较获取的值与预期值是否相同，相同则修改成新值；若另外的线程修改了变量的值，导致与预期值不同，CAS操作就会失败。 这里我们可以看一下Java原子类AtomicInteger.getAndIncrement()的实现。 123public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1);} 接着看一下unsafe.getAndAddInt()的实现： 123456789public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;} 这里我们可以看到AtomicInteger.getAndIncrement()就是通过CAS操作实现的，在期望值与真实值相同的情况下，CAS操作才会执行成功；否则会一直循环，知道成功。 存在的问题ABA问题ABA问题是指在CAS操作时，其他线程将变量值A改为了B，但是又被改回了A，等到本线程使用期望值A与当前变量进行比较时，发现变量A没有变，于是CAS就将A值进行了交换操作，但是实际上该值已经被其他线程改变过，这与乐观锁的设计思想不符合。ABA问题的解决思路是，每次变量更新的时候把变量的版本号加1，那么A-B-A就会变成A1-B2-A3，只要变量被某一线程修改过，改变量对应的版本号就会发生递增变化，从而解决了ABA问题。在JDK的java.util.concurrent.atomic包中提供了AtomicStampedReference来解决ABA问题，该类的compareAndSet是该类的核心方法，实现如下： 123456789101112131415public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair&lt;V&gt; current = pair; return // 预期值对比 expectedReference == current.reference &amp;&amp; // 预期版本号对比 expectedStamp == current.stamp &amp;&amp; // 期望值对比，期望值版本号对比 ((newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)));} 1234567891011private static class Pair&lt;T&gt; { final T reference; final int stamp; private Pair(T reference, int stamp) { this.reference = reference; this.stamp = stamp; } static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) { return new Pair&lt;T&gt;(reference, stamp); }} 我们可以发现，该类检查了当前引用与当前标志是否与预期相同，如果全部相等，才会以原子方式将该引用和该标志的值设为新的更新值，这样CAS操作中的比较就不依赖于变量的值了。 自旋锁消耗问题多个线程争夺同一个资源时，如果自旋一直不成功，将会一直占用CPU。在Unsafe类中，如果有多个线程进入，只有一个线程能成功CAS，其他线程都失败。失败的线程会重复进行下一轮的CAS，但是下一轮还是只有一个线程成功。 12345678public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;} 解决方法：破坏掉for死循环，当超过一定时间或者一定次数时，return退出。JDK8新增的LongAdder,和ConcurrentHashMap类似的方法。当多个线程竞争时，将粒度变小，将一个变量拆分为多个变量，达到多个线程访问多个资源的效果，最后再调用sum把它合起来。 虽然base和cells都是volatile修饰的，但感觉这个sum操作没有加锁，可能sum的结果不是那么精确。 1234567891011public long sum() { Cell[] as = cells; Cell a; long sum = base; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } 多变量共享一致性问题CAS的原子操作只能针对一个共享变量，假如需要针对多个变量进行原子操作也是可以解决的。 方法：CAS操作是针对一个变量的，如果对多个变量操作，1. 可以加锁来解决。2 .封装成对象类解决","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"redis通信协议","slug":"redis/redis通信协议","date":"2023-03-31T16:00:00.000Z","updated":"2023-07-17T13:41:27.724Z","comments":true,"path":"2023/04/01/redis/redis通信协议/","link":"","permalink":"https://yichenfirst.github.io/2023/04/01/redis/redis%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"Redis通信协议Redis是一个CS架构的软件，通信一般分两步（不包括pipeline和PubSub）： 客户端（client）向服务端（server）发送一条命令 服务端解析并执行命令，返回响应结果给客户端 因此客户端发送命令的格式、服务端响应结果的格式必须有一个规范，这个规范就是通信协议。而在Redis中采用的是RESP（Redis Serialization Protocol）协议： Redis 1.2版本引入了RESP协议 Redis 2.0版本中成为与Redis服务端通信的标准，称为RESP2 Redis 6.0版本中，从RESP2升级到了RESP3协议，增加了更多数据类型并且支持6.0的新特性–客户端缓存 但目前，默认使用的依然是RESP2协议，也是我们要学习的协议版本（以下简称RESP）。在RESP中，通过首字节的字符来区分不同数据类型，常用的数据类型包括5种： 单行字符串：首字节是 ‘+’ ，后面跟上单行字符串，以CRLF（ “\\r\\n” ）结尾。例如返回”OK”： “+OK\\r\\n” 错误（Errors）：首字节是 ‘-’ ，与单行字符串格式一样，只是字符串是异常信息，例如：”-Error message\\r\\n” 数值：首字节是 ‘:’ ，后面跟上数字格式的字符串，以CRLF结尾。例如：”:10\\r\\n” 多行字符串：首字节是 ‘$’ ，表示二进制安全的字符串，最大支持512MB： 如果大小为0，则代表空字符串：”$0\\r\\n\\r\\n” 如果大小为-1，则代表不存在：”$-1\\r\\n” 数组：首字节是 ‘*’，后面跟上数组元素个数，再跟上元素，元素数据类型不限: 基于Socket自定义redis客户端Redis支持TCP通信，因此我们可以使用Socket来模拟客户端，与Redis服务端建立连接： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public class Main { static Socket s; static PrintWriter writer; static BufferedReader reader; public static void main(String[] args) { try { // 1.建立连接 String host = \"192.168.150.101\"; int port = 6379; s = new Socket(host, port); // 2.获取输出流、输入流 writer = new PrintWriter(new OutputStreamWriter(s.getOutputStream(), StandardCharsets.UTF_8)); reader = new BufferedReader(new InputStreamReader(s.getInputStream(), StandardCharsets.UTF_8)); // 3.发出请求 // 3.1.获取授权 auth 123321 sendRequest(\"auth\", \"123321\"); Object obj = handleResponse(); System.out.println(\"obj = \" + obj); // 3.2.set name 虎哥 sendRequest(\"set\", \"name\", \"虎哥\"); // 4.解析响应 obj = handleResponse(); System.out.println(\"obj = \" + obj); // 3.2.set name 虎哥 sendRequest(\"get\", \"name\"); // 4.解析响应 obj = handleResponse(); System.out.println(\"obj = \" + obj); // 3.2.set name 虎哥 sendRequest(\"mget\", \"name\", \"num\", \"msg\"); // 4.解析响应 obj = handleResponse(); System.out.println(\"obj = \" + obj); } catch (IOException e) { e.printStackTrace(); } finally { // 5.释放连接 try { if (reader != null) reader.close(); if (writer != null) writer.close(); if (s != null) s.close(); } catch (IOException e) { e.printStackTrace(); } } } private static Object handleResponse() throws IOException { // 读取首字节 int prefix = reader.read(); // 判断数据类型标示 switch (prefix) { case '+': // 单行字符串，直接读一行 return reader.readLine(); case '-': // 异常，也读一行 throw new RuntimeException(reader.readLine()); case ':': // 数字 return Long.parseLong(reader.readLine()); case '$': // 多行字符串 // 先读长度 int len = Integer.parseInt(reader.readLine()); if (len == -1) { return null; } if (len == 0) { return \"\"; } // 再读数据,读len个字节。我们假设没有特殊字符，所以读一行（简化） return reader.readLine(); case '*': return readBulkString(); default: throw new RuntimeException(\"错误的数据格式！\"); } } private static Object readBulkString() throws IOException { // 获取数组大小 int len = Integer.parseInt(reader.readLine()); if (len &lt;= 0) { return null; } // 定义集合，接收多个元素 List&lt;Object&gt; list = new ArrayList&lt;&gt;(len); // 遍历，依次读取每个元素 for (int i = 0; i &lt; len; i++) { list.add(handleResponse()); } return list; } // set name 虎哥 private static void sendRequest(String ... args) { writer.println(\"*\" + args.length); for (String arg : args) { writer.println(\"$\" + arg.getBytes(StandardCharsets.UTF_8).length); writer.println(arg); } writer.flush(); }}","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"redis内存回收","slug":"redis/redis内存回收","date":"2023-03-24T16:00:00.000Z","updated":"2023-07-17T13:41:27.724Z","comments":true,"path":"2023/03/25/redis/redis内存回收/","link":"","permalink":"https://yichenfirst.github.io/2023/03/25/redis/redis%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/","excerpt":"","text":"Redis内存回收过期key处理Redis之所以性能强，最主要的原因就是基于内存存储。然而单节点的Redis其内存大小不宜过大，会影响持久化或主从同步性能。我们可以通过修改配置文件来设置Redis的最大内存： 当内存使用达到上限时，就无法存储更多数据了。为了解决这个问题，Redis提供了一些策略实现内存回收： 内存过期策略 在学习Redis缓存的时候我们说过，可以通过expire命令给Redis的key设置TTL（存活时间）： 可以发现，当key的TTL到期以后，再次访问name返回的是nil，说明这个key已经不存在了，对应的内存也得到释放。从而起到内存回收的目的。 Redis本身是一个典型的key-value内存存储数据库，因此所有的key、value都保存在之前学习过的Dict结构中。不过在其database结构体中，有两个Dict：一个用来记录key-value；另一个用来记录key-TTL。 这里有两个问题需要我们思考：Redis是如何知道一个key是否过期呢？ 利用两个Dict分别记录key-value对及key-ttl对 是不是TTL到期就立即删除了呢？ 惰性删除 惰性删除：顾明思议并不是在TTL到期后就立刻删除，而是在访问一个key的时候，检查该key的存活时间，如果已经过期才执行删除。 周期删除 周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种：Redis服务初始化函数initServer()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为SLOWRedis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST 周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。执行周期有两种：Redis服务初始化函数initServer()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为SLOWRedis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST SLOW模式规则： 执行频率受server.hz影响，默认为10，即每秒执行10次，每个执行周期100ms。 执行清理耗时不超过一次执行周期的25%.默认slow模式耗时不超过25ms 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期 如果没达到时间上限（25ms）并且过期key比例大于10%，再进行一次抽样，否则结束 FAST模式规则（过期key比例小于10%不执行 ）： 执行频率受beforeSleep()调用频率影响，但两次FAST模式间隔不低于2ms 执行清理耗时不超过1ms 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期如果没达到时间上限（1ms）并且过期key比例大于10%，再进行一次抽样，否则结束 小总结： RedisKey的TTL记录方式： 在RedisDB中通过一个Dict记录每个Key的TTL时间 过期key的删除策略： 惰性清理：每次查找key时判断是否过期，如果过期则删除 定期清理：定期抽样部分key，判断是否过期，如果过期则删除。定期清理的两种模式： SLOW模式执行频率默认为10，每次不超过25ms FAST模式执行频率不固定，但两次间隔不低于2ms，每次耗时不超过1ms 内存淘汰策略内存淘汰：就是当Redis内存使用达到设置的上限时，主动挑选部分key删除以释放更多内存的流程。Redis会在处理客户端命令的方法processCommand()中尝试做内存淘汰： 淘汰策略 Redis支持8种不同策略来选择要删除的key： noeviction： 不淘汰任何key，但是内存满时不允许写入新数据，默认就是这种策略。 volatile-ttl： 对设置了TTL的key，比较key的剩余TTL值，TTL越小越先被淘汰 allkeys-random：对全体key ，随机进行淘汰。也就是直接从db-&gt;dict中随机挑选 volatile-random：对设置了TTL的key ，随机进行淘汰。也就是从db-&gt;expires中随机挑选。 allkeys-lru： 对全体key，基于LRU算法进行淘汰 volatile-lru： 对设置了TTL的key，基于LRU算法进行淘汰 allkeys-lfu： 对全体key，基于LFU算法进行淘汰 volatile-lfu： 对设置了TTL的key，基于LFI算法进行淘汰比较容易混淆的有两个： LRU（Least Recently Used），最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。 LFU（Least Frequently Used），最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高。 Redis的数据都会被封装为RedisObject结构： LFU的访问次数之所以叫做逻辑访问次数，是因为并不是每次key被访问都计数，而是通过运算： 生成0~1之间的随机数R 计算 (旧次数 * lfu_log_factor + 1)，记录为P 如果 R &lt; P ，则计数器 + 1，且最大不超过255 访问次数会随时间衰减，距离上一次访问时间每隔 lfu_decay_time 分钟，计数器 -1 最后用一副图来描述当前的这个流程吧","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"redis底层数据结构","slug":"redis/redis底层数据结构","date":"2023-03-19T16:00:00.000Z","updated":"2023-07-17T13:41:27.774Z","comments":true,"path":"2023/03/20/redis/redis底层数据结构/","link":"","permalink":"https://yichenfirst.github.io/2023/03/20/redis/redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"redis底层数据结构动态字符串（SDS）字符串是redis中最常用的一种数据结构，不过Redis没有直接使用C语言中的字符串，因为C语言字符存在很多问题，比如： 获取字符串长度需要运算 非二进制安全（以”\\0”作为结束标识） 不可修改 Redis构建了一种新的字符串结构，称为简单动态字符串（Simple Dynamic String），简称SDS。 Redis是C语言实现的，其中SDS是一个结构体，源码如下： 例如，一个包含字符串”name”的sds结构如下： sds之所以叫做动态字符串，是因为它具备动态扩容的能力，例如一个内容为”hi”的sds： 假如我们要给SDS追加一段字符串“,Amy”，这里首先会申请新内存空间： 如果新字符串小于1M，则新空间为扩展后字符串长度的两倍+1； 如果新字符串大于1M，则新空间为扩展后字符串长度+1M+1。称为内存预分配。 sds优点： 获取字符串长度的时间复杂度为O(1) 支持动态扩容 减少内存分配次数 IntSetIntSet是Redis中set集合的一种实现方式，基于整数数组来实现，并且具备长度可变、有序等特征。 结构如下： 其中的encoding包含三种模式，表示存储的整数大小不同： 为了方便查找，Redis会将intset中所有的整数按照升序依次保存在contents数组中，结构如图： 现在，数组中每个数字都在int16_t的范围内，因此采用的编码方式是INTSET_ENC_INT16，每部分占用的字节大小为： encoding：4字节 length：4字节 contents：2字节 * 3 = 6字节 现在，假设有一个intset，元素为{5,10，20}，采用的编码是INTSET_ENC_INT16，则每个整数占2字节： 我们向该其中添加一个数字：50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。 以当前案例来说流程如下： ①升级编码为INTSET_ENC_INT32, 每个整数占4字节，并按照新的编码方式及元素个数扩容数组 ②倒序依次将数组中的元素拷贝到扩容后的正确位置 ③将待添加的元素放入数组末尾 ④最后，将inset的encoding属性改为INTSET_ENC_INT32，将length属性改为4 总结： IntSet可以看做是特殊的整数数据，具备一下特点： Redis会确保IntSet中的元素唯一、有序 具备类型升级，可以节省内存空间 底层采用二分查找方式来查询 Dict我们知道Redis是一个键值型（Key-Value Pair）的数据库，我们可以根据键实现快速的增删改查。而键与值的映射关系正是通过Dict来实现的。 Dict由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict） 当我们向Dict添加键值对时，Redis首先根据key计算出hash值（h），然后利用 h &amp; sizemask来计算元素应该存储到数组中的哪个索引位置。我们存储k1=v1，假设k1的哈希值h =1，则1&amp;3 =1，因此k1=v1要存储到数组角标1位置。 现在有一个键值对k2=v2经过hash运算后，也要存到数据下标为1的位置，此时就发生了冲突，如下图，使用头插法。 上面介绍了Dict的哈希表（DictHashTable）和哈希节点（DictEntry），剩下的字典（Dict）结构如下： Dict的扩容 Dict中的HashTable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。 Dict在每次新增键值对时都会检查负载因子（LoadFactor = used/size） ，满足以下两种情况时会触发哈希表扩容： 哈希表的 LoadFactor &gt;= 1，并且服务器没有执行 BGSAVE 或者 BGREWRITEAOF 等后台进程； 哈希表的 LoadFactor &gt; 5 ； Dict的收缩 Dict除了扩容以外，每次删除元素时，也会对负载因子做检查，当LoadFactor &lt; 0.1 时，会做哈希表收缩： Dict的rehash 不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的： ①计算新hash表的realeSize，值取决于当前要做的是扩容还是收缩： 如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的2^n 如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n （不得小于4） ②按照新的realeSize申请内存空间，创建dictht，并赋值给dict.ht[1] ③设置dict.rehashidx = 0，标示开始rehash ④将dict.ht[0]中的每一个dictEntry都rehash到dict.ht[1] ⑤将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存 Dict的渐进式rehash Dict的rehash并不是一次性完成的。试想一下，如果Dict中包含数百万的entry，要在一次rehash完成，极有可能导致主线程阻塞。因此Dict的rehash是分多次、渐进式的完成，因此称为渐进式rehash。流程如下： ①计算新hash表的size，值取决于当前要做的是扩容还是收缩： 如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的$2^n$ 如果是收缩，则新size为第一个大于等于dict.ht[0].used的$2^n $（不得小于4） ②按照新的size申请内存空间，创建dictht，并赋值给dict.ht[1] ③设置dict.rehashidx = 0，标示开始rehash ④每次执行新增、查询、修改、删除操作时，都检查一下dict.rehashidx是否大于-1，如果是则将dict.ht[0].table[rehashidx]的entry链表rehash到dict.ht[1]，并且将rehashidx++。直至dict.ht[0]的所有数据都rehash到dict.ht[1] ⑤将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存 ⑥将rehashidx赋值为-1，代表rehash结束 ⑦在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空 总结： Dict的结构： 类似java的HashTable，底层是数组加链表来解决哈希冲突 Dict包含两个哈希表，ht[0]平常用，ht[1]用来rehash Dict的伸缩： 当LoadFactor大于5或者LoadFactor大于1并且没有子进程任务时，Dict扩容 当LoadFactor小于0.1时，Dict收缩 扩容大小为第一个大于等于used + 1的$2^n$ 收缩大小为第一个大于等于used 的$2^n$ Dict采用渐进式rehash，每次访问Dict时执行一次rehash rehash时ht[0]只减不增，新增操作只在ht[1]执行，其它操作在两个哈希表 ZipList ZipList 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成。可以在任意一端进行压入/弹出操作, 并且该操作的时间复杂度为 O(1)。 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量。 最大值为UINT16_MAX （65534），如果超过这个值，此处会记录为65535，但节点的真实数量需要遍历整个压缩列表才能计算得出。 entry 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 ZipListEntry ZipList 中的Entry并不像普通链表那样记录前后节点的指针，因为记录两个指针要占用16个字节，浪费内存。而是采用了下面的结构： previous_entry_length：前一节点的长度，占1个或5个字节。 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据 encoding：编码属性，记录content的数据类型（字符串还是整数）以及长度，占用1个、2个或5个字节 contents：负责保存节点的数据，可以是字符串或整数 ZipList中所有存储长度的数值均采用小端字节序，即低位字节在前，高位字节在后。例如：数值0x1234，采用小端字节序后实际存储值为：0x3412 Encoding编码 ZipListEntry中的encoding编码分为字符串和整数两种：字符串：如果encoding是以“00”、“01”或者“10”开头，则证明content是字符串 编码 编码长度 字符串大小 |00pppppp| 1 bytes &lt;= 63 bytes |01pppppp|qqqqqqqq| 2 bytes &lt;= 16383 bytes |10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| 5 bytes &lt;= 4294967295 bytes 例如，我们要保存字符串：“ab”和 “bc” ZipListEntry中的encoding编码分为字符串和整数两种： 整数：如果encoding是以“11”开始，则证明content是整数，且encoding固定只占用1个字节 编码 编码长度 整数类型 11000000 1 int16_t（2 bytes） 11010000 1 int32_t（4 bytes） 11100000 1 int64_t（8 bytes） 11110000 1 24位有符整数(3 bytes) 11111110 1 8位有符整数(1 bytes) 1111xxxx 1 直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值 ZipList的连锁更新问题 ZipList的每个Entry都包含previous_entry_length来记录上一个节点的大小，长度是1个或5个字节：如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值如果前一节点的长度大于等于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据现在，假设我们有N个连续的、长度为250~253字节之间的entry，因此entry的previous_entry_length属性用1个字节即可表示，如图所示： ZipList这种特殊情况下产生的连续多次空间扩展操作称之为连锁更新（Cascade Update）。新增、删除都可能导致连锁更新的发生。 ZipList特性： 压缩列表的可以看做一种连续内存空间的”双向链表” 列表的节点之间不是通过指针连接，而是记录上一节点和本节点长度来寻址，内存占用较低 如果列表数据过多，导致链表过长，可能影响查询性能 增或删较大数据时有可能发生连续更新问题 QuickList问题1：ZipList虽然节省内存，但申请内存必须是连续空间，如果内存占用较多，申请内存效率很低。怎么办？ ​ 答：为了缓解这个问题，我们必须限制ZipList的长度和entry大小。 问题2：但是我们要存储大量数据，超出了ZipList最佳的上限该怎么办？ ​ 答：我们可以创建多个ZipList来分片存储数据。 问题3：数据拆分后比较分散，不方便管理和查找，这多个ZipList如何建立联系？ ​ 答：Redis在3.2版本引入了新的数据结构QuickList，它是一个双端链表，只不过链表中的每个节点都是一个ZipList。 为了避免QuickList中的每个ZipList中entry过多，Redis提供了一个配置项：list-max-ziplist-size来限制。如果值为正，则代表ZipList的允许的entry个数的最大值如果值为负，则代表ZipList的最大内存大小，分5种情况： -1：每个ZipList的内存占用不能超过4kb -2：每个ZipList的内存占用不能超过8kb -3：每个ZipList的内存占用不能超过16kb -4：每个ZipList的内存占用不能超过32kb -5：每个ZipList的内存占用不能超过64kb 其默认值为 -2： 以下是QuickList的和QuickListNode的结构源码： 我们接下来用一段流程图来描述当前的这个结构 总结： QuickList的特点： 是一个节点为ZipList的双端链表 节点采用ZipList，解决了传统链表的内存占用问题 控制了ZipList大小，解决连续内存空间申请效率问题 中间节点可以压缩，进一步节省了内存 SkipListSkipList（跳表）首先是链表，但与传统链表相比有几点差异：元素按照升序排列存储节点可能包含多个指针，指针跨度不同。 SkipList（跳表）首先是链表，但与传统链表相比有几点差异：元素按照升序排列存储节点可能包含多个指针，指针跨度不同。 小总结： SkipList的特点： 跳跃表是一个双向链表，每个节点都包含score和ele值 节点按照score值排序，score值一样则按照ele字典排序 每个节点都可以包含多层指针，层数是1到32之间的随机数 不同层指针到下一个节点的跨度不同，层级越高，跨度越大 增删改查效率与红黑树基本一致，实现却更简单 RedisObjectRedis中的任意数据类型的键和值都会被封装为一个RedisObject，也叫做Redis对象，源码如下： 1、什么是redisObject：从Redis的使用者的角度来看，⼀个Redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，⽽value可以是多种数据类型，比如：string, list, hash、set、sorted set等。我们可以看到，key的类型固定是string，而value可能的类型是多个。⽽从Redis内部实现的⾓度来看，database内的这个映射关系是用⼀个dict来维护的。dict的key固定用⼀种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同⼀个dict内能够存储不同类型的value，这就需要⼀个通⽤的数据结构，这个通用的数据结构就是robj，全名是redisObject。 Redis的编码方式 Redis中会根据存储的数据类型不同，选择不同的编码方式，共包含11种不同类型： 编号 编码方式 说明 0 OBJ_ENCODING_RAW raw编码动态字符串 1 OBJ_ENCODING_INT long类型的整数的字符串 2 OBJ_ENCODING_HT hash表（字典dict） 3 OBJ_ENCODING_ZIPMAP 已废弃 4 OBJ_ENCODING_LINKEDLIST 双端链表 5 OBJ_ENCODING_ZIPLIST 压缩列表 6 OBJ_ENCODING_INTSET 整数集合 7 OBJ_ENCODING_SKIPLIST 跳表 8 OBJ_ENCODING_EMBSTR embstr的动态字符串 9 OBJ_ENCODING_QUICKLIST 快速列表 10 OBJ_ENCODING_STREAM Stream流 五种数据结构 Redis中会根据存储的数据类型不同，选择不同的编码方式。每种数据类型的使用的编码方式如下： 数据类型 编码方式 OBJ_STRING int、embstr、raw OBJ_LIST LinkedList和ZipList(3.2以前)、QuickList（3.2以后） OBJ_SET intset、HT OBJ_ZSET ZipList、HT、SkipList OBJ_HASH ZipList、HT Redis基础数据结构的实现StringString是Redis中最常见的数据存储类型： 其基本编码方式是RAW，基于简单动态字符串（SDS）实现，存储上限为512mb。 如果存储的SDS长度小于44字节，则会采用EMBSTR编码，此时object head与SDS是一段连续空间。申请内存时只需要调用一次内存分配函数，效率更高。 如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码：直接将数据保存在RedisObject的ptr指针位置（刚好8字节），不再需要SDS了。 底层实现⽅式：动态字符串sds 或者 longString的内部存储结构⼀般是sds（Simple Dynamic String，可以动态扩展内存），但是如果⼀个String类型的value的值是数字，那么Redis内部会把它转成long类型来存储，从⽽减少内存的使用。 ListRedis的List类型可以从首、尾操作列表中的元素： LinkedList ：普通链表，可以从双端访问，内存占用较高，内存碎片较多 ZipList ：压缩列表，可以从双端访问，内存占用低，存储上限低 QuickList：LinkedList + ZipList，可以从双端访问，内存占用较低，包含多个ZipList，存储上限高 Redis的List结构类似一个双端链表，可以从首、尾操作列表中的元素： 在3.2版本之前，Redis采用ZipList和LinkedList来实现List，当元素数量小于512并且元素大小小于64字节时采用ZipList编码，超过则采用LinkedList编码。 在3.2版本之后，Redis统一采用QuickList来实现List： SetSet是Redis中的单列集合，满足下列特点： 不保证有序性 保证元素唯一 求交集、并集、差集 可以看出，Set对查询元素的效率要求非常高，思考一下，什么样的数据结构可以满足？HashTable，也就是Redis中的Dict，不过Dict是双列集合（可以存键、值对） Set是Redis中的集合，不一定确保元素有序，可以满足元素唯一、查询效率要求极高。 为了查询效率和唯一性，set采用HT编码（Dict）。Dict中的key用来存储元素，value统一为null。 当存储的所有数据都是整数，并且元素数量不超过set-max-intset-entries时，Set会采用IntSet编码，以节省内存 结构如下： ZsetZSet也就是SortedSet，其中每一个元素都需要指定一个score值和member值： 可以根据score值排序后 member必须唯一 可以根据member查询分数 因此，zset底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求。之前学习的哪种编码结构可以满足？ SkipList：可以排序，并且可以同时存储score和ele值（member） HT（Dict）：可以键值存储，并且可以根据key找value 当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此zset还会采用ZipList结构来节省内存，不过需要同时满足两个条件： 元素数量小于zset_max_ziplist_entries，默认值128 每个元素都小于zset_max_ziplist_value字节，默认值64 ziplist本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现： ZipList是连续内存，因此score和element是紧挨在一起的两个entry， element在前，score在后 score越小越接近队首，score越大越接近队尾，按照score值升序排列 HashHash结构与Redis中的Zset非常类似： 都是键值存储 都需求根据键获取值 键必须唯一 区别如下： zset的键是member，值是score；hash的键和值都是任意值 zset要根据score排序；hash则无需排序 （1）底层实现方式：压缩列表ziplist 或者 字典dict当Hash中数据项比较少的情况下，Hash底层才⽤压缩列表ziplist进⾏存储数据，随着数据的增加，底层的ziplist就可能会转成dict，具体配置如下： hash-max-ziplist-entries 512 hash-max-ziplist-value 64 当满足上面两个条件其中之⼀的时候，Redis就使⽤dict字典来实现hash。Redis的hash之所以这样设计，是因为当ziplist变得很⼤的时候，它有如下几个缺点： 每次插⼊或修改引发的realloc操作会有更⼤的概率造成内存拷贝，从而降低性能。 ⼀旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更⼤的⼀块数据。 当ziplist数据项过多的时候，在它上⾯查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。 总之，ziplist本来就设计为各个数据项挨在⼀起组成连续的内存空间，这种结构并不擅长做修改操作。⼀旦数据发⽣改动，就会引发内存realloc，可能导致内存拷贝。 hash结构如下： zset集合如下： 因此，Hash底层采用的编码与Zset也基本一致，只需要把排序有关的SkipList去掉即可： Hash结构默认采用ZipList编码，用以节省内存。 ZipList中相邻的两个entry 分别保存field和value 当数据量较大时，Hash结构会转为HT编码，也就是Dict，触发条件有两个： ZipList中的元素数量超过了hash-max-ziplist-entries（默认512） ZipList中的任意entry大小超过了hash-max-ziplist-value（默认64字节）","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"redis高可用机制","slug":"redis/redis高可用机制","date":"2023-03-17T16:00:00.000Z","updated":"2023-07-17T13:41:27.734Z","comments":true,"path":"2023/03/18/redis/redis高可用机制/","link":"","permalink":"https://yichenfirst.github.io/2023/03/18/redis/redis%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%BA%E5%88%B6/","excerpt":"","text":"高可用主从复制主从复制，是指将一台Redis服务器的数据复制到其他的Redis服务器。前者称为主节点（master），后者称为从节点（slave）。且数据的复制是单向的，只能由主节点到从节点。Redis主从复制支持主从同步和从从同步两种，后者是Redis后续版本新增的功能，以减轻主节点的同步负担。 主从复制的作用 数据冗余：主从复制实现的数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复： 当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复（服务的冗余） 负载均衡： 在主从复制的基础上配合读写分离，可以由主节点提供写服务，由从节点提供读服务，分担服务器负载。尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 主从复制原理Redis主从复制的工作流程如下： 保存主节点信息，主要是主节点的ip和port 主节点与从节点建立连接，从节点发现主节点后会尝试与主节点建立网络连接 发送ping命令，连接建立成功后从节点发送ping请求首次通信，主要是检测主从之间网络套接字是否可用，主节点当前是否可以接受处理命令 权限验证，如果主节点要求密码验证，从节点必须正确的密码才能通过验证 同步数据，主从复制连接正常通信后，主节点会把持有的数据全部发送给从节点 命令持续复制，接下来主节点会持续地把写命令发送给从节点，保证主从数据的一致性 主从数据同步方式全量复制主从第一次建立连接时，会执行全量同步，将maser节点的所有数据都拷贝给slave节点，流程如下： 那master是如何得知slave是第一次连接呢？ 通过Replication Id和offset字段来判断。 Replication Id： 简称replid，是数据集的标记，id一致说明是同一个数据集。每一个master都有一个唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须想master声明自己的replication id和offset，master才可以判断到底需要同步哪些数据。 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master简历连接时，发送的replid和offset是自己的replid和offset。master判断发现slave发送过来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步。 master会将自己replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 因此，master判断一个节点是否是第一次同步的依据就是看replid是否一致。如图： 完整流程： slave节点请求增量同步，发送自己的replid和offset master节点判断replid，发现不一致，拒绝则增量同步 master将自己的replid和offset发送给slave slave保存master发送过来的replid和offset master将完整的内存数据生成RDB文件，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log的命令发送给slave slave执行接受到的命令，保持与master之间的同步 增量同步全量同步需要生成RDB文件，然后将RDB文件通过网络传输给各个slave，成本比较高。因此除了第一次做全量同步，其他大多数时候slave与master都是做增量同步，只更新slave与master存在差异的部分数据，如图： 那么master怎么知道slave与自己的数据差异在哪里呢？ 通过全量同步时的repl_baklog文件。 这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。repl_baklog中会记录Redis处理过的命令日志和offset，包括master当前的offset，和slave已经拷贝到的offset。 slave与master的offset之间的差异，就是slave需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 直到数组被填满： 此时如果有新的数组写入，就会覆盖数组中的旧数据。不过旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色的部分。 但是，如果slave网络出现阻塞导致master的offset远远超过了slave的offset 如果过master继续写入新数据，其offset就会覆盖旧的数据，知道将slave现在的offset也覆盖： 棕色框中的红色部分，就是尚未同步，但是已经被覆盖了的数据。如果此时slave恢复，需要同步，发现自己的offset没有了，就无法完成增量同步，只能做全量同步。 主从同步优化 在master中配置repl-diskless-sync yes启动无磁盘复制，避免全量同步时的磁盘IO Redis单节点上的内存不要太大，减少RDB导致过多的磁盘IO 适当提高repl-baklog的大小，发现slave宕机尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实现是太多的slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图 小结问：全量同步和增量同步的区别？ 答： 全量同步: master将完整的内存数据库生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave 增量同步：slave提交自己的offset到master，master获取从repl_baklog中从offset之后的命令给slave 问：什么时候执行全量同步？ 答： slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖 问：什么时候执行增量同步？ **答:**slave节点断开又恢复，并且repl_baklog中能找到offset时 哨兵机制哨兵实现了什么功能？下图是一个经典的哨兵集群监控的逻辑图： 监控： 哨兵会不断地检查主节点和从节点是否运作正常 自动故障转义： 当主节点不能正常工作时，哨兵会开始自动故障转义操作，它会将失效主节点的其中一个节点升级为新的主节点，并让其他从节点改为复制新的主节点 配置提供者： 客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址 通知： 哨兵可以将故障转移的结果发送给客户端 哨兵集群是通过什么方式组建的？哨兵实例之前可以相互发现，要归功于Redis提供的pub/sub机制，也就是发布/订阅机制。 在主从集群中，主库上有一个名为__sentinel_:hello的频道，不同哨兵就是通过它来相互发现，实现通信的。在下图中，哨兵1将自己的IP（172.16.19.3）和端口（26579）发布到__sentinel__hello:频道上，哨兵2和3订阅了该频道。那么此时，哨兵2和3就可以从这个频道直接获取哨兵1的IP和端口号。然后然后2、3就可以和哨兵1建立网络连接。 通过这个机制，哨兵2和3也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如说对主库有没有下线这件事进行协商。 哨兵是如何监控redis集群的？这是又哨兵向主库发送INFO命令来完成的。如下图，哨兵2给主库发送INFO命令，主库接受这个命令后，就会发从库列表返回给哨兵。接着哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵1和哨兵3可以通过相同的方式和从库建立连接。 哨兵如何判断主库已经下线了？首先要理解两个概念：主观下线和客观下线 主观下线：任何一个哨兵都可以监控探测，并做出Redis节点下线的判断 客观下线：由哨兵集群共同决定Redis节点是否下线 当某个哨兵判断主库主观下线后，就会给其他哨兵发送is-master-down-by-addr命令。接着，其他哨兵会根据自己和主库的连接情况，做出Y或N的响应，Y相当于赞成票，N相当于反对票。 如果赞成票大于等于哨兵配置文件中的quorum配置项，则可以判断主库客观下线了。 哨兵的选举机制 为什么必然会出现选举/共识机制？ 为了避免哨兵的单点情况发生，所以需要一个哨兵的分布式集群。作为分布式集群，必然涉及共识问题（即选举问题）；同时故障的转移和通知都只需要一个主的哨兵节点就可以了。 哨兵的选举机制是什么样的？ 哨兵的选举机制其实很简单，就是一个Raft选举算法： 选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举 任何一个想成为 Leader 的哨兵，要满足两个条件： 第一，拿到半数以上的赞成票； 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。 主库判定客观下线了，那么如何从剩余的从库中选择一个新的主库？ 过滤掉不健康的（下线或断线），没有回复过哨兵ping响应的从节点 选择slave-priority从节点优先级最高的 选择复制偏移量最大的，支付至最完整的从节点 选择run id最小的 新的主库选择出来后如何进行故障转移？ 将slave-1脱离原从节点，升级为主节点 将从节点slave-2指向新的主节点 通知客户端主节点已更换 将原主节点变成从节点，指向新的主节点 转移之后","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"redis持久化机制","slug":"redis/redis持久化机制","date":"2023-03-15T16:00:00.000Z","updated":"2023-07-17T13:41:27.724Z","comments":true,"path":"2023/03/16/redis/redis持久化机制/","link":"","permalink":"https://yichenfirst.github.io/2023/03/16/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/","excerpt":"","text":"redis的持久化机制redis持久化方案分为RDB和AOF两种。 RDBRDB持久化是把当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发。 RDB文件是一个压缩的二进制文件，通过它可以还原某个时刻数据库的状态。由于RDB文件是保存在硬盘上的，所以即使Redis崩溃或退出，只要RDB文件存在，就可以用它来恢复还原数据库的状态。 手动触发分别对应save和bgsave命令： save命令：阻塞当前redis服务器，知道RDB过程完成为止，对于内存比较大的实例会造成长时间阻塞，线上环境不建议使用。 bgsave命令：redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。 以下场景会自动触发RDB持久化： 使用save相关配置， 如”save m n”。表示m秒内数据存在n次修改，自动触发bgsave。 如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 执行debug reload命令重现加载Redis时，也会自动触发save操作 默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave 问：RDB由于生产环境中我们为Redis开辟的内存区域都比较大（例如6GB），那么将内存中的数据同步到硬盘的过程可能就会持续比较长的时间，而实际情况是这段时间Redis服务一般都会收到数据写操作请求。那么如何保证数据一致性呢？ 答：RDB中核心的思路是Copy-on-Write，来保证在进行快照操作的这段时间，需要压缩写入磁盘的数据在内存中不会发生变化。在正常的快照操作中，一方面redis主进程会fork一个新的快照线程专门来做这个事情，这样保证了redis服务不会停止对客户端包括写请求在内的任何响应。另一方面这顿时间发生的数据变化都会以副本的方式存放在另一个新的内存区域，等快照操作结束后才会同步到原来的内存区域。 例如： 如果主线程对这些数据也是读操作（例如图中的键值对A），那么，主线程和bgsave线程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对C），那么这块数据就会被复制一份，生成该数据的副本。然后，bgsave线程会把这个副本写入RBD文件，而在这个过程中，主线程人可以修改原来的数据。 问：在进行RDB快照操作的这段时间，如果服务崩溃怎么办？ 答：在没有将数据全部写入到磁盘前，这次快照操作都不算成功。如果出现了服务崩溃的情况，将以上一次完整的RDB快照文件作为恢复内存数据的参考。也就是说，在快照操作的过程中不能影响上一次的备份数据。Redis服务会在磁盘上创建一个临时文件进行数据操作，待操作成功后才会用这个临时文件替换掉上一次的备份。 AOFAOF持久化：以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中命令达到恢复数据的目的。AOF的主要作用是解决了数据持久化的实时性，目前已经成为Redis持久化的主流方式。 AOF的工作流程操作：命令写入（append）、文件同步（sync）、文件重写（rewrite）、重启加载（load）： 流程如下： 所有的写入命令会追加到aof_buf（缓冲区）中。 AOF缓冲区根据对应的策略向硬盘做同步操作。 随着AOF文件越来越大，需要定期对AOF进行重写，达到压缩的目的。 当redis服务器重启时，可以加载AOF文件进行输入恢复。 问：AOF是写前日志还是写后日志 答：AOF日志采用写后日志，即先写内存，后写日志 为什么采用写后日志？ Redis要求高性能，采用写后日志有两方面好处： 避免额外的检查开销： Redis在线AOF里记录日志的时候，并不会先对这些命令进行语法检查。所以先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis在使用日志恢复数据时，就有可能会出错。 不会阻塞当前的写操作 优缺点RDB优点 只有一个紧凑的二进制文件，非常适合备份、全量复制的场景 容灾性好，可以把RDB文件拷贝到远程机器或者文件系统中，用于容灾恢复 恢复速度快，RDB恢复数据的速度远远快于AOF的方式 RDB缺点 实时性低，RDB是间隔一段时间进行持久化，没办法做到实时持久化。如果在这一间隔时间发生故障，数据则会丢失。 存在兼容问题，redis演进过程存在多个格式的RDB版本，存在老版本Redis无法兼容新版本的RDB的问题。 AOF优点 实时性好，AOF持久化可以配置appendsync属性，有always，每进行一次命令操作就记录到AOF文件中一次。 通过append模式写文件，即使中途服务器宕机，可以通过redis-check-aof工具解决数据一致性问题 AOF缺点 AOF比RDB文件大，且恢复速度慢。 数据集大的时候，比RDB启动效率低。 数据恢复当Redis发生了故障，可以从RDB或者AOF中恢复数据。 Redis启动时加载数据的流程： AOF持久化开启且存在AOF文件时，优先加载AOF文件。 AOF关闭或者AOF文件不存在时，加载RDB文件。 加载AOF/RDB文件成功后，Redis启动成功。 AOF/RDB文件存在错误时，Redis启动失败并打印错误信息。","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"TCP协议","slug":"计算机网络/TCP","date":"2023-03-09T16:00:00.000Z","updated":"2023-07-17T13:41:27.794Z","comments":true,"path":"2023/03/10/计算机网络/TCP/","link":"","permalink":"https://yichenfirst.github.io/2023/03/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP/","excerpt":"","text":"TCP特点报文格式 连接管理建立连接假设一台主机（客户）上的一个进程相遇另一台主机（服务器）上的一个进程建立一条连接，客户应用进程首先会通知客户TCP，它想要建立一个与服务器上某个进程之间的连接，客户中的TCP会用以下的步骤与服务器中的TCP建立一条TCP连接： 建立TCP连接时，客户端进程A会像服务端进程B发送连接请求报文段，这时首部中的同步位SYN = 1，同时生成一个初始序号seq = x。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但是要消耗掉一个序号。这时，TCP客户端进入SYN-SENT（同步已发送）状态。 B收到连接请求报文后，如同意建立连接，则向A发送确认。在确认报文中应把SYN位和ACK位都置1，确认号ack = x + 1，同时生成一个徐序号seq = y。注意，这个报文段也不能携带数据，但是同样要消耗一个序号。这时TCP服务器进入TCP-RCVD（同步收到）状态。 客户端A收到B的确认后，还要向B给出确认。确认报文段的ACK置1，确认号ack = y + 1，而自己的序号seq = x + 1。TCP标准规定，ACK报文段可以携带数据。但如果不携带数据则不消耗序号，在这种情况下，下一个数据报文段的序号仍是seq = x + 1。这时，TCP连接已经建立，A进入ESTABLISHED（已建立连接）状态。 当B收到A的确认后，也进入ESTABLISHED状态。 为什么A最后还要发送一次确认呢？ 答： 防止已失效的连接请求报文突然又传送到了B 如果A最后不向B发出确认。当A向B发出第一个连接请求报文，但是由于网络原因长时间未到达B。于是A重传一次连接请求，B收到后向A出发确认报文，建立TCP连接。这时A第一次发送的连接请求报文到达B，B也会向A发送确认报文并建立TCP连接，但是这条TCP并不会进行数据传输，造成了资源的浪费。 A最后向B发送确认，可以防止上述现象的发生。例如在刚才的异常情况下，A不会向B发送确认报文（通过seq与ack），由于B没有收到确认报文，就不会建立TCP连接。 同步双方的初始序列号 如图，同步一段的序列号需要发送两次请求，一次SYN同步请求，一次ACK确认请求。但是由于服务器端ACK和SYN请求可以合并，所以同步双方序列号只需要三次握手。当只有两次握手时，客户端不会对服务端的SYN进行确认，所以服务端的序号无法同步成功。 为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？ 关闭连接 数据传输结束后，双方都可以释放连接。如何，客户端A先发出连接释放报文段，并停止发送数据，主动关闭TCP连接。A报连接释放报文首部的终止控制位FIN置1，序号seq = u，它等于前面已传送过的数据的最后一个字节的序号加1.这时A进入FIN-WAIT-1（终止等待1）状态，等待B的确认。TCP规定，FIN报文即使不携带数据，它也消耗掉一个序号。 B收到连接释放报文后即发出确认报文，确认号是ack = u + 1，而这个报文段自己的序号的v，等于B前面已传送过的数据的最后一个字节的序号加1。然后B就进入CLOASE-WAIT（关闭等待）状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时TCP连接处于半关闭状态，即A已经没有数据发送的能力，但B若发送数据，A仍要接受。 A收到B的确认后，进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。 若B已经没有要向A发送的数据，其应用程序就通知TCP释放连接。这时B发送的连接释放报文段必须使FIN=1。现假设B的序号为w（在半关闭状态B可能又发送了一些数据）。B还必须重复上次已发送过的确认号ack = u + 1。这时B就进入LAST-ACK（最后确认）状态，等待A的确认。 A在收到B的连接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ACK = w + 1，而自己的需要seq = u + 1（根据TCP标准，前面发送的FIN报文段要消耗一个序号）。然后进入到TIME-WAIT（时间等待）状态。注意，此时TCP连接还没有释放掉，必须经过时间等待计时器设置的时间2MSL后，A才进入到CLOSED状态。MSL叫最长报文段寿命。 也就是说，A主动释放TCP连接，需要发送FIN报文通知B自己没有数据发送需要发送，B会对返回A一个ACK确认报文，当B没有数据发送时会向A发送一个FIN报文表示自己没有数据需要发送，A发送确认报文后，等待2MSL后TCP连接就关闭了。 除时间等待计时器外，TCP还设有一个保活计时器，当客户端与服务端建立TCP连接后，客户端突然出现故障，服务端无法收到来自客户端的消息。因此，需要有措施使服务器不再白白等待下去。这就是保活计时器。服务器每收到一次客户端数据，就重新设置保活计时器，时间设置通常是两小时，若两小时没有收到客户的数据，服务端就发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测blown后仍无客户端的响应，服务端就认为客户端出现故障，接着关闭这个连接。 为什么A在TIME-WAIT状态必须等待2MSL的时间 第一，为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因此使处在LAST-ACK状态的B收不到已发送的FIN+ACK报文的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的报文，接着A重传一次确认，重新启动2MSL计时器。最后，A和B都能正常进入到CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文后立即释放连接，那么就无法收到重传的FIN+ACK报文，也就不会再一次发送确认报文，这样B就不会按正常步骤进入CLOSED状态。 第二，如图，服务端在关闭连接之前发送的seq = 301报文，被网络延迟了。接着，服务端以相同的四元组重新打开了新连接，前面被延迟的seq = 301这时抵达客户端，而且该数据报文的序列号刚好在客户端接受窗口内，因此客户端会正常接受这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。 为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 2MSL 时长，这个时间足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。 为什么需要四次握手 主要是当客户端主动关闭连接时，服务端可能还有数据发送，所以不能立马关闭连接，需要等客户端和服务端都没有数据发送时才能关闭，客户端和服务端都需要向对方发送一次FIN报文，以及对方需要返回一个ACK报文。 TIME_WAIT过多有什么危害 第一占用系统资源，比如文件描述符，内存资源，CPU资源，线程资源 第二占用端口资源 服务器出现大量TIME_WAIT状态的原因有哪些？ TIME_WAIT是主动关闭连接放才会出现的状态，服务器产生大量TIME_WAIT说明服务器关闭了大量TCP连接。 第一，客户端与服务器至少一方禁用了HTTP keep-alive。一般来说不管哪一方禁用了HTTP keep-alive都是由服务端主动关闭连接。因此任意一方没有开启HTTP keep-alive都会导致服务端在处理完一个HTTP请求后，就主动关闭连接，此时服务端上就会出现大量的TIME_WAIT。 第二，HTTP长连接超时。HTTP长连接可以在同一个TCP连接上接受和发送多个HTTP请求/应答，避免了连接建立和释放的开销。为了避免资源浪费的情况，web服务器一般会提供一个参数，用来制定指定HTTP长连接的超时时间，比如nginx提供的keepalive_timeout参数。如果keepalive_time = 60，客户端在完成一次HTTP请求后，在60秒内都没有在发起新的请求，nginx会触发回调函数关闭连接，那么服务器上就会出现TIME_WAIT状态的连接。 当服务端出现大量 TIME_WAIT 状态的连接时，如果现象是有大量的客户端建立完 TCP 连接后，很长一段时间没有发送数据，那么大概率就是因为 HTTP 长连接超时，导致服务端主动关闭连接，产生大量处于 TIME_WAIT 状态的连接。 可以往网络问题的方向排查，比如是否是因为网络问题，导致客户端发送的数据一直没有被服务端接收到，以至于 HTTP 长连接超时。 第三，HTTP长连接请求数达到上限。Web 服务端通常会有个参数，来定义一条 HTTP 长连接上最大能处理的请求数量，当超过最大限制时，就会主动关闭连接。 比如 nginx 的 keepalive_requests 这个参数，这个参数是指一个 HTTP 长连接建立之后，nginx 就会为这个连接设置一个计数器，记录这个 HTTP 长连接上已经接收并处理的客户端请求的数量。如果达到这个参数设置的最大值时，则 nginx 会主动关闭这个长连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。 keepalive_requests 参数的默认值是 100 ，意味着每个 HTTP 长连接最多只能跑 100 次请求，这个参数往往被大多数人忽略，因为当 QPS (每秒请求数) 不是很高时，默认值 100 凑合够用。 但是，对于一些 QPS 比较高的场景，比如超过 10000 QPS，甚至达到 30000 , 50000 甚至更高，如果 keepalive_requests 参数值是 100，这时候就 nginx 就会很频繁地关闭连接，那么此时服务端上就会出大量的 TIME_WAIT 状态。 针对这个场景下，解决的方式也很简单，调大 nginx 的 keepalive_requests 参数就行。 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？ CLOSE_WAIT 状态是「被动关闭方」才会有的状态，而且如果「被动关闭方」没有调用 close 函数关闭连接，那么就无法发出 FIN 报文，从而无法使得 CLOSE_WAIT 状态的连接转变为 LAST_ACK 状态。 所以，当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序没有调用 close 函数关闭连接。 可靠传输TCP发送的报文时交给IP才层传送的，但IP层只能提供尽最大努力服务，因此TCP下面的网络提供的是不可靠的传输。因此TCP必须采用适当的措施才能使两个运输层之间的通信变得可靠。 停止等待协议停止等待协议可用下图（a）说明。图描述的是没有出现差错的情况。A发送分组M1，发送完就暂停发送，等待B的确认。B收到M1就向A发送确认。A在收到了对M1的确认后，就在发送下一个分组M2。同样，在收到B对M2的确认后，再发送M3。 当传输出现错误时，如图b所示。A发送的分组M1在传输过程中出现错误（丢失或出现差错），B不会向A进行确认，当A超过一段时间没有收到确认，就会认为刚才发送的分组丢失了，因此hi重传恰年发送过的分组。有三点需要注意，第一，A在发送完一个分组后，必须暂时保留已发送的分组分副本。第二，分组和确认分组都必须进行编号。这样才能明确是哪一个发送出去的分组收到确认，而哪一个分组没有收到确认。第三，超时计时器设置的重传时间应当比数据在分组传输的平均往返时间更长一些。 使用上述的确认和重传机制，就可以在不可靠的传输网络上实现可靠的通信。像上述的这种可靠传输协议常称为自动重传请求ARQ（Automatic Repeat request）。 连续ARQ协议为了提高效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。流水线传输就是发送方可以连续发送多个分组，不必每发送完一个分组就停顿下来等待对方的确认。这样可使信道上一直有数据不间断地在传送，这种传输方式可以获得很高的新到利用率。当使用流水线传输时，就需要使用连续ARQ协议和滑动窗口协议。 如图，当传送数据的顺序时1～12，滑动窗口的大小是5。发送方会连续发送滑动窗口内的分组，而不需要等待接收到的确认。当收到接受方的确认后，滑动窗口会向数据传输方向移动。图b，发送方接收到分组1的确认，则滑动窗口向前移动，然后发送窗口内没发送的分组。 接收方 一般都是采用累积确认的方式。这就是说，接收方不必对收到的分组逐个发送确认，而是在收到几个分组后，对按序到达的最后一个分组发送确认，这就表示:到这个分组为止的所有分组都已正确收到了。 累积确认有优点也有缺点。优点是:容易实现，即使确认丢失也不必重传。但缺点是 不能向发送方反映出接收方己经正确收到的所有分组的信息。 例如，如果发送方发送了前5 个分组，而中间的第3个分组丢失了。这时接收方只能对前两个分组发出确认。发送方无法知道后面三个分组的下落，而只好把后面的三个分组都 再重传一次 。 这就叫做 Go-back-N (回退N )，表示需要再退回来重传己发 送过的N个分组。可见当通信线路质量不好时，连续ARQ 协议会带来负面的影响。 选择确认SACK 如果接收到的报文段无差错，只是未按序号，中间缺少一些序号的数据，那么能否只传送缺少的数据而不重传已经正确到达接收方的数据？选择确认（SACK）就是一种可行的处理方法。 TCP的接收方在接受对方发过来的数据字节流的序号不连续，结果形成了一些不连续的字节块，如上图。可以看出序号11500收到了，但序号10011500没有收到。接下来的字节流又收到了，可是又缺少了3001 ~ 3500。再后面从序号4501 起又没有收到。也就是说，接收方收到了和前面的字节流不连续的两个字节块。如果这些字节的序号都在接收窗又之内，那么 接收方就先收 下这些数据，但要把这些信息准确地告诉发送方，使发送方不要再重复发送这 些已收到的数据。 从图中可以看出呵前后字节不连续的每一个字节块都有两个边界：左边界和右边界。因此一个字节块需要两个指针进行标记。TCP 的首部没有哪个字段能够提供上述这些字节块的边界信息。RFC 2018 规定，如果要使用选择确认SACK，那么在建立TCP连接时，就要在ICP 首部的选项中加 上“ 允许SACK〞的选项，而双方必须都事先商定好。如果使用选择确认，那么原来首部中 的“确认号字段” 的用法仍然不变。只是以后在 TCP 报文段的首部中都增加了SACK 选 项，以便报告收到的不连续的字节块的边界。由于首部选项的长度最多只有 40 字节，而指明一个边界就要用掉4字节 ( 因为序号有32位 ，需要使用4个字节表示 )， 因此在选项中最多只能指明4 个字节块的边界信息。这是因为4 个字节块共有8 个边界，因而需要用 32 个 字节来描述。另外还需要两个字节。一个字节用来指明是SACK 选项，另一个字节是指明 这个选项要占用多少字节。如果要报告五个字节块的边界信息，那么至少需要42 个字节。 这就超过了选项长度的40字节的上限。 流量控制 利用滑动窗口实现流量控制 一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制(flow control)就是让发送方 的发送速率不要太快，要让接收方来得及接收。 利用滑动窗又机制可以很方便地在TCP 连接上实现对发送方的流量控制。 设A向B发送数据。在连接建立时，B告诉了A: “我的接收窗又rwnd = 400” (这 里 rwnd 表示receiver window)。因此，发送方的发送窗又不能超过接收方给出的接收窗口的数值。请注意，TCP 的窗又单位是字节，不是报文段。TCP 连接建立时的窗又协商过程在图中没有展示出来。再设每一个报文段为 100 字节长，而数据报文段序号的初始值设为1。 请注意 ， 图中箭头上面大写ACK 表示首部中的确认位ACK，小写ack 表示确认字段的值。我们应注意到，接收方的主机B进行了三次流量控制。第一次把窗又减小到rwnd = 300, 第二次又减到rwnd=100，最后减到rwnd =0，即不允许发送方再发送数据了。这种使发送方暂停发送的状态將持续到主机B 重新发出一个新的窗又值为止。我们还应注意到，B 向 A 发送的三个报文段都设置了ACK = 1，只有在ACK = 1 时确认号字段才有意义。现在我们考虑一种情况。在图5-22 中，B 向A 发送了零窗又的报文段后不久，B 的接 收缓存又有了一些存储空间。于是B向A发送了rwnd= 400的报文段。然而这个报文段在传送过程中丢失了 。 A一直等待收到B发送的非零窗又的通知 ， 而B也一直等待A发 送 的数据。如果没有其他措施，这种互相等待的死锁局面将一直延续下去。 为了解决这个问题，TCP 为每一个连接设有一个持续计时器(persistence timer)。只要TCP 连接的一方收到对方的零窗又通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗又探测报文段 (仅携带 1 字节的数据)。，而对方就在确认这个探测报文段时给出了现在的窗又值。如果窗又仍然是零，那么收到这个报文段的 一方就重新设置持 续计时器。如果窗又不是零，那么死锁的僵局就可以打破 了。 拥塞控制 为什么要有拥塞控制，不是有流量控制了吗 流量控制是避免发送方得数据填满接收方的缓存，但是并不知道网络的中发生了什么。 一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。 于是，就有了拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 什么事拥塞窗口？和发送窗口有什么关系？ 拥塞窗口cwnd是发送方维护的一个状态变量，它会根据网络的拥塞程度动态变化。 发送窗口swnd和接受窗口rwnd是约等于的关系，加入拥塞窗口的概念口，此时发送窗口swnd = min(cwnd，rwnd)，也就是拥塞窗口和接受窗口中的最小值。 拥塞窗口cwnd变化的规则： 只要网络中没有出现拥塞，cwnd就会增大 但是网络中出现了拥塞，cwnd就减少 只要发送方没有在规定时间内接收到ACK应答报文，也就是发生了超时重传，就会认为网络出现了拥塞。 拥塞控制的相关算法 慢启动 拥塞避免 拥塞发生 快速恢复 慢启动TCP在刚建立连接完成后，首先是有一个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果刚开始就发送大量的数据，很容易就会造成网络的拥堵。 慢启动算法规则：当发送方没收到一个ACK，拥塞窗口cwnd的大小就会加1。 假设拥塞窗口cwnd和发送窗口相等，下面举个例子： 连接建立完成后，一开始初始化cwnd = 1，标识可以传送一个MSS大小的数据 当收到一个ACK确认应答后，cwnd增加1，于是一次可以发送2个 当收到2个ACK确认应答后，cwnd加2，于是就可以比之前多发送2个，所以这一次能发送4个 当收到这4个ACK确认应答后，cwnd加4，所以这一次能发送8个 满期启动算法的变化过程如下图： 可以看出慢启动算法，发包的个数是指数型增长的。 那慢启动涨到什么时候是个头呢? 有一个叫慢启动门限ssthresh的状态变量 当cwnd &lt; ssthresh时，使用慢启动算法 当cwnd &gt; ssthresh时，使用拥塞避免算法 拥塞避免算法当拥塞窗口cwnd超过慢启动门限ssthresh就会进入拥塞避免算法。 一般来说ssthresh的大小是65535字节。 拥塞避免算法的规则是：每当收到一个ACK时，cwnd增加1/cwnd。 接着上面面启动的例子，现在假定ssthresh为8： 当发送放8个ACK确认应答后，每个确认增加1/8,8个ACK确认cwnd一共增加1，于是这一次能发送9个MSS大小的数据，变成了线性增长。 所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。 就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。 当触发了重传机制，也就进入了拥塞发生算法。 拥塞发生当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 这两种使用的拥塞发送算法是不同的，接下来分别来说说。 超时重传 当发生了超时重传，则就会使用拥塞发生算法。 ssthresh设为cwnd/2 cwnd重置1（重置为cwnd初始值，这里假定初始值1） 当网络发生拥塞时，会重新开始慢启动，这种方式太激进，会造成网络卡顿。 快速重传 比超时重传更好的方式是，快速重传算法。当接收方发现丢了一个中间包的时候，发送三次前一个包的ACK，于是发送端就会快速地重传，不必等待超时再重传。 TCP认为这种情况不严重，因为大部分没丢，只丢了一小部分，则ssthresh和cwnd变化如下： cwnd = cwnd / 2，也就是设置为原来的一般 ssthresh = cwnd 进入快速恢复算法 快速恢复快速重传和快速恢复算法一般同时使用，快速恢复算法认为，你还能收到3个重复ACK说明网络也不那么糟糕，所以没必要向超时那么激烈。 进入快速恢复之前，cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，也就是设置为原来的一半; ssthresh = cwnd; 然后，进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）； 重传丢失的数据包； 如果再收到重复的 ACK，那么 cwnd 增加 1； 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态； 快速恢复算法变化过程如下图：","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://yichenfirst.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"https://yichenfirst.github.io/tags/tcp/"}]},{"title":"进程与线程切换的区别","slug":"操作系统/进程与线程切换的区别","date":"2023-02-23T16:00:00.000Z","updated":"2023-07-28T14:00:31.015Z","comments":true,"path":"2023/02/24/操作系统/进程与线程切换的区别/","link":"","permalink":"https://yichenfirst.github.io/2023/02/24/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%88%87%E6%8D%A2%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"我们都知道线程切换的开销比进程切换的开销小，那么小在什么地方？切换的过程是怎样的？ 无论是在多核还是单核系统中，一个CPU看上去都像是在并发的执行多个进程，这是通过处理器在进程间切换来实现的。在任何一个时刻，单处理器系统都只能执行一个进程的代码。 操作系统实现这种交错执行的机制称为上下文切换。 操作系统保持跟踪进程运行所需的所有状态信息，这种状态，也就是上下文，它包括许多信息，例如PC和寄存器文件的当前值，以及主存的内容。 进程切换系统中的每个程序都是运行在某个进程的上下文中的。 上下文是由程序正确运行所需的状态组成的，这个状态主要是存放在存储器中的程序的代码和数据、用户栈，通用目的寄存器的内容，程序计数器，环境变量以及打开文件描述符的集合、内核栈。 所以进程切换就是上下文切换。 通用目的寄存器 浮点寄存器 程序计数器 用户栈 状态寄存器 内核栈 各种内核数据结构：比如描绘地址空间的页表，包含有关当前进程信息的进程表，以及包含进程已打开文件的信息的文件表。 其实就是整个虚拟地址空间里的东西，包括用户空间和内核空间 线程切换当然这里的线程指的是同一个进程中的线程。要想正确回答这个问题，需要理解虚拟内存。 虚拟内存虚拟内存是操作系统为每个进程提供的一种抽象，每个进程都有属于自己的、私有的、地址连续的虚拟内存，当然我们知道最终进程的数据及代码必然要放到物理内存上，那么必须有某种机制能记住虚拟地址空间中的某个数据被放到了哪个物理内存地址上，这就是所谓的地址空间映射，那么操作系统是如何记住这种映射关系的呢，答案就是页表。 为了加速页表，还引入了高速缓存TLB 还需要的明确的是，页表是放在内存中，每个进程有自己的页表；TLB放再CPU MMU中，是进程间共享的（不是很准确的描述，后面会细讲） 每个进程有自己独立的虚拟地址空间，进程内的所有线程共享进程的虚拟地址空间 那回到最开始的问题，进程切换和线程切换有什么区别？ 进程切换和线程切换的区别最主要的一个区别在于进程切换涉及虚拟地址空间的切换而线程不会。因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。 为什么虚拟地址空间切换会比较耗时呢？因为cache和TLB会失效 Flushing the TLBcache和TLB会失效的前提是它们的entry不会记录pid一些处理器，每次上下文切换时都会刷新整个TLB，这可能会非常昂贵，因为这意味着新进程不得不经历缺页、查找页表和插入条目整个过程有些处理器，会给每个TLB项添加一个额外的唯一字段ASID(address space ID)，这意味这每个地址空间都有自己的ID，且标记在TLB上。因此上下文切换的时候TLB不需要被刷新，新来的进程讲会有不同的地址空间ID，甚至可以请求相同的虚拟地址，因为地址空间ID不同，翻译之后的物理地址也会不同。这种方式能减少清空、增加系统性能，但是需要更多的TLB硬件来存储ASIB位","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"Java多线程依次打印ABC","slug":"java/Java多线程依次打印ABC","date":"2023-01-10T16:00:00.000Z","updated":"2023-08-01T13:00:37.974Z","comments":true,"path":"2023/01/11/java/Java多线程依次打印ABC/","link":"","permalink":"https://yichenfirst.github.io/2023/01/11/java/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%BE%9D%E6%AC%A1%E6%89%93%E5%8D%B0ABC/","excerpt":"","text":"synchronized+wait/notifyAll基本思路就是线程A、线程B、线程C三个线程同时启动，因为变量num的初始值为0，所以线程B或线程C拿到锁后，进入while()循环，然后执行wait()方法，线程线程阻塞，释放锁。只有线程A拿到锁后，不进入while()循环，执行num++，打印字符A，最后唤醒线程B和线程C。此时num值为1，只有线程B拿到锁后，不被阻塞，执行num++，打印字符B，最后唤醒线程A和线程C，后面以此类推 123456789101112131415161718192021222324252627282930313233343536public class Main { private static final Object lock = new Object(); private int num = 0; private void printABC(int targetNum) { while(true) { synchronized (lock) { while(num % 3 != targetNum){ try { Thread.sleep(100); lock.wait(); } catch (Exception e){ e.printStackTrace(); } } System.out.println(Thread.currentThread().getName()); num++; lock.notifyAll(); } } } public void main() { new Thread(() -&gt; printABC(0), \"A\").start(); new Thread(() -&gt; printABC(1), \"B\").start(); new Thread(() -&gt; printABC(2), \"C\").start(); } public static void main(String[] args) { Main main = new Main(); main.main(); }} join()join()方法：在A线程中调用了B线程的join()方法时，表示只有当B线程执行完毕时，A线程才能继续执行。基于这个原理，我们使得三个线程按顺序执行，然后循环多次即可。无论线程1、线程2、线程3哪个先执行，最后执行的顺序都是线程1——&gt;线程2——&gt;线程3。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Main { private static final Lock lock = new ReentrantLock(); private int num = 0; private void printABC(Thread boforeThread) { try { Thread.sleep(100); } catch (Exception e){ e.printStackTrace(); } if(boforeThread == null) { System.out.println(Thread.currentThread().getName()); } else { try { boforeThread.join(); } catch (InterruptedException e) { throw new RuntimeException(e); } System.out.println(Thread.currentThread().getName()); } } public void main() { while(true){ Thread t1 = new Thread(() -&gt; printABC(null), \"A\"); Thread t2 = new Thread(() -&gt; printABC(t1), \"B\"); Thread t3 = new Thread(() -&gt; printABC(t2), \"C\"); t1.start(); t2.start(); t3.start(); } } public static void main(String[] args) { Main main = new Main(); main.main(); }} lock与synchromized+wait/notify类似。不管哪个线程拿到锁，只有符合条件才能打印。 1234567891011121314151617181920212223242526272829303132333435public class Main { private static final Lock lock = new ReentrantLock(); private int num = 0; private void printABC(int targetNum) { while(true) { lock.lock(); if(num % 3 == targetNum){ System.out.println(Thread.currentThread().getName()); try { Thread.sleep(100); } catch (Exception e){ e.printStackTrace(); }finally { num++; } } lock.unlock(); // 注意解锁过程最好放在finally块中 } } public void main() { new Thread(() -&gt; printABC(0), \"A\").start(); new Thread(() -&gt; printABC(1), \"B\").start(); new Thread(() -&gt; printABC(2), \"C\").start(); } public static void main(String[] args) { Main main = new Main(); main.main(); }} Lock+ConditionSemaphore","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"Spring AOP源码分析","slug":"spring/Spring AOP源码分析","date":"2023-01-02T16:00:00.000Z","updated":"2023-07-17T13:41:27.734Z","comments":true,"path":"2023/01/03/spring/Spring AOP源码分析/","link":"","permalink":"https://yichenfirst.github.io/2023/01/03/spring/Spring%20AOP%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"spring源码分析spring中aop可以通过注解或xml文件来使用，下面使用注解的方式分析aop的实现过程。 测试样例MyAspects.java 123456789101112131415161718192021222324import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.stereotype.Component;import org.aspectj.lang.annotation.Aspect;@Component@Aspectpublic class MyAspects { @Pointcut(\"execution(* *.test(..))\") public void test(){ } @Before(\"test()\") public void beforeTest(){ System.out.println(\"before\" ); } @After(\"test()\") public void afterTest(){ System.out.println(\"after\" ); }} UserService.java 12345678910111213141516import org.springframework.stereotype.Service;@Servicepublic class UserService { public void test() { System.out.println(\"test()\"); } public void test2() { System.out.println(\"test2()\"); }} SpringText.java 123456789101112@EnableAspectJAutoProxy@ComponentScan(\"com.yichen.base.aop\")public class SpringTest { public static void main(String[] args) { ApplicationContext applicationContext = new AnnotationConfigApplicationContext(SpringTest.class); UserService userService = (UserService) applicationContext.getBean(\"userService\"); userService.test(); }} MyAspects.java对UserService.java的方法进行了增强。 注意: 使用aop注解需要引入相关jar包(为方便debug, 在spring源码的基础上一个新的模块, 添加测试程序代码。) 123compile('org.aspectj:aspectjrt:1.9.7')compile('org.aspectj:aspectjweaver:1.9.7')compile('org.aspectj:aspectjtools:1.9.7') 注入AnnotationAwareAspectJAutoProxyCreator以注解的方式使用aop需要开启@EnableAspectJAutoProxy, 如下代码, 在@EnableAspectJAutoProxy中使用@Import注解向容器添注入用于处理AOP的bean。 123456789101112@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(AspectJAutoProxyRegistrar.class)public @interface EnableAspectJAutoProxy { // true使用CGLIB做动态代理, false使用JDK动态代理 boolean proxyTargetClass() default false; // 是否暴露被代理的类 boolean exposeProxy() default false;} AspectJAutoProxyRegistrar通过实现ImportBeanDefinitionRegistrar接口，重写了registerBeanDefinitions()方法, 实现向将AnnotationAwareAspectJAutoProxyCreator类的定义加入beanDefinitionMap中, 为后续生成bean对象做准备 123456789101112131415161718192021class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { // 注册internalAutoProxyCreator AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class); if (enableAspectJAutoProxy != null) { if (enableAspectJAutoProxy.getBoolean(\"proxyTargetClass\")) { AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); } if (enableAspectJAutoProxy.getBoolean(\"exposeProxy\")) { AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); } } }} registerOrEscalateApcAsRequired()方法具体实现了加入BeanDefinitionMap的过程 在BeanDefinitionMap中存在 判断之前存在的与传入的BeanDefinition优先级,使用优先级高的那个 在BeanDefinitionMap中不存在 根据AnnotationAwareAspectJAutoProxyCreator.class创建新的BeanDefinition, 并设置先关属性 加入到BeanDefinitionMap中 12345678910111213141516171819202122232425262728293031323334353637383940414243public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry) { return registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry, null);}public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) { return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);}private static BeanDefinition registerOrEscalateApcAsRequired( Class&lt;?&gt; cls, BeanDefinitionRegistry registry, @Nullable Object source) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); // 查看注册中心是否存在internalAutoProxyCreator的定义 // internalAutoProxyCreator的类型是AnnotationAwareAspectJAutoProxyCreator, // 即beanName为internalAutoProxyCreator if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) { BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) { // 根据优先级选择使用哪个BeanDefinition int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority &lt; requiredPriority) { // 传进来的参数优先级更大, 修改注册的beanName, 使用传进来的internalAutoProxyCreator apcDefinition.setBeanClassName(cls.getName()); } } // internalAutoProxyCreator已存在, 不需要创建, 直接返回 return null; } //利用AnnotationAwareAspectJAutoProxyCreator的class类型新建RootBeanDefinition RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); //向beanDefinition对象中加入其它一些属性, 优先级, role, source beanDefinition.setSource(source); beanDefinition.getPropertyValues().add(\"order\", Ordered.HIGHEST_PRECEDENCE); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 将新定义的BeanDefinition注入到注册中心 registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition;} AnnotationAwareAspectJAutoProxyCreator继承关系分析AnnotationAwareAspectJAutoProxyCreator的继承关系如下 AnnotationAwareAspectJAutoProxyCreator实现了Ordered、BeanClassLoaderAware、BeanFactoryAware、BeanPostProcessor等接口 Ordered: 多个Aop的切面同时增强一个Bean对象时, 执行的先后顺序由Ordered实现 BeanClassLoaderAware: 获取ClassLoader BeanFactoryAware: 获取BeanFactory BeanPostProcessor: 为在Bean初始化前后执行另外的逻辑, 即可以通过BeanPostProcessor实现对bean对象的增强 在看一下AnnotationAwareAspectJAutoProxyCreator的父类AbstractAutoProxyCreator AbstractAutoProxyCreatorAbstractAutoProxyCreator中重写了BeanPostProcessor中的postProcessBeforeInitialization()与postProcessAfterInitialization(), 所以被AOP增强的bean在初始化的前后会调用这两个方法创建代理对象, 完成增强。其中postProcessBeforeInitialization()方法没有对bean进行处理,直接返回, 主要看postProcessAfterInitialization()方法 123456789101112@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); // 在解决循环依赖时, 可能会提前创建代理对象,为保证单例, 如果bean已经是代理对象,返回本身 if (this.earlyProxyReferences.remove(cacheKey) != bean) { // 判断当前bean是否可以创建对象, 可以则创建代理对象返回,不可以则返回当前bean return wrapIfNecessary(bean, beanName, cacheKey); } } return bean;} 1234567891011121314151617181920212223242526272829303132protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { // 如果已经处理过 if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) { return bean; } // 不需要增强 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) { return bean; } // 若bean是基础类型(Advice, Pointcut, Advisor, AopInfrastructureBean) // 或 配置了bean不需要代理 // 则返回bean本身 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // Create proxy if we have advice. // 如果存在增强方法(@Before, @After, @Around),则创建代理 Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); // 创建代理 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;}","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"Spring IOC源码分析","slug":"spring/Spring IOC源码分析","date":"2022-12-25T16:00:00.000Z","updated":"2023-07-17T13:41:27.764Z","comments":true,"path":"2022/12/26/spring/Spring IOC源码分析/","link":"","permalink":"https://yichenfirst.github.io/2022/12/26/spring/Spring%20IOC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"1、Bean的创建 实例化 填充属性 执行Aware回调 初始化前 初始化 初始化后（AOP） 将完整Bean放到单例池中 bean的创建过程从getBean开始 getBean(beanName) 此处会判断bean是否是FactoryBean，但最终都会调用getBean(beanName) doGetBean(name, null, null, false) getSingleton(beanName) getSingleton(beanName，()-&gt; {}) 从singletonObjects查找 无，则创建bean beforeSingletonCreation(beanName) singletonFactory.getObject() createBean(beanName, mbd, args) resolveBeanClass(mbd, beanName) mbdToUse.prepareMethodOverrides() resolveBeforeInstantiation(beanName, mbdToUse) 处理BeanPostProcessors doCreateBean(beanName, mbdToUse, args) createBeanInstance(beanName, mbd, args) 根据指定bean使用对应的策略创建实例，如工厂方法，无参构造函数，有参构造函数 instantiateUsingFactoryMethod(beanName, mbd, args) autowireConstructor(beanName, mbd, null, null) instantiateBean(beanName, mbd) applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName) 对于可能出现循环依赖的情况(单例 &amp;&amp; 允许循环依赖 &amp;&amp; 当前bean正在创建中) addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)) populateBean(beanName, mbd, instanceWrapper) applyPropertyValues(beanName, mbd, bw, pvs) valueResolver.resolveValueIfNecessary(pv, originalValue) resolveReference(argName, ref) this.beanFactory.getBean(resolvedName) 此处调用getBean(递归处理依赖) initializeBean(beanName, exposedObject, mbd) invokeAwareMethods(beanName, bean) applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName) invokeInitMethods(beanName, wrappedBean, mbd) 执行InitializingBean接口的afterPropertiesSet()方法 applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName) getSingleton(beanName, false) 从earlySingletonObjects中获取bean afterSingletonCreation(beanName) addSingleton(beanName, singletonObject) 有，则返回对应bean getObjectForBeanInstance(sharedInstance, name, beanName, mbd) 如果是普通Bean对象，返回Bean本身；如果是Factorybean，则返回FactoryBean创建的Bean对象 2、循环依赖1234567A { B b;}B { A a;} 实例化A—–&gt; A的普通对象a—-&gt;存入三级缓存中addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, A类型普通对象)) 填充属性b（a对象的b属性） —-&gt; 从单例池中查询（singletonObjects）—–&gt;创建B对象 实例化—-&gt;普通对象b—-&gt;存入三级缓存，addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, A类型普通对象)) 填充属性（b对象的a属性） —-&gt;从单例池中查询（singletonObjects） ​ —-&gt;查询二级缓存（earlySingletonObjects） ​ —-&gt;查询三级缓存(singletonFactories) ​ —-&gt;执行lambda表达式，() -&gt; getEarlyBeanReference(beanName, mbd, A类型普通对象) ​ —-&gt;AOP，返回A的代理对象（无AOP操作则返回普通对象）（此时已经拿到了需要获取的a对象） ​ —-&gt;将此对象存入二级缓存中，并移除三级缓存中的值 ​ —–&gt;将获取到的bean对象赋值给a属性 填充其他属性 添加到单例池中（singletonObjects），并从二三级缓存中删除 填充其他属性 AOP等 —&gt; A的代理对象 添加到单例池中（singletonObjects），并从二三级缓存中删除 12345678910111213141516getBean(beanName)doGetBean(name, null, null, false)getSingleton(beanName) // 从一二三级缓存中获取beangetSingleton(beanName, ()-&gt;{return createBean(beanName, mbd, args)})//执行了singletonFactory.getObject(); createBean(beanName, mbd, args) doCreateBean(beanName, mbdToUse, args) instanceWrapper = createBeanInstance(beanName, mbd, args) // 创建bean实例 Object bean = instanceWrapper.getWrappedInstance(); //得到未初始化的bean对象 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); //添加到三级缓存中 populateBean(beanName, mbd, instanceWrapper); //填充bean对象属性 applyPropertyValues(beanName, mbd, bw, pvs); valueResolver.resolveValueIfNecessary(pv, originalValue) resolveReference(Object argName, RuntimeBeanReference ref) this.beanFactory.getBean(resolvedName); //开始获取b对象的Bean实例 getBean(beanName) doGetBean(beanName, null, null, false) 1、三级缓存解决循环依赖问题的关键是什么？为什么通过提前暴露对象能解决？ 实例化与初始化分开操作，在中间过程给其他对象赋值的时候，并不是一个完整对象，而是吧半成品对象赋值给了其他对象 2、如果只使用一级缓存能不能解决问题？ 不能。在整个处理过程中，缓存中存放的是半成品和完整对象，如果只有一级缓存，那么半成品和完整对象都会放到一级缓存中，有可能在获取过程中获取到半成品对象，此时半成品对象是无法使用的，不能直接进行相关处理，因此需要把半成品和完整对象的存放空间分隔开来。 3、只使用二级缓存行不行？为什么需要三级缓存？ 使用三级缓存的本质是在于使用AOP","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"spring源码编译","slug":"spring/spring源码编译","date":"2022-12-02T16:00:00.000Z","updated":"2023-07-17T13:41:27.764Z","comments":true,"path":"2022/12/03/spring/spring源码编译/","link":"","permalink":"https://yichenfirst.github.io/2022/12/03/spring/spring%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/","excerpt":"","text":"1、在github上下载源码 下载地址 2、用idea打开spring-framework，等待 3、报错 123456789101112131415161718Build scan background action failed.org.gradle.process.internal.ExecException: Process 'command 'git'' finished with non-zero exit value 128 at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:409) at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:38) at org.gradle.process.internal.DefaultExecActionFactory.exec(DefaultExecActionFactory.java:145) at io.spring.ge.conventions.gradle.WorkingDirectoryProcessOperations.exec(WorkingDirectoryProcessOperations.java:45) at io.spring.ge.conventions.gradle.ProcessOperationsProcessRunner.run(ProcessOperationsProcessRunner.java:41) at io.spring.ge.conventions.core.BuildScanConventions.run(BuildScanConventions.java:166) at io.spring.ge.conventions.core.BuildScanConventions.addGitMetadata(BuildScanConventions.java:113) at io.spring.ge.conventions.gradle.GradleConfigurableBuildScan.lambda$background$0(GradleConfigurableBuildScan.java:104) at com.gradle.scan.plugin.internal.api.j.a(SourceFile:22) at com.gradle.scan.plugin.internal.api.k$a.a(SourceFile:112) at com.gradle.scan.plugin.internal.api.h.a(SourceFile:62) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 解决办法：注释build.gradle中id 'io.spring.ge.conventions' version '0.0.7' 4、重新构建项目 5、构建成功 6、测试，新建一个模块 导入spring-context模块 测试项目结构 SpringTest.java 12345678910111213package com.yichen;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class SpringTest { public static void main(String[] args) { ApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"people.xml\"); System.out.println(applicationContext.getBean(\"people\")); }} People.java 123456789101112131415161718192021222324252627282930313233package com.yichen.pojo;public class People { private String name; private String sex; private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; }} people.xml 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"people\" class=\"com.yichen.pojo.People\"&gt; &lt;property name=\"name\" value=\"zhangsan\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"1\"&gt;&lt;/property&gt; &lt;property name=\"sex\" value=\"zhangsan\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt;","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"java总结","slug":"面试/java总结","date":"2022-11-18T16:00:00.000Z","updated":"2023-07-17T13:41:27.794Z","comments":true,"path":"2022/11/19/面试/java总结/","link":"","permalink":"https://yichenfirst.github.io/2022/11/19/%E9%9D%A2%E8%AF%95/java%E6%80%BB%E7%BB%93/","excerpt":"","text":"数组下标为什么是从零开始的减少CPU运算 数组在内存中是一段连续的空间，并且支持随机访问。 访问数组a中第i号位置的元素时，需要找到的地址是base_address + i * data_type_size 如果下标从1开始，数组a中第i号位置的元素的地址就是base_address + （i - 1）* data_type_size 可以看出，下标从0开始，寻址的时候可以减少一次减法运算。 for和foreach有什么区别for与foreach都可以遍历数组/集合，不过for则在较复杂的循环中效率更高。 foreach不可以删除/修改集合元素，而for可以 foreach和for都可以修改元素里面的属性 foreach会引发并发修改异常ConcurrentModificationException foreach底层原理数组 12345678public class Test { public static void main(String[] args) { String[] strs= new String[3]; for(String str : strs){ System.out.println(str); } }} 编译后 1234567891011121314151617181920212223//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)// package sdu.edu.Test; public class Test { public Test() { } public static void main(String[] args) { String[] strs = new String[3]; String[] var2 = strs; int var3 = strs.length; for(int var4 = 0; var4 &lt; var3; ++var4) { String str = var2[var4]; System.out.println(str); } }} 集合 12345678910public class Test { public static void main(String[] args) { List&lt;Integer&gt; list = Collections.emptyList(); for(int a : list){ System.out.println(a); } } } 编译后 123456789101112131415public class Test { public Test() { } public static void main(String[] args) { List&lt;Integer&gt; list = Collections.emptyList(); Iterator var2 = list.iterator(); while(var2.hasNext()) { int a = (Integer)var2.next(); System.out.println(a); } }} String 类能不能被继承？为什么？不能，因为String类是被final修饰的类型，final类是不能被继承的，String类是不可变类型(Immutable)类。 1、字符串常量池的需要 字符串池（字符串内部池） 是在方法区域的特殊区域。当一个string被创建如果这个string已经在内存里面存在了，那个存在的string的引用被返回，而不是创建个新的对象和返回它的引用。下面的代码将在堆上创建一个string对象。 12String string1 = \"abcd\"; String string2 = \"abcd\"; 如果这个string是可以改变的，通过一个引用改变一个string将导致另一引用指向错误的值。 2、缓存哈希值在Java中，string的哈希值经常被用。举个例子，在HashMap中。保持不变，可以保证总是返回相同的哈希值。所以它可以被缓存而不用担心被改变。 这意味这不需要在使用的时候每次都计算哈希值。这将更高效。在String类中，它有以下的代码： 1private int hash;//this is used to cache hash code. 3、安全 网络连接地址URL,文件路径path通常情况下都是以String类型保存, 假若String不是固定不变的,将会引起各种安全隐患。就好比我们的密码不能以String的类型保存，，如果你将密码以明文的形式保存成字符串，那么它将一直留在内存中，直到垃圾收集器把它清除。而由于字符串被放在字符串缓冲池中以方便重复使用，所以它就可能在内存中被保留很长时间，而这将导致安全隐患 4、线程安全 因为不可变对象是不可以改变的，它能够被多个线程自由的共享。这消除了同步。总结，String被设计成不可以更改的是为了效率和安全。这也是为什么现在很多不可以改变的类。 == 和 equals() 的区别？ 对象类型不同，equals()是超类Object中的方法，==是操作符 比较的对象不同，equals用来检测两个对象是否相等，即两个对象的内容是否相等，==在基础数据类型比较的是值是否相等，引用数据类型比较的是引用地址是否相等。 运行速度不用，equals要慢与== 简述 Java 的反射机制及其应用场景动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。 在运行状态中，对于任意一个类，能够知道这个类的所有属性和方法。对于任意一个对象，能够调用它的任意一个方法和属性。 应用场景 JDBC连接数据库时使用 Class.forName() 通过反射加载数据库的驱动程序 Eclispe、IDEA等开发工具利用反射动态解析对象的类型与结构，动态提示对象的属性和方法 Web服务器中利用反射调用了Sevlet的 service 方法 JDK动态代理底层依赖反射实现 Java创建对象的方式123456789101112131415161718192021//1.通过new申请对象空间（堆内存）Person person = new Person();//2.通过Class类中的newInstance()方法Class clazz = Person.class;Person p = (Person) clazz.newInstance();//3.通过Constructor类中的newInstance()方法Class claqq = Person.class;Constructor con = claqq.getConstructor(String.class);Person per = (Person) con.newInstance(\"zzt\");//4.通过Object类中的clone()方法//类对象，实现实现Cloneable接口//不会执行构造方法//设计模式 ------&gt;Prototype原型模式//5.通过对象的反序列化FileInputStream fileInputStream = new FileInputStream(\"path\");ObjectInputStream ois = new ObjectInputStream(fileInputStream);Person person1 = (Person) ois.readObject(); String最大长度是多少？String类提供了一个length方法，返回值为int类型，而int的取值上限为2^31 -1。所以理论上String的最大长度为2^31 -1。 达到这个长度的话需要多大的内存吗？ String内部是使用一个char数组来维护字符序列的，一个char占用两个字节。如果说String最大长度是 2^31 -1的话，那么最大的字符串占用内存空间约等于4GB。 也就是说，我们需要有大于4GB的JVM运行内存才行。 那String一般都存储在JVM的哪块区域呢？ 字符串在JVM中的存储分两种情况，一种是String对象，存储在JVM的堆栈中。一种是字符串常量，存储在常量池里面。 什么情况下字符串会存储在常量池呢？ 当通过字面量进行字符串声明时，比如String s = \"hello, wold\"，这个字符串在编译之后会以常量的形式进入到常量池。 那常量池中的字符串最大长度是2^31-1吗？ 不是的，常量池对String的长度是有另外限制的。。Java中的UTF-8编码的Unicode字符串在常量池中以 CONSTANT_Utf8类型表示。 String在不同的状态下，具有不同的长度限制。 字符串常量长度不能超过2^16 - 1 堆内字符串的长度不超过2^31 - 1 什么是字符串常量池？字符串常量池（String Pool）保存着所有字符串字面量，这些字面量在编译时期就确定。字符串常量池位于堆内存中，专门用来存储字符串常量。在创建字符串时，JVM首先会检查字符串常量池，如果该字符串已经存在池中，则返回其引用，如果不存在，则创建此字符串并放入池中，并返回其引用。 String，StringBuffer，StringBuilder 之间有什么区别？String：不可变，线程安全 StringBuilder：可变，线程不安全 StringBuffer：可变，线程安全 什么是内存泄漏，怎么确定内存泄漏？内存泄漏：对系统申请内存使用，将其内存分配给对象使用，但内存空间使用完毕后未释放，一直占用内存空间。（长期的堆积，内存迟早被耗尽，所以今早的解决内存泄漏无疑是好事） 检测内存泄露 使用Java VisualVM 简述 Java 中的自动装箱与拆箱 自动装箱是 Java 编译器在基本类型和它们对应的包装类之间进行的自动转换。例如，将 int 类型转换为 Integer 类型，将 double 类型转换为 Double 型等等。如果转换以相反的方式进行，则称为开箱。 Integer→int，实际上是调用了方法Integer.intValue()。 当我们用Integer i = xxx创建对象时，会发生自动装箱，调用Integer.valueOf() hashcode 和 equals 方法的联系1）如果两个对象需要相等（equals），那么它们必须有着相同的哈希码（hashCode）； 2）但如果两个对象有着相同的哈希码，它们却不一定相等。 Java 中接口和抽象类的区别1、语法层面上的区别 抽象类可以有方法实现，而接口的方法中只能是抽象方法（Java 8 之后接口方法可以有默认实现）； 抽象类中的成员变量可以是各种类型的，接口中的成员变量只能是public static final类型； 接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法（Java 8之后接口可以有静态方法）； 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2、设计层面上的区别 抽象层次不同。 抽象类是对整个类整体进行抽象，包括属性、行为，但是接口只是对类行为进行抽象。继承抽象类是一种”是不是”的关系，而接口实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是具备不具备的关系，比如鸟是否能飞。 继承抽象类的是具有相似特点的类，而实现接口的却可以不同的类。 Java 缓冲流 buffer 的用途和原理是什么？缓冲流也称为处理流，对文件或者其他目标频繁的操作，效率低，性能差。缓冲流目的是提高程序读取和写出的性能。缓冲流也分为字节缓冲流（如FileInputStream与FileOutputStream）和字符缓冲流（如FileReader与FileWriter） 而缓冲流的基本原理,是创建流对象时候,会创建一个内置的默认大小的缓冲区数组, 通过缓冲区读写使得性能大大提升。 简述 Java 内置排序算法的实现原理Collections.sort方法底层就是调用的Arrays.sort方法，所以我们只分析Arrays.sort就好。 Arrays.sort(int[])方法来概述基本类型排序的基本思路如下： 如果数组元素个数小于47个,那么使用改进的插入排序进行排序 如果元素个数大于47个并且小于快速排序的阈值286个，则使用双轴快速排序进行排序 如果元素个数大于286个，根据数组的无序程度来判定继续使用哪种算法，无序程度通过将数组划分为不同的有序序列的个数来判定。 如果有序序列的个数大于67个,则认为原数组基本无序,则仍然使用双轴快速排序，如果小于67个，则认为原数组基本有序，使用归并排序进行排序。 也就是说，划分出有序序列个数越多，其实原数组是越无序的。这里理解不了的同学，可以往极限想一下。一个完全有序的数组，只能划分出1个有序数组。而一个完全逆序的数组，数组size有多大，就能划分出多少个有序数组。 若果是Arrays.sort(Object object)，会涉及稳定性的问题。因为快排是是不稳定的，所以当数组个数超过插入排序的阈值后，不会使用快排，而是使用归并排序。 这里的稳定是指比较相等的数据在排序之后仍然按照排序之前的前后顺序排列。 Java 如何高效进行数组拷贝 使用 Arrays.copyOf或 System.arraycopy. 本质前者是基于后者的, 两者性能相当, 前者多了2次native调用获取数组元素类型和创建数组 什么是重写和重载？重载(Overload)：（1）Overloading是一个类中多态性的一种表现，让类以统一的方式处理不同类型数据的一种手段。多个同名函数同时存在，具有不同的参数个数/类型。（2）重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同，也可以不相同。无法以返回型别作为重载函数的区分标准。 重写（Override):（1） 父类与子类之间的多态性，对父类的函数进行重新定义。即在子类中定义某方法与其父类有相同的名称和参数。（2）若子类中的方法与父类中的某一方法具有相同的方法名、返回类型和参数表，则新方法将覆盖原有的方法。如需父类中原有的方法，可使用super关键字，该关键字引用了当前类的父类。 简述 Java 中 final 关键字的作用inal的意思是最终，不可变的。是一个修饰符，可用来修饰类、类的成员以及局部变量； final修饰的类，该类不能被继承 被final修饰的类，final类中的成员变量可以根据自己的实际需要设计为final； 被final修饰的类，final类中的成员方法都会被隐式的指定为final方法； 在JDK中，被设计为final类的有String、System。 被final修饰的方法不能被重写 注：类的private方法会隐式地被指定为final方法。 final修饰的变量叫常量，常量必须初始化，初始化之后就不能被修改。 简述 Java 的序列化和使用场景序列化：指把堆内存中的 Java 对象数据，通过某种方式把对象存储到磁盘文件中或者传递给其他网络节点（在网络上传输）。这个过程称为序列化。通俗来说就是将数据结构或对象转换成二进制数据流的过程 反序列化：把磁盘文件中的对象数据或者把网络节点上的对象数据，恢复成Java对象模型的过程。也就是将在序列化过程中所生成的二进制数据流转换成数据结构或者对象的过程 使用场景 ①、在分布式系统中，此时需要把对象在网络上传输，就得把对象数据转换为二进制形式，需要共享的数据的 JavaBean 对象，都得做序列化。 ②、服务器钝化：如果服务器发现某些对象好久没活动了，那么服务器就会把这些内存中的对象持久化在本地磁盘文件中（Java对象转换为二进制文件）；如果服务器发现某些对象需要活动时，先去内存中寻找，找不到再去磁盘文件中反序列化我们的对象数据，恢复成 Java 对象。这样能节省服务器内存。 序列化的好处 描述数据的传输格式，这样可以方便自己组织数据传输格式，以至于避免一些麻烦及错误 如果是跨平台的序列化，则发送方序列化后，接收方可以用任何其支持的平台反序列化成相应的版本，比如Java序列化后， 用.net、phython等反序列化 利用序列化实现远程通信，即在网络上传送对象的字节序列。 为了解决对象流读写操作时可能引发的问题(如果不进行序列化,可能会存在数据乱序的问题) 实现了数据的持久化，通过序列化可以把数据永久地保存到硬盘上（通常存放在文件里）， 序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆 new Integer 和 Integer.valueOf 的区别是什么？（1）不管是new创建的Integer对象，还是通过直接赋值Int值创建的Integer对象，它们与Int类型变量通过“==”进行比较时都会自动拆箱变成Int类型，所以Integer对象和Int变量比较的是内容大小。 123456789public class Main { public static void main(String[] args) { int a = 100; Integer b = 100;//等价于b=Integer.valueOf(100); Integer c = new Integer(100); System.out.println(a == b);//true System.out.println(a == c);//true }} （2）new创建的Integer对象和直接赋Int值创建的Integer对象使用==比较的是它们的内存地址。 1234567public class Main { public static void main(String[] args) { Integer b = 100;//等价于b=Integer.valueOf(100); Integer c = new Integer(100); System.out.println(b == c);//false }} （3）赋Int值创建的Integer对象比较：当Int值在-128-127之间时，两个Integer变量引用的是IntegerCache中的同一个对象，内存地址相同，因此的 == ,结果为true；当Int值不在以上范围时，两个Integer对象都是通过new创建的，内存地址不同，因此 == 的结果为false 12345678910111213public class Main { public static void main(String[] args) { Integer a = 100; Integer b = 100; Integer c = 200; Integer d = 200; Integer f = new Integer(100); System.out.println(a == b);//true System.out.println(c == d);//false System.out.println(a == f);//false }} 经典面试题 123456789101112public class Main { public static void main(String[] args) { Integer a = 49; int b = 49; Integer c = Integer.valueOf(49); Integer d = new Integer(49); System.out.println(a == b);//true System.out.println(a == c);//true System.out.println(b == c);//true System.out.println(c == d);//false }} Java 有几种基本数据类型，分别占多少字节？ boolean 1 bit，不到一个字节 byte 8 bit，1字节 short 16 bit，2字节 char 16 bit，2字节 int 32 bit，4字节 float 32 bit，4字节 long 64 bit，8字节 double 64 bit，8字节 有哪些解决哈希表冲突的方式？1、开放定址法：我们在遇到哈希冲突时，去寻找一个新的空闲的哈希地址。 （1）线性探测法 （2）平方探测法 2、再哈希法：同时构造多个不同的哈希函数，等发生哈希冲突时就使用第二个、第三个……等其他的哈希函数计算地址，直到不发生冲突为止。虽然不易发生聚集，但是增加了计算时间。 3、链地址法：将所有哈希地址相同的记录都链接在同一链表中。 简述封装、继承、多态的特性及使用场面向对象三大特性：封装，继承，多态 1、封装就是将类的信息隐藏在类内部，不允许外部程序直接访问，而是通过该类的方法实现对隐藏信息 的操作和访问。 良好的封装能够减少耦合。 2、继承是从已有的类中派生出新的类，新的类继承父类的属性和行为，并能扩展新的能力，大大增加程序的重用性和易维护性。在Java中是单继承的，也就是说一个子类只有一个父类。 3、多态是同一个行为具有多个不同表现形式的能力。在不修改程序代码的情况下改变程序运行时绑定的代码。实现多态的三要素：继承、重写、父类引用指向子类对象。 静态多态性：通过重载实现，相同的方法有不同的參数列表，可以根据参数的不同，做出不同的处 理。 动态多态性：在子类中重写父类的方法。运行期间判断所引用对象的实际类型，根据其实际类型调 用相应的方法。 Java 中 int 的最大值是多少？java int 类整数的最大值是 2 的 31 次方 - 1 = 2147483648 - 1 = 2147483647(21亿多) Java 异常有哪些类型？ 常见的RuntimeException： ClassCastException //类型转换异常 IndexOutOfBoundsException //数组越界异常 NullPointerException //空指针 ArrayStoreException //数组存储异常 NumberFormatException //数字格式化异常 ArithmeticException //数学运算异常 checked Exception： NoSuchFieldException //反射异常，没有对应的字段 ClassNotFoundException //类没有找到异常 IllegalAccessException //安全权限异常，可能是反射时调用了private方法","categories":[{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"},{"name":"总结","slug":"总结","permalink":"https://yichenfirst.github.io/tags/%E6%80%BB%E7%BB%93/"},{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Docker笔记","slug":"docker/Docker笔记","date":"2022-11-04T16:00:00.000Z","updated":"2023-07-17T13:41:27.804Z","comments":true,"path":"2022/11/05/docker/Docker笔记/","link":"","permalink":"https://yichenfirst.github.io/2022/11/05/docker/Docker%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Docker 官方文档地址:https://www.docker.com/get-started 中文参考手册:https://docker_practice.gitee.io/zh-cn/ 1.什么是 Docker1.1 官方定义 最新官网首页 1234# 1.官方介绍- We have a complete container solution for you - no matter who you are and where you are on your containerization journey.- 翻译: 我们为你提供了一个完整的容器解决方案,不管你是谁,不管你在哪,你都可以开始容器的的旅程。- 官方定义: docker是一个容器技术。 1.2 Docker的起源12345Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目 已经超过 5 万 7 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 2.为什么是Docker 在开发的时候，在本机测试环境可以跑，生产环境跑不起来 这里我们拿java Web应用程序举例，我们一个java Web应用程序涉及很多东西，比如jdk、tomcat、mysql等软件环境。当这些其中某一项版本不一致的时候，可能就会导致应用程序跑不起来这种情况。Docker则将程序以及使用软件环境直接打包在一起，无论在那个机器上保证了环境一致。 优势1: 一致的运行环境,更轻松的迁移 服务器自己的程序挂了，结果发现是别人程序出了问题把内存吃完了，自己程序因为内存不够就挂了 这种也是一种比较常见的情况，如果你的程序重要性不是特别高的话，公司基本上不可能让你的程序独享一台服务器的，这时候你的服务器就会跟公司其他人的程序共享一台服务器，所以不可避免地就会受到其他程序的干扰，导致自己的程序出现问题。Docker就很好解决了环境隔离的问题，别人程序不会影响到自己的程序。 优势2：对进程进行封装隔离,容器与容器之间互不影响,更高效的利用系统资源 公司要弄一个活动，可能会有大量的流量进来，公司需要再多部署几十台服务器 在没有Docker的情况下，要在几天内部署几十台服务器，这对运维来说是一件非常折磨人的事，而且每台服务器的环境还不一定一样，就会出现各种问题，最后部署地头皮发麻。用Docker的话，我只需要将程序打包到镜像，你要多少台服务，我就给力跑多少容器，极大地提高了部署效率。 优势3: 通过镜像复制N多个环境一致容器 3.Docker和虚拟机区别 关于Docker与虚拟机的区别，我在网上找到的一张图，非常直观形象地展示出来，话不多说，直接上图。 比较上面两张图，我们发现虚拟机是携带操作系统，本身很小的应用程序却因为携带了操作系统而变得非常大，很笨重。Docker是不携带操作系统的，所以Docker的应用就非常的轻巧。另外在调用宿主机的CPU、磁盘等等这些资源的时候，拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机 Docker容器 磁盘占用 几个GB到几十个GB左右 几十MB到几百MB左右 CPU内存占用 虚拟操作系统非常占用CPU和内存 Docker引擎占用极低 启动速度 （从开机到运行项目）几分钟 （从开启容器到运行项目）几秒 安装管理 需要专门的运维技术 安装、管理方便 应用部署 每次部署都费时费力 从第二次部署开始轻松简捷 耦合性 多个应用服务安装到一起，容易互相影响 每个应用服务一个容器，达成隔离 系统依赖 无 需求相同或相似的内核，目前推荐是Linux 4.Docker的安装4.1 安装docker(centos7.x) 卸载原始docker 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装docker依赖 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 设置docker的yum源 123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装最新版的docker 1$ sudo yum install docker-ce docker-ce-cli containerd.io 指定版本安装docker 123$ yum list docker-ce --showduplicates | sort -r$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io$ sudo yum install docker-ce-18.09.5-3.el7 docker-ce-cli-18.09.5-3.el7 containerd.io 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 关闭docker 1$ sudo systemctl stop docker 测试docker安装 1$ sudo docker run hello-world 4.2 bash安装(通用所有平台) 在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装：执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 的稳定(stable)版本安装在系统中。 12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 创建docker用户组 1$ sudo groupadd docker 将当前用户加入docker组 1$ sudo usermod -aG docker $USER 测试docker安装是否正确 1$ docker run hello-world 5.Docker 的核心架构 镜像: 一个镜像代表一个应用环境,他是一个只读的文件,如 mysql镜像,tomcat镜像,nginx镜像等 容器: 镜像每次运行之后就是产生一个容器,就是正在运行的镜像,特点就是可读可写 仓库:用来存放镜像的位置,类似于maven仓库,也是镜像下载和上传的位置 dockerFile:docker生成镜像配置文件,用来书写自定义镜像的一些配置 tar:一个对镜像打包的文件,日后可以还原成镜像 6. Docker 配置阿里镜像加速服务6.1 docker 运行流程 6.2 docker配置阿里云镜像加速 访问阿里云登录自己账号查看docker镜像加速服务 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ \"registry-mirrors\": [\"https://lz2nib3q.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker 验证docker的镜像加速是否生效 1234567[root@localhost ~]# docker info .......... 127.0.0.0/8 Registry Mirrors: 'https://lz2nib3q.mirror.aliyuncs.com/' Live Restore Enabled: false Product License: Community Engine 7.Docker的入门应用7.1 docker 的第一个程序 docker run hello-world 12345678910111213141516171819202122[root@localhost ~]# docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 8.常用命令6.1 辅助命令1234# 1.安装完成辅助命令 docker version -------------------------- 查看docker的信息 docker info -------------------------- 查看更详细的信息 docker --help -------------------------- 帮助命令 6.2 Images 镜像命令12345678910111213141516# 1.查看本机中所有镜像 docker images -------------------------- 列出本地所有镜像 -a 列出所有镜像（包含中间映像层） -q 只显示镜像id# 2.搜索镜像 docker search [options] 镜像名 ------------------- 去dockerhub上查询当前镜像 -s 指定值 列出收藏数不少于指定值的镜像 --no-trunc 显示完整的镜像信息# 3.从仓库下载镜像 docker pull 镜像名[:TAG|@DIGEST] ----------------- 下载镜像# 4.删除镜像 docker rmi 镜像名 -------------------------- 删除镜像 -f 强制删除 6.3 Contrainer 容器命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 1.运行容器 docker run 镜像名 -------------------------- 镜像名新建并启动容器 --name 别名为容器起一个名字 -d 启动守护式容器（在后台启动容器） -p 映射端口号：原始端口号 指定端口号启动 例：docker run -it --name myTomcat -p 8888:8080 tomcat docker run -d --name myTomcat -P tomcat# 2.查看运行的容器 docker ps -------------------------- 列出所有正在运行的容器 -a 正在运行的和历史运行过的容器 -q 静默模式，只显示容器编号# 3.停止|关闭|重启容器 docker start 容器名字或者容器id --------------- 开启容器 docker restart 容器名或者容器id --------------- 重启容器 docker stop 容器名或者容器id ------------------ 正常停止容器运行 docker kill 容器名或者容器id ------------------ 立即停止容器运行# 4.删除容器 docker rm -f 容器id和容器名 docker rm -f $(docker ps -aq) -------------------------- 删除所有容器# 5.查看容器内进程 docker top 容器id或者容器名 ------------------ 查看容器内的进程# 6.查看查看容器内部细节 docker inspect 容器id ------------------ 查看容器内部细节# 7.查看容器的运行日志 docker logs [OPTIONS] 容器id或容器名 ------------------ 查看容器日志 -t 加入时间戳 -f 跟随最新的日志打印 --tail 数字 显示最后多少条# 8.进入容器内部 docker exec [options] 容器id 容器内命令 ------------------ 进入容器执行命令 -i 以交互模式运行容器，通常与-t一起使用 -t 分配一个伪终端 shell窗口 bash # 9.容器和宿主机之间复制文件 docker cp 文件|目录 容器id:容器路径 ----------------- 将宿主机复制到容器内部 docker cp 容器id:容器内资源路径 宿主机目录路径 ----------------- 将容器内资源拷贝到主机上# 10.数据卷(volum)实现与宿主机共享目录 docker run -v 宿主机的路径|任意别名:/容器内的路径 镜像名 注意: 1.如果是宿主机路径必须是绝对路径,宿主机目录会覆盖容器内目录内容 2.如果是别名则会在docker运行容器时自动在宿主机中创建一个目录,并将容器目录文件复制到宿主机中# 11.打包镜像 docker save 镜像名 -o 名称.tar# 12.载入镜像 docker load -i 名称.tar# 13.容器打包成新的镜像 docker commit -m \"描述信息\" -a \"作者信息\" （容器id或者名称）打包的镜像名称:标签 7.docker的镜像原理7.1 镜像是什么？ 镜像是一种轻量级的，可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时所需的库、环境变量和配置文件。 7.2 为什么一个镜像会那么大？镜像就是花卷 UnionFS（联合文件系统）: Union文件系统是一种分层，轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。Union文件系统是Docker镜像的基础。这种文件系统特性:就是一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 。 7.3 Docker镜像原理 docker的镜像实际是由一层一层的文件系统组成。 bootfs（boot file system）主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。在docker镜像的最底层就是bootfs。这一层与Linux/Unix 系统是一样的，包含boot加载器（bootloader）和内核（kernel）。当boot加载完,后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时会卸载bootfs。 rootfs（root file system），在bootfs之上，包含的就是典型的linux系统中的/dev，/proc，/bin，/etc等标准的目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu/CentOS等等。 我们平时安装进虚拟机的centos都有1到几个GB，为什么docker这里才200MB？对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令，工具，和程序库就可以了，因为底层直接使用Host的Kernal，自己只需要提供rootfs就行了。由此可见不同的linux发行版，他们的bootfs是一致的，rootfs会有差别。因此不同的发行版可以共用bootfs。 7.4 为什么docker镜像要采用这种分层结构呢? 最大的一个好处就是资源共享 比如：有多个镜像都是从相同的base镜像构建而来的，那么宿主机只需在磁盘中保存一份base镜像。同时内存中也只需要加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。Docker镜像都是只读的。当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称为容器层，容器层之下都叫镜像层。 8.Docker安装常用服务8.1 安装mysql1234567891011121314151617181920212223242526272829303132333435363738394041# 1.拉取mysql镜像到本地 docker pull mysql:tag (tag不加默认最新版本) # 2.运行mysql服务 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:tag --没有暴露外部端口外部不能连接 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag --没有暴露外部端口# 3.进入mysql容器 docker exec -it 容器名称|容器id bash# 4.外部查看mysql日志 docker logs 容器名称|容器id# 5.使用自定义配置参数 docker run --name mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -d mysql:tag# 6.将容器数据位置与宿主机位置挂载保证数据安全 docker run --name mysql -v /root/mysql/data:/var/lib/mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag# 7.通过其他客户端访问 如在window系统|macos系统使用客户端工具访问 # 8.将mysql数据库备份为sql文件 docker exec mysql|容器id sh -c 'exec mysqldump --all-databases -uroot -p\"$MYSQL_ROOT_PASSWORD\"' &gt; /root/all-databases.sql --导出全部数据 docker exec mysql sh -c 'exec mysqldump --databases 库表 -uroot -p\"$MYSQL_ROOT_PASSWORD\"' &gt; /root/all-databases.sql --导出指定库数据 docker exec mysql sh -c 'exec mysqldump --no-data --databases 库表 -uroot -p\"$MYSQL_ROOT_PASSWORD\"' &gt; /root/all-databases.sql --导出指定库数据不要数据# 9.执行sql文件到mysql中 docker exec -i mysql sh -c 'exec mysql -uroot -p\"$MYSQL_ROOT_PASSWORD\"' &lt; /root/xxx.sql docker run \\--name mysql \\-d \\-p 3306:3306 \\--restart unless-stopped \\-v /usr/local/mysql/log:/var/log/mysql \\-v /usr/local/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=custhitachi23-- \\mysql:5.7 8.2 安装Redis服务12345678910111213141516171819202122232425262728# 1.在docker hub搜索redis镜像 docker search redis# 2.拉取redis镜像到本地 docker pull redis# 3.启动redis服务运行容器 docker run --name redis -d redis:tag (没有暴露外部端口) docker run --name redis -p 6379:6379 -d redis:tag (暴露外部宿主机端口为6379进行连接) # 4.查看启动日志 docker logs -t -f 容器id|容器名称# 5.进入容器内部查看 docker exec -it 容器id|名称 bash # 6.加载外部自定义配置启动redis容器 默认情况下redis官方镜像中没有redis.conf配置文件 需要去官网下载指定版本的配置文件 1. wget http://download.redis.io/releases/redis-5.0.8.tar.gz 下载官方安装包 2. 将官方安装包中配置文件进行复制到宿主机指定目录中如 /root/redis/redis.conf文件 3. 修改需要自定义的配置 bind 0.0.0.0 开启远程权限 appenonly yes 开启aof持久化 4. 加载配置启动 docker run --name redis -v /root/redis:/usr/local/etc/redis -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf # 7.将数据目录挂在到本地保证数据安全 docker run --name redis -v /root/redis/data:/data -v /root/redis/redis.conf:/usr/local/etc/redis/redis.conf -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf 8.3 安装Nginx123456789101112131415161718192021222324252627# 1.在docker hub搜索nginx docker search nginx# 2.拉取nginx镜像到本地 [root@localhost ~]# docker pull nginx Using default tag: latest latest: Pulling from library/nginx afb6ec6fdc1c: Pull complete b90c53a0b692: Pull complete 11fa52a0fdc0: Pull complete Digest: sha256:30dfa439718a17baafefadf16c5e7c9d0a1cde97b4fd84f63b69e13513be7097 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest# 3.启动nginx容器 docker run -p 80:80 --name nginx01 -d nginx# 4.进入容器 docker exec -it nginx01 /bin/bash 查找目录: whereis nginx 配置文件: /etc/nginx/nginx.conf# 5.复制配置文件到宿主机 docker cp nginx01(容器id|容器名称):/etc/nginx/nginx.conf 宿主机名录# 6.挂在nginx配置以及html到宿主机外部 docker run --name nginx02 -v /root/nginx/nginx.conf:/etc/nginx/nginx.conf -v /root/nginx/html:/usr/share/nginx/html -p 80:80 -d nginx 8.4 安装Tomcat123456789101112131415# 1.在docker hub搜索tomcat docker search tomcat# 2.下载tomcat镜像 docker pull tomcat# 3.运行tomcat镜像 docker run -p 8080:8080 -d --name mytomcat tomcat# 4.进入tomcat容器 docker exec -it mytomcat /bin/bash# 5.将webapps目录挂载在外部 docker run -p 8080:8080 -v /root/webapps:/usr/local/tomcat/webapps -d --name mytomcat tomcat 8.5 安装MongoDB数据库12345678910111213141516171819# 1.运行mongDB docker run -d -p 27017:27017 --name mymongo mongo ---无须权限 docker logs -f mymongo --查看mongo运行日志# 2.进入mongodb容器 docker exec -it mymongo /bin/bash 直接执行mongo命令进行操作# 3.常见具有权限的容器 docker run --name mymongo -p 27017:27017 -d mongo --auth# 4.进入容器配置用户名密码 mongo use admin 选择admin库 db.createUser({user:\"root\",pwd:\"root\",roles:[{role:'root',db:'admin'}]}) //创建用户,此用户创建成功,则后续操作都需要用户认证 exit# 5.将mongoDB中数据目录映射到宿主机中 docker run -d -p 27017:27017 -v /root/mongo/data:/data/db --name mymongo mongo 8.6 安装ElasticSearch 注意:调高JVM线程数限制数量 0.拉取镜像运行elasticsearch123456# 1.dockerhub 拉取镜像 docker pull elasticsearch:6.4.2# 2.查看docker镜像 docker images# 3.运行docker镜像 docker run -p 9200:9200 -p 9300:9300 elasticsearch:6.4.2 启动出现如下错误 1. 预先配置12345678# 1.在centos虚拟机中，修改配置sysctl.conf vim /etc/sysctl.conf# 2.加入如下配置 vm.max_map_count=262144 # 3.启用配置 sysctl -p 注：这一步是为了防止启动容器时，报出如下错误： bootstrap checks failed max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 2.启动EleasticSearch容器1234# 0.复制容器中data目录到宿主机中 docker cp 容器id:/usr/share/share/elasticsearch/data /root/es# 1.运行ES容器 指定jvm内存大小并指定ik分词器位置 docker run -d --name es -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=\"-Xms128m -Xmx128m\" -v /root/es/plugins:/usr/share/elasticsearch/plugins -v /root/es/data:/usr/share/elasticsearch/data elasticsearch:6.4.2 3.安装IK分词器123456789101112131415161718192021222324252627# 1.下载对应版本的IK分词器 wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.4.2/elasticsearch-analysis-ik-6.4.2.zip# 2.解压到plugins文件夹中 yum install -y unzip unzip -d ik elasticsearch-analysis-ik-6.4.2.zip# 3.添加自定义扩展词和停用词 cd plugins/elasticsearch/config vim IKAnalyzer.cfg.xml &lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=\"ext_dict\"&gt;ext_dict.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=\"ext_stopwords\"&gt;ext_stopwords.dic&lt;/entry&gt; &lt;/properties&gt;# 4.在ik分词器目录下config目录中创建ext_dict.dic文件 编码一定要为UTF-8才能生效 vim ext_dict.dic 加入扩展词即可# 5. 在ik分词器目录下config目录中创建ext_stopword.dic文件 vim ext_stopwords.dic 加入停用词即可# 6.重启容器生效 docker restart 容器id# 7.将此容器提交成为一个新的镜像 docker commit -a=\"xiaochen\" -m=\"es with IKAnalyzer\" 容器id xiaochen/elasticsearch:6.4.2 4. 安装Kibana12345# 1.下载kibana镜像到本地 docker pull kibana:6.4.2# 2.启动kibana容器 docker run -d --name kibana -e ELASTICSEARCH_URL=http://10.15.0.3:9200 -p 5601:5601 kibana:6.4.2 Docker中出现如下错误解决方案12[root@localhost ~]# docker search mysql 或者 docker pull 这些命令无法使用Error response from daemon: Get https://index.docker.io/v1/search?q=mysql&amp;n=25: x509: certificate has expired or is not yet valid 注意:这个错误的原因在于是系统的时间和docker hub时间不一致,需要做系统时间与网络时间同步 1234567# 1.安装时间同步 sudo yum -y install ntp ntpdate# 2.同步时间 sudo ntpdate cn.pool.ntp.org# 3.查看本机时间 date# 4.从新测试 9.Dockerfile9.1 什么是DockerfileDockerfile可以认为是Docker镜像的描述文件，是由一系列命令和参数构成的脚本。主要作用是用来构建docker镜像的构建文件。 通过架构图可以看出通过DockerFile可以直接构建镜像 9.2 Dockerfile解析过程 9.3 Dockerfile的保留命令官方说明:https://docs.docker.com/engine/reference/builder/ 保留字 作用 FROM 当前镜像是基于哪个镜像的 第一个指令必须是FROM MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 构建镜像时需要运行的指令 EXPOSE 当前容器对外暴露出的端口号 WORKDIR 指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar包 COPY 类似于ADD，拷贝文件和目录到镜像中将从构建上下文目录中&lt;原路径&gt;的文件/目录复制到新的一层的镜像内的&lt;目标路径&gt;位置 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被docker run之后的参数替换 ENTRYPOINT 指定一个容器启动时要运行的命令ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及其参数 9.3.1 FROM 命令 基于那个镜像进行构建新的镜像,在构建时会自动从docker hub拉取base镜像 必须作为Dockerfile的第一个指令出现 语法: 123FROM &lt;image&gt;FROM &lt;image&gt;[:&lt;tag&gt;] 使用版本不写为latestFROM &lt;image&gt;[@&lt;digest&gt;] 使用摘要 9.3.2 MAINTAINER 命令 镜像维护者的姓名和邮箱地址[废弃] 语法: 1MAINTAINER &lt;name&gt; 9.3.3 RUN 命令 RUN指令将在当前映像之上的新层中执行任何命令并提交结果。生成的提交映像将用于Dockerfile中的下一步 语法: 12345RUN &lt;command&gt; (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)RUN echo helloRUN [\"executable\", \"param1\", \"param2\"] (exec form)RUN [\"/bin/bash\", \"-c\", \"echo hello\"] 9.3.4 EXPOSE 命令 用来指定构建的镜像在运行为容器时对外暴露的端口 语法: 12EXPOSE 80/tcp 如果没有显示指定则默认暴露都是tcpEXPOSE 80/udp 9.3.5 CMD 命令 用来为启动的容器指定执行的命令,在Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 注意: Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 语法: 123CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)CMD command param1 param2 (shell form) 9.3.6 WORKDIR 命令 用来为Dockerfile中的任何RUN、CMD、ENTRYPOINT、COPY和ADD指令设置工作目录。如果WORKDIR不存在，即使它没有在任何后续Dockerfile指令中使用，它也将被创建。 语法: 123456WORKDIR /path/to/workdirWORKDIR /aWORKDIR bWORKDIR c`注意:WORKDIR指令可以在Dockerfile中多次使用。如果提供了相对路径，则该路径将与先前WORKDIR指令的路径相对` 9.3.7 ENV 命令 用来为构建镜像设置环境变量。这个值将出现在构建阶段中所有后续指令的环境中。 语法： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... 9.3.8 ADD 命令 用来从context上下文复制新文件、目录或远程文件url，并将它们添加到位于指定路径的映像文件系统中。 语法: 12345ADD hom* /mydir/ 通配符添加多个文件ADD hom?.txt /mydir/ 通配符添加ADD test.txt relativeDir/ 可以指定相对路径ADD test.txt /absoluteDir/ 也可以指定绝对路径ADD url 9.3.9 COPY 命令 用来将context目录中指定文件复制到镜像的指定目录中 语法: 12COPY src destCOPY [\"&lt;src&gt;\",... \"&lt;dest&gt;\"] 9.3.10 VOLUME 命令 用来定义容器运行时可以挂在到宿主机的目录 语法: 1VOLUME [\"/data\"] 9.3.11 ENTRYPOINT命令 用来指定容器启动时执行命令和CMD类似 语法: 12 [\"executable\", \"param1\", \"param2\"]ENTRYPOINT command param1 param2 ENTRYPOINT指令，往往用于设置容器启动后的第一个命令，这对一个容器来说往往是固定的。CMD指令，往往用于设置容器启动的第一个命令的默认参数，这对一个容器来说可以是变化的。 9.3.11 ENTRYPOINT命令9.4 Dockerfile构建springboot项目部署1.准备springboot可运行项目 2.将可运行项目放入linux虚拟机中 3.编写Dockerfile123456FROM openjdk:8WORKDIR /emsADD ems.jar /emsEXPOSE 8989ENTRYPOINT [\"java\",\"-jar\"]CMD [\"ems.jar\"] 4.构建镜像1[root@localhost ems]# docker build -t ems . 5.运行镜像1[root@localhost ems]# docker run -p 8989:8989 ems 6.访问项目1http://10.15.0.8:8989/ems/login.html 10.高级网络配置10.1 说明当 Docker 启动时，会自动在主机上创建一个 docker0 虚拟网桥，实际上是 Linux 的一个 bridge，可以理解为一个软件交换机。它会在挂载到它的网口之间进行转发。 同时，Docker 随机分配一个本地未占用的私有网段（在 RFC1918 中定义）中的一个地址给 docker0 接口。比如典型的 172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段（172.17.0.0/16）的地址。 当创建一个 Docker 容器的时候，同时会创建了一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0；另一端在本地并被挂载到 docker0 网桥，名称以 veth 开头（例如 vethAQI2QT）。通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。Docker 就创建了在主机和所有容器之间一个虚拟共享网络。 10.2 查看网络信息1# docker network ls 10.3 创建一个网桥1# docker network create -d bridge 网桥名称 10.4 删除一个网桥1# docker network rm 网桥名称 10.5 容器之前使用网络通信12# 1.查询当前网络配置- docker network ls 1234NETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local 12# 2.创建桥接网络- docker network create -d bridge info 12345678[root@centos ~]# docker network create -d bridge info6e4aaebff79b1df43a064e0e8fdab08f52d64ce34db78dd5184ce7aaaf550a2f[root@centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local6e4aaebff79b info bridge local 1234# 3.启动容器指定使用网桥- docker run -d -p 8890:80 --name nginx001 --network info nginx - docker run -d -p 8891:80 --name nginx002 --network info nginx `注意:一旦指定网桥后--name指定名字就是主机名,多个容器指定在同一个网桥时,可以在任意一个容器中使用主机名与容器进行互通` 12345678910111213141516[root@centos ~]# docker run -d -p 8890:80 --name nginx001 --network info nginx c315bcc94e9ddaa36eb6c6f16ca51592b1ac8bf1ecfe9d8f01d892f3f10825fe[root@centos ~]# docker run -d -p 8891:80 --name nginx002 --network info nginxf8682db35dd7fb4395f90edb38df7cad71bbfaba71b6a4c6e2a3a525cb73c2a5[root@centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf8682db35dd7 nginx \"/docker-entrypoint.…\" 3 seconds ago Up 2 seconds 0.0.0.0:8891-&gt;80/tcp nginx002c315bcc94e9d nginx \"/docker-entrypoint.…\" 7 minutes ago Up 7 minutes 0.0.0.0:8890-&gt;80/tcp nginx001b63169d43792 mysql:5.7.19 \"docker-entrypoint.s…\" 7 minutes ago Up 7 minutes 3306/tcp mysql_mysql.1.s75qe5kkpwwttyf0wrjvd2cda[root@centos ~]# docker exec -it f8682db35dd7 /bin/bashroot@f8682db35dd7:/# curl http://nginx001&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;..... 11.高级数据卷配置11.1 说明数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会复制到数据卷中（仅数据卷为空时会复制）。 11.2 创建数据卷12[root@centos ~]# docker volume create my-volmy-vol 11.3 查看数据卷123456789101112[root@centos ~]# docker volume inspect my-vol [ { \"CreatedAt\": \"2020-11-25T11:43:56+08:00\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" }] 11.4 挂载数据卷1234567891011121314[root@centos ~]# docker run -d -P --name web -v my-vol:/usr/share/nginx/html nginx[root@centos ~]# docker inspect web \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"my-vol\", \"Source\": \"/var/lib/docker/volumes/my-vol/_data\", \"Destination\": \"/usr/share/nginx/html\", \"Driver\": \"local\", \"Mode\": \"z\", \"RW\": true, \"Propagation\": \"\" } ], 11.5 删除数据卷1docker volume rm my-vol 12.Docker Compose12.1 简介Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。 其代码目前在 https://github.com/docker/compose 上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 12.2 安装与卸载1.linux 在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。例如，在 Linux 64 位系统上直接下载对应的二进制包。 12$ sudo curl -L https://github.com/docker/compose/releases/download/1.25.5/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 2.macos、window Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。Docker Desktop for Mac/Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。 3.bash命令补全1$ curl -L https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose 4.卸载 如果是二进制包方式安装的，删除二进制文件即可。 1$ sudo rm /usr/local/bin/docker-compose 5.测试安装成功12$ docker-compose --version docker-compose version 1.25.5, build 4667896b 12.3 docker compose使用1# 1.相关概念 首先介绍几个术语。 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。∂一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。 1# 2.场景 最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。 springboot应用 mysql服务 redis服务 elasticsearch服务 ……. 12# 3.docker-compose模板- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/compose_file.html 12345678910111213141516171819202122232425262728293031version: \"3.0\"services: mysqldb: image: mysql:5.7.19 container_name: mysql ports: - \"3306:3306\" volumes: - /root/mysql/conf:/etc/mysql/conf.d - /root/mysql/logs:/logs - /root/mysql/data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: root networks: - ems depends_on: - redis redis: image: redis:4.0.14 container_name: redis ports: - \"6379:6379\" networks: - ems volumes: - /root/redis/data:/data command: redis-server networks: ems: 12# 4.通过docker-compose运行一组容器- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/commands.html 12[root@centos ~]# docker-compose up //前台启动一组服务[root@centos ~]# docker-compose up -d //后台启动一组服务 12.4 docker-compose 模板文件模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。 默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。 123456789version: \"3\"services: webapp: image: examples/web ports: - \"80:80\" volumes: - \"/data\" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中重复设置。 下面分别介绍各个指令的用法。 build指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 12345version: '3'services: webapp: build: ./dir 你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。 使用 dockerfile 指令指定 Dockerfile 文件名。 使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 command覆盖容器启动后默认执行的命令。 1command: echo \"hello world\" container_name指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 depends_on解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。 只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"] interval: 1m30s timeout: 10s retries: 3 image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。 123image: ubuntuimage: orchardup/postgresqlimage: a4bc65fd networks配置容器连接的网络。 1234567891011version: \"3\"services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: ports暴露端口信息。 使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - \"3000\" - \"8000:8000\" - \"49100:22\" - \"127.0.0.1:8001:8001\" 注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。 例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置为宿主机路径(HOST:CONTAINER)或者数据卷名称(VOLUME:CONTAINER)，并且可以设置访问模式 （HOST:CONTAINER:ro）。 该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 如果路径为数据卷名称，必须在文件中配置数据卷。 12345678910version: \"3\"services: my_src: image: mysql:8.0 volumes: - mysql_data:/var/lib/mysqlvolumes: mysql_data: 12.5 docker-compose 常用命令1. 命令对象与格式对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。 执行 docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。 docker-compose 命令的基本的使用格式是 1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 2. 命令选项 -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。 --x-networking 使用 Docker 的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出。 3.命令使用说明up格式为 docker-compose up [options] [SERVICE...]。 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容 down 此命令将会停止 up 命令所启动的容器，并移除网络 exec 进入指定的容器。 ps格式为 docker-compose ps [options] [SERVICE...]。 列出项目中目前的所有容器。 选项： -q 只打印容器的 ID 信息。 restart格式为 docker-compose restart [options] [SERVICE...]。 重启项目中的服务。 选项： -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。 rm格式为 docker-compose rm [options] [SERVICE...]。 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 选项： -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。 -v 删除容器所挂载的数据卷。 start格式为 docker-compose start [SERVICE...]。 启动已经存在的服务容器。 stop格式为 docker-compose stop [options] [SERVICE...]。 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 top查看各个服务容器内运行的进程。 unpause格式为 docker-compose unpause [SERVICE...]。 恢复处于暂停状态中的服务。 13.docker可视化工具13.1 安装Portainer官方安装说明：https://www.portainer.io/installation/ 123456789[root@ubuntu1804 ~]#docker pull portainer/portainer[root@ubuntu1804 ~]#docker volume create portainer_dataportainer_data[root@ubuntu1804 ~]#docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer20db26b67b791648c2ef6aee444a5226a9c897ebcf0160050e722dbf4a4906e3[root@ubuntu1804 ~]#docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES20db26b67b79 portainer/portainer \"/portainer\" 5 seconds ago Up 4 seconds 0.0.0.0:8000-&gt;8000/tcp, 0.0.0.0:9000-&gt;9000/tcp portainer 13.2 登录和使用Portainer 用浏览器访问：http://localhost:9000","categories":[{"name":"docker","slug":"docker","permalink":"https://yichenfirst.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://yichenfirst.github.io/tags/docker/"}]},{"title":"SpringCloud笔记","slug":"spring/SpringCloud","date":"2022-11-04T16:00:00.000Z","updated":"2023-07-17T13:41:27.824Z","comments":true,"path":"2022/11/05/spring/SpringCloud/","link":"","permalink":"https://yichenfirst.github.io/2022/11/05/spring/SpringCloud/","excerpt":"","text":"1.认识微服务随着互联网行业的发展，对服务的要求也越来越高，服务架构也从单体架构逐渐演变为现在流行的微服务架构。这些架构之间有怎样的差别呢？ 1.1.单体架构单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。 单体架构的优缺点如下： 优点： 架构简单 部署成本低 缺点： 耦合度高（维护困难、升级困难） 1.2.分布式架构分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。 分布式架构的优缺点： 优点： 降低服务耦合 有利于服务升级和拓展 缺点： 服务调用关系错综复杂 分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考： 服务拆分的粒度如何界定？ 服务之间如何调用？ 服务的调用关系如何管理？ 人们需要制定一套行之有效的标准来约束分布式架构。 1.3.微服务微服务的架构特征： 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责 自治：团队独立、技术独立、数据独立，独立部署和交付 面向服务：服务提供统一标准的接口，与语言和技术无关 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题 微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。 因此，可以认为微服务是一种经过良好架构设计的分布式架构方案 。 但方案该怎么落地？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案。 其中在Java领域最引人注目的就是SpringCloud提供的方案了。 1.4.SpringCloudSpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。 SpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。 其中常见的组件包括： 另外，SpringCloud底层是依赖于SpringBoot的，并且有版本的兼容关系，如下： 我们课堂学习的版本是 Hoxton.SR10，因此对应的SpringBoot版本是2.3.x版本。 1.5.总结 单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统 分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝 微服务：一种良好的分布式架构方案 ①优点：拆分粒度更小、服务更独立、耦合度更低 ②缺点：架构非常复杂，运维、监控、部署难度提高 SpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件 2.服务拆分和远程调用任何分布式架构都离不开服务的拆分，微服务也是一样。 2.1.服务拆分原则这里我总结了微服务拆分时的几个原则： 不同微服务，不要重复开发相同业务 微服务数据独立，不要访问其它微服务的数据库 微服务可以将自己的业务暴露为接口，供其它微服务调用 2.2.服务拆分示例以微服务cloud-demo为例，其结构如下： cloud-demo：父工程，管理依赖 order-service：订单微服务，负责订单相关业务 user-service：用户微服务，负责用户相关业务 要求： 订单微服务和用户微服务都必须有各自的数据库，相互独立 订单服务和用户服务都对外暴露Restful的接口 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库 2.2.1.导入Sql语句首先，将课前资料提供的cloud-order.sql和cloud-user.sql导入到mysql中： cloud-user表中初始数据如下： cloud-order表中初始数据如下： cloud-order表中持有cloud-user表中的id字段。 2.2.2.导入demo工程用IDEA导入课前资料提供的Demo： 项目结构如下： 导入后，会在IDEA右下角出现弹窗： 点击弹窗，然后按下图选择： 会出现这样的菜单： 配置下项目使用的JDK： 2.3.实现远程调用案例在order-service服务中，有一个根据id查询订单的接口： 根据id查询订单，返回值是Order对象，如图： 其中的user为null 在user-service中有一个根据id查询用户的接口： 查询的结果如图： 2.3.1.案例需求：修改order-service中的根据id查询订单业务，要求在查询订单的同时，根据订单中包含的userId查询出用户信息，一起返回。 因此，我们需要在order-service中 向user-service发起一个http的请求，调用http://localhost:8081/user/{userId}这个接口。 大概的步骤是这样的： 注册一个RestTemplate的实例到Spring容器 修改order-service服务中的OrderService类中的queryOrderById方法，根据Order对象中的userId查询User 将查询的User填充到Order对象，一起返回 2.3.2.注册RestTemplate首先，我们在order-service服务中的OrderApplication启动类中，注册RestTemplate实例： 123456789101112131415161718192021package cn.itcast.order;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@MapperScan(\"cn.itcast.order.mapper\")@SpringBootApplicationpublic class OrderApplication { public static void main(String[] args) { SpringApplication.run(OrderApplication.class, args); } @Bean public RestTemplate restTemplate() { return new RestTemplate(); }} 2.3.3.实现远程调用修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法： 2.4.提供者与消费者在服务调用关系中，会有两个不同的角色： 服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务） 服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口） 但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。 如果服务A调用了服务B，而服务B又调用了服务C，服务B的角色是什么？ 对于A调用B的业务而言：A是服务消费者，B是服务提供者 对于B调用C的业务而言：B是服务消费者，C是服务提供者 因此，服务B既可以是服务提供者，也可以是服务消费者。 3.Eureka注册中心假如我们的服务提供者user-service部署了多个实例，如图： 大家思考几个问题： order-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？ 有多个user-service实例地址，order-service调用时该如何选择？ order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ 3.1.Eureka的结构和作用这些问题都需要利用SpringCloud中的注册中心来解决，其中最广为人知的注册中心就是Eureka，其结构如下： 回答之前的各个问题。 问题1：order-service如何得知user-service实例地址？ 获取地址信息的流程如下： user-service服务实例启动后，将自己的信息注册到eureka-server（Eureka服务端）。这个叫服务注册 eureka-server保存服务名称到服务实例地址列表的映射关系 order-service根据服务名称，拉取实例地址列表。这个叫服务发现或服务拉取 问题2：order-service如何从多个user-service实例中选择具体的实例？ order-service从实例列表中利用负载均衡算法选中一个实例地址 向该实例地址发起远程调用 问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ user-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳 当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除 order-service拉取服务时，就能将故障实例排除了 注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端 因此，接下来我们动手实践的步骤包括： 3.2.搭建eureka-server首先大家注册中心服务端：eureka-server，这必须是一个独立的微服务 3.2.1.创建eureka-server服务在cloud-demo父工程下，创建一个子模块： 填写模块信息： 然后填写服务信息： 3.2.2.引入eureka依赖引入SpringCloud为eureka提供的starter依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 3.2.3.编写启动类给eureka-server服务编写一个启动类，一定要添加一个@EnableEurekaServer注解，开启eureka的注册中心功能： 12345678910111213package cn.itcast.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication { public static void main(String[] args) { SpringApplication.run(EurekaApplication.class, args); }} 3.2.4.编写配置文件编写一个application.yml文件，内容如下： 123456789server: port: 10086spring: application: name: eureka-servereureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 3.2.5.启动服务启动微服务，然后在浏览器访问：http://127.0.0.1:10086 看到下面结果应该是成功了： 3.3.服务注册下面，我们将user-service注册到eureka-server中去。 1）引入依赖在user-service的pom文件中，引入下面的eureka-client依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置文件在user-service中，修改application.yml文件，添加服务名称、eureka地址： 1234567spring: application: name: userserviceeureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 3）启动多个user-service实例为了演示一个服务有多个实例的场景，我们添加一个SpringBoot的启动配置，再启动一个user-service。 首先，复制原来的user-service启动配置： 然后，在弹出的窗口中，填写信息： 现在，SpringBoot窗口会出现两个user-service启动配置： 不过，第一个是8081端口，第二个是8082端口。 启动两个user-service实例： 查看eureka-server管理页面： 3.4.服务发现下面，我们将order-service的逻辑修改：向eureka-server拉取user-service的信息，实现服务发现。 1）引入依赖之前说过，服务发现、服务注册统一都封装在eureka-client依赖，因此这一步与服务注册时一致。 在order-service的pom文件中，引入下面的eureka-client依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置文件服务发现也需要知道eureka地址，因此第二步与服务注册一致，都是配置eureka信息： 在order-service中，修改application.yml文件，添加服务名称、eureka地址： 1234567spring: application: name: orderserviceeureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 3）服务拉取和负载均衡最后，我们要去eureka-server中拉取user-service服务的实例列表，并且实现负载均衡。 不过这些动作不用我们去做，只需要添加一些注解即可。 在order-service的OrderApplication中，给RestTemplate这个Bean添加一个@LoadBalanced注解： 修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法。修改访问的url路径，用服务名代替ip、端口： spring会自动帮助我们从eureka-server端，根据userservice这个服务名称，获取实例列表，而后完成负载均衡。 4.Ribbon负载均衡上一节中，我们添加了@LoadBalanced注解，即可实现负载均衡功能，这是什么原理呢？ 4.1.负载均衡原理SpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。 那么我们发出的请求明明是http://userservice/user/1，怎么变成了http://localhost:8081的呢？ 4.2.源码跟踪为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。 显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor，这个类会在对RestTemplate的请求进行拦截，然后从Eureka根据服务id获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务id。 我们进行源码跟踪： 1）LoadBalancerIntercepor 可以看到这里的intercept方法，拦截了用户的HttpRequest请求，然后做了几件事： request.getURI()：获取请求uri，本例中就是 http://user-service/user/8 originalUri.getHost()：获取uri路径的主机名，其实就是服务id，user-service this.loadBalancer.execute()：处理服务id，和用户请求。 这里的this.loadBalancer是LoadBalancerClient类型，我们继续跟入。 2）LoadBalancerClient继续跟入execute方法： 代码是这样的： getLoadBalancer(serviceId)：根据服务id获取ILoadBalancer，而ILoadBalancer会拿着服务id去eureka中获取服务列表并保存起来。 getServer(loadBalancer)：利用内置的负载均衡算法，从服务列表中选择一个。本例中，可以看到获取了8082端口的服务 放行后，再次访问并跟踪，发现获取的是8081： 果然实现了负载均衡。 3）负载均衡策略IRule在刚才的代码中，可以看到获取服务使通过一个getServer方法来做负载均衡: 我们继续跟入： 继续跟踪源码chooseServer方法，发现这么一段代码： 我们看看这个rule是谁： 这里的rule默认值是一个RoundRobinRule，看类的介绍： 这不就是轮询的意思嘛。 到这里，整个负载均衡的流程我们就清楚了。 4）总结SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结一下： 基本流程如下： 拦截我们的RestTemplate请求http://userservice/user/1 RibbonLoadBalancerClient会从请求url中获取服务名称，也就是user-service DynamicServerListLoadBalancer根据user-service到eureka拉取服务列表 eureka返回列表，localhost:8081、localhost:8082 IRule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081 RibbonLoadBalancerClient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求 4.3.负载均衡策略4.3.1.负载均衡策略负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类： 不同规则的含义如下： 内置负载均衡规则类 规则描述 RoundRobinRule 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 AvailabilityFilteringRule 对以下两种服务器进行忽略： （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。 （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的..ActiveConnectionsLimit属性进行配置。 WeightedResponseTimeRule 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 ZoneAvoidanceRule 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 BestAvailableRule 忽略那些短路的服务器，并选择并发数较低的服务器。 RandomRule 随机选择一个可用的服务器。 RetryRule 重试机制的选择逻辑 默认的实现就是ZoneAvoidanceRule，是一种轮询方案 4.3.2.自定义负载均衡策略通过定义IRule实现可以修改负载均衡规则，有两种方式： 代码方式：在order-service中的OrderApplication类中，定义一个新的IRule： 1234@Beanpublic IRule randomRule(){ return new RandomRule();} 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则： 123userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务 ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 注意，一般用默认的负载均衡规则，不做修改。 4.4.饥饿加载Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。 而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载： 1234ribbon: eager-load: enabled: true clients: userservice 5.Nacos注册中心国内公司一般都推崇阿里巴巴的技术，比如注册中心，SpringCloudAlibaba也推出了一个名为Nacos的注册中心。 5.1.认识和安装NacosNacos是阿里巴巴的产品，现在是SpringCloud中的一个组件。相比Eureka功能更加丰富，在国内受欢迎程度较高。 安装方式可以参考课前资料《Nacos安装指南.md》 1.Windows安装开发阶段采用单机安装即可。 1.1.下载安装包在Nacos的GitHub页面，提供有下载链接，可以下载编译好的Nacos服务端或者源代码： GitHub主页：https://github.com/alibaba/nacos GitHub的Release下载页：https://github.com/alibaba/nacos/releases 如图： 本课程采用1.4.1.版本的Nacos，课前资料已经准备了安装包： windows版本使用nacos-server-1.4.1.zip包即可。 1.2.解压将这个包解压到任意非中文目录下，如图： 目录说明： bin：启动脚本 conf：配置文件 1.3.端口配置Nacos的默认端口是8848，如果你电脑上的其它进程占用了8848端口，请先尝试关闭该进程。 如果无法关闭占用8848端口的进程，也可以进入nacos的conf目录，修改配置文件中的端口： 修改其中的内容： 1.4.启动启动非常简单，进入bin目录，结构如下： 然后执行命令即可： windows命令： 1startup.cmd -m standalone 执行后的效果如图： 1.5.访问在浏览器输入地址：http://127.0.0.1:8848/nacos即可： 默认的账号和密码都是nacos，进入后： 2.Linux安装Linux或者Mac安装方式与Windows类似。 2.1.安装JDKNacos依赖于JDK运行，索引Linux上也需要安装JDK才行。 上传jdk安装包： 上传到某个目录，例如：/usr/local/ 然后解压缩： 1tar -xvf jdk-8u144-linux-x64.tar.gz 然后重命名为java 配置环境变量： 12export JAVA_HOME=/usr/local/javaexport PATH=$PATH:$JAVA_HOME/bin 设置环境变量： 1source /etc/profile 2.2.上传安装包如图： 也可以直接使用课前资料中的tar.gz： 上传到Linux服务器的某个目录，例如/usr/local/src目录下： 2.3.解压命令解压缩安装包： 1tar -xvf nacos-server-1.4.1.tar.gz 然后删除安装包： 1rm -rf nacos-server-1.4.1.tar.gz 目录中最终样式： 目录内部： 2.4.端口配置与windows中类似 2.5.启动在nacos/bin目录中，输入命令启动Nacos： 1sh startup.sh -m standalone 3.Nacos的依赖父工程： 1234567&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 客户端： 123456&lt;!-- nacos客户端依赖包 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 5.2.服务注册到nacosNacos是SpringCloudAlibaba的组件，而SpringCloudAlibaba也遵循SpringCloud中定义的服务注册、服务发现规范。因此使用Nacos和使用Eureka对于微服务来说，并没有太大区别。 主要差异在于： 依赖不同 服务地址不同 1）引入依赖在cloud-demo父工程的pom文件中的&lt;dependencyManagement&gt;中引入SpringCloudAlibaba的依赖： 1234567&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 然后在user-service和order-service中的pom文件中引入nacos-discovery依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 注意：不要忘了注释掉eureka的依赖。 2）配置nacos地址在user-service和order-service的application.yml中添加nacos地址： 1234spring: cloud: nacos: server-addr: localhost:8848 注意：不要忘了注释掉eureka的地址 3）重启重启微服务后，登录nacos管理页面，可以看到微服务信息： 5.3.服务分级存储模型一个服务可以有多个实例，例如我们的user-service，可以有: 127.0.0.1:8081 127.0.0.1:8082 127.0.0.1:8083 假如这些实例分布于全国各地的不同机房，例如： 127.0.0.1:8081，在上海机房 127.0.0.1:8082，在上海机房 127.0.0.1:8083，在杭州机房 Nacos就将同一机房内的实例 划分为一个集群。 也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图： 微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如： 杭州机房内的order-service应该优先访问同机房的user-service。 5.3.1.给user-service配置集群修改user-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 重启两个user-service实例后，我们可以在nacos控制台看到下面结果： 我们再次复制一个user-service启动配置，添加属性： 1-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH 配置如图所示： 启动UserApplication3后再次查看nacos控制台： 5.3.2.同集群优先的负载均衡默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。 因此Nacos中提供了一个NacosRule的实现，可以优先从同集群中挑选实例。 1）给order-service配置集群信息 修改order-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 2）修改负载均衡规则 修改order-service的application.yml文件，修改负载均衡规则： 123userservice: ribbon: NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 5.4.权重配置实际部署中会出现这样的场景： 服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。 但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。 因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。 在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重： 在弹出的编辑窗口，修改权重： 注意：如果权重修改为0，则该实例永远不会被访问 5.5.环境隔离Nacos提供了namespace来实现环境隔离功能。 nacos中可以有多个namespace namespace下可以有group、service等 不同namespace之间相互隔离，例如不同namespace的服务互相不可见 5.5.1.创建namespace默认情况下，所有service、data、group都在同一个namespace，名为public： 我们可以点击页面新增按钮，添加一个namespace： 然后，填写表单： 就能在页面看到一个新的namespace： 5.5.2.给微服务配置namespace给微服务配置namespace只能通过修改配置来实现。 例如，修改order-service的application.yml文件： 1234567spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID 重启order-service后，访问控制台，可以看到下面的结果： 此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错： 5.6.Nacos与Eureka的区别Nacos的服务实例分为两种l类型： 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。 配置一个服务实例为永久实例： 12345spring: cloud: nacos: discovery: ephemeral: false # 设置为非临时实例 Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异： Nacos与eureka的共同点 都支持服务注册和服务拉取 都支持服务提供者心跳方式做健康检测 Nacos与Eureka的区别 Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式 临时实例心跳不正常会被剔除，非临时实例则不会被剔除 Nacos支持服务列表变更的消息推送模式，服务列表更新更及时 Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式 6.Nacos配置管理Nacos除了可以做注册中心，同样可以做配置管理来使用。 6.1.统一配置管理当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。 Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。 1.1.1.在nacos中添加配置文件如何在nacos中管理配置呢？ 然后在弹出的表单中，填写配置信息： 注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。 1.1.2.从微服务拉取配置微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。 但如果尚未读取application.yml，又如何得知nacos地址呢？ 因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下： 1）引入nacos-config依赖 首先，在user-service服务中，引入nacos-config的客户端依赖： 12345&lt;!--nacos配置管理依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加bootstrap.yaml 然后，在user-service中添加一个bootstrap.yaml文件，内容如下： 12345678910spring: application: name: userservice # 服务名称 profiles: active: dev #开发环境，这里是dev cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据 ${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}作为文件id，来读取配置。 本例中，就是去读取userservice-dev.yaml： 3）读取nacos配置 在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置： 完整代码： 1234567891011121314151617181920212223242526272829package cn.itcast.user.web;import cn.itcast.user.pojo.User;import cn.itcast.user.service.UserService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.*;import java.time.LocalDateTime;import java.time.format.DateTimeFormatter;@Slf4j@RestController@RequestMapping(\"/user\")public class UserController { @Autowired private UserService userService; @Value(\"${pattern.dateformat}\") private String dateformat; @GetMapping(\"now\") public String now(){ return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat)); } // ...略} 在页面访问，可以看到效果： 1.2.配置热更新我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。 要实现配置热更新，可以使用两种方式： 6.2.1.方式一在@Value注入的变量所在类上添加注解@RefreshScope： 6.2.2.方式二使用@ConfigurationProperties注解代替@Value注解。 在user-service服务中，添加一个类，读取patterrn.dateformat属性： 123456789101112package cn.itcast.user.config;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@Data@ConfigurationProperties(prefix = \"pattern\")public class PatternProperties { private String dateformat;} 在UserController中使用这个类代替@Value： 完整代码： 123456789101112131415161718192021222324252627282930313233package cn.itcast.user.web;import cn.itcast.user.config.PatternProperties;import cn.itcast.user.pojo.User;import cn.itcast.user.service.UserService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.time.LocalDateTime;import java.time.format.DateTimeFormatter;@Slf4j@RestController@RequestMapping(\"/user\")public class UserController { @Autowired private UserService userService; @Autowired private PatternProperties patternProperties; @GetMapping(\"now\") public String now(){ return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat())); } // 略} 6.3.配置共享其实微服务启动时，会去nacos读取多个配置文件，例如： [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml [spring.application.name].yaml，例如：userservice.yaml 而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。 下面我们通过案例来测试配置共享 1）添加一个环境共享配置我们在nacos中添加一个userservice.yaml文件： 2）在user-service中读取共享配置在user-service服务中，修改PatternProperties类，读取新添加的属性： 在user-service服务中，修改UserController，添加一个方法： 3）运行两个UserApplication，使用不同的profile修改UserApplication2这个启动项，改变其profile值： 这样，UserApplication(8081)使用的profile是dev，UserApplication2(8082)使用的profile是test。 启动UserApplication和UserApplication2 访问http://localhost:8081/user/prop，结果： 访问http://localhost:8082/user/prop，结果： 可以看出来，不管是dev，还是test环境，都读取到了envSharedValue这个属性的值。 4）配置共享的优先级当nacos、服务本地同时出现相同属性时，优先级有高低之分： 7.Feign远程调用先来看我们以前利用RestTemplate发起远程调用的代码： 存在下面的问题： •代码可读性差，编程体验不统一 •参数复杂URL难以维护 Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign 其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。 7.1.Feign替代RestTemplateFegin的使用步骤如下： 1）引入依赖我们在order-service服务的pom文件中引入feign的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加注解在order-service的启动类添加注解开启Feign的功能： 3）编写Feign的客户端在order-service中新建一个接口，内容如下： 123456789101112package cn.itcast.order.client;import cn.itcast.order.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(\"userservice\")public interface UserClient { @GetMapping(\"/user/{id}\") User findById(@PathVariable(\"id\") Long id);} 这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如： 服务名称：userservice 请求方式：GET 请求路径：/user/{id} 请求参数：Long id 返回值类型：User 这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。 4）测试修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate： 是不是看起来优雅多了。 5）总结使用Feign的步骤： ① 引入依赖 ② 添加@EnableFeignClients注解 ③ 编写FeignClient接口 ④ 使用FeignClient中定义的方法代替RestTemplate 7.2.自定义配置Feign可以支持很多的自定义配置，如下表所示： 类型 作用 说明 feign.Logger.Level 修改日志级别 包含四种不同的级别：NONE、BASIC、HEADERS、FULL feign.codec.Decoder 响应结果的解析器 http远程调用的结果做解析，例如解析json字符串为java对象 feign.codec.Encoder 请求参数编码 将请求参数编码，便于通过http请求发送 feign. Contract 支持的注解格式 默认是SpringMVC的注解 feign. Retryer 失败重试机制 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。 下面以日志为例来演示如何自定义配置。 7.2.1.配置文件方式基于配置文件修改feign的日志级别可以针对单个服务： 12345feign: client: config: userservice: # 针对某个微服务的配置 loggerLevel: FULL # 日志级别 也可以针对所有服务： 12345feign: client: config: default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 而日志的级别分为四种： NONE：不记录任何日志信息，这是默认值。 BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 7.2.2.Java代码方式也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象： 123456public class DefaultFeignConfiguration { @Bean public Logger.Level feignLogLevel(){ return Logger.Level.BASIC; // 日志级别为BASIC }} 如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中： 1@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration .class) 如果是局部生效，则把它放到对应的@FeignClient这个注解中： 1@FeignClient(value = \"userservice\", configuration = DefaultFeignConfiguration .class) 7.3.Feign使用优化Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括： •URLConnection：默认实现，不支持连接池 •Apache HttpClient ：支持连接池 •OKHttp：支持连接池 因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。 这里我们用Apache的HttpClient来演示。 1）引入依赖 在order-service的pom文件中引入Apache的HttpClient依赖： 12345&lt;!--httpClient的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置连接池 在order-service的application.yml中添加配置： 123456789feign: client: config: default: # default全局的配置 loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息 httpclient: enabled: true # 开启feign对HttpClient的支持 max-connections: 200 # 最大的连接数 max-connections-per-route: 50 # 每个路径的最大连接数 接下来，在FeignClientFactoryBean中的loadBalance方法中打断点： Debug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient： 总结，Feign的优化： 1.日志级别尽量用basic 2.使用HttpClient或OKHttp代替URLConnection ① 引入feign-httpClient依赖 ② 配置文件开启httpClient功能，设置连接池参数 7.4.最佳实践所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。 自习观察可以发现，Feign的客户端与服务提供者的controller代码非常相似： feign客户端： UserController： 有没有一种办法简化这种重复的代码编写呢？ 7.4.1.继承方式一样的代码可以通过继承来共享： 1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。 2）Feign客户端和Controller都集成改接口 优点： 简单 实现了代码共享 缺点： 服务提供方、服务消费方紧耦合 参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解 7.4.2.抽取方式将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。 例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。 7.4.3.实现基于抽取的最佳实践1）抽取首先创建一个module，命名为feign-api： 项目结构： 在feign-api中然后引入feign的starter依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中 2）在order-service中使用feign-api首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。 在order-service的pom文件中中引入feign-api的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;cn.itcast.demo&lt;/groupId&gt; &lt;artifactId&gt;feign-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包 3）重启测试重启后，发现服务报错了： 这是因为UserClient现在在cn.itcast.feign.clients包下， 而order-service的@EnableFeignClients注解是在cn.itcast.order包下，不在同一个包，无法扫描到UserClient。 4）解决扫描包问题方式一： 指定Feign应该扫描的包： 1@EnableFeignClients(basePackages = \"cn.itcast.feign.clients\") 方式二： 指定需要加载的Client接口： 1@EnableFeignClients(clients = {UserClient.class}) 8.Gateway服务网关Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 8.1.为什么需要网关Gateway网关是我们服务的守门神，所有微服务的统一入口。 网关的核心功能特性： 请求路由 权限控制 限流 架构图： 权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。 路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。 限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。 在SpringCloud中网关的实现包括两种： gateway zuul Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。 8.2.gateway快速入门下面，我们就演示下网关的基本路由功能。基本步骤如下： 创建SpringBoot工程gateway，引入网关依赖 编写启动类 编写基础配置和路由规则 启动网关服务进行测试 1）创建gateway服务，引入依赖创建服务： 引入依赖： 12345678910&lt;!--网关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--nacos服务发现依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 2）编写启动类123456789101112package cn.itcast.gateway;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GatewayApplication { public static void main(String[] args) { SpringApplication.run(GatewayApplication.class, args); }} 3）编写基础配置和路由规则创建application.yml文件，内容如下： 123456789101112131415server: port: 10010 # 网关端口spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址 uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。 本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。 4）重启测试重启网关，访问http://localhost:10010/user/1时，符合`/user/**`规则，请求转发到uri：http://userservice/user/1，得到了结果： 5）网关路由的流程图整个访问的流程如下： 总结： 网关搭建步骤： 创建项目，引入nacos服务发现和gateway依赖 配置application.yml，包括服务基本信息、nacos地址、路由 路由配置包括： 路由id：路由的唯一标示 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡 路由断言（predicates）：判断路由的规则， 路由过滤器（filters）：对请求或响应做处理 接下来，就重点来学习路由断言和路由过滤器的详细知识 8.3.断言工厂我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件 例如Path=/user/**是按照路径匹配，这个规则是由 org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来 处理的，像这样的断言工厂在SpringCloudGateway还有十几个: 名称 说明 示例 After 是某个时间点后的请求 - After=2037-01-20T17:42:47.789-07:00[America/Denver] Before 是某个时间点之前的请求 - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai] Between 是某两个时间点之前的请求 - Between=2037-01-20T17:42:47.789-07:00[America/Denver], 2037-01-21T17:42:47.789-07:00[America/Denver] Cookie 请求必须包含某些cookie - Cookie=chocolate, ch.p Header 请求必须包含某些header - Header=X-Request-Id, \\d+ Host 请求必须是访问某个host（域名） - Host=.somehost.org,.anotherhost.org Method 请求方式必须是指定方式 - Method=GET,POST Path 请求路径必须符合指定规则 - Path=/red/{segment},/blue/** Query 请求参数必须包含指定参数 - Query=name, Jack或者- Query=name RemoteAddr 请求者的ip必须是指定范围 - RemoteAddr=192.168.1.1/24 Weight 权重处理 我们只需要掌握Path这种路由工程就可以了。 8.4.过滤器工厂GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理： 8.4.1.路由过滤器的种类Spring提供了31种不同的路由过滤器工厂。例如： 名称 说明 AddRequestHeader 给当前请求添加一个请求头 RemoveRequestHeader 移除请求中的一个请求头 AddResponseHeader 给响应结果中添加一个响应头 RemoveResponseHeader 从响应结果中移除有一个响应头 RequestRateLimiter 限制请求的流量 8.4.2.请求头过滤器下面我们以AddRequestHeader 为例来讲解。 需求：给所有进入userservice的请求添加一个请求头：Truth=itcast is freaking awesome! 只需要修改gateway服务的application.yml文件，添加路由过滤即可： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** filters: # 过滤器 - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头 当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。 8.4.3.默认过滤器如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** default-filters: # 默认过滤项 - AddRequestHeader=Truth, Itcast is freaking awesome! 8.4.4.总结过滤器的作用是什么？ ① 对路由的请求或响应做加工处理，比如添加请求头 ② 配置在路由下的过滤器只对当前路由的请求生效 defaultFilters的作用是什么？ ① 对所有路由都生效的过滤器 8.5.全局过滤器上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。 8.5.1.全局过滤器作用全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。 定义方式是实现GlobalFilter接口。 12345678910public interface GlobalFilter { /** * 处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理 * * @param exchange 请求上下文，里面可以获取Request、Response等信息 * @param chain 用来把请求委托给下一个过滤器 * @return {@code Mono&lt;Void&gt;} 返回标示当前过滤器业务结束 */ Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain);} 在filter中编写自定义逻辑，可以实现下列功能： 登录状态判断 权限校验 请求限流等 8.5.2.自定义全局过滤器需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件： 参数中是否有authorization， authorization参数值是否为admin 如果同时满足则放行，否则拦截 实现： 在gateway中定义一个过滤器： 12345678910111213141516171819202122232425262728293031package cn.itcast.gateway.filters;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.annotation.Order;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;@Order(-1)@Componentpublic class AuthorizeFilter implements GlobalFilter { @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { // 1.获取请求参数 MultiValueMap&lt;String, String&gt; params = exchange.getRequest().getQueryParams(); // 2.获取authorization参数 String auth = params.getFirst(\"authorization\"); // 3.校验 if (\"admin\".equals(auth)) { // 放行 return chain.filter(exchange); } // 4.拦截 // 4.1.禁止访问，设置状态码 exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN); // 4.2.结束处理 return exchange.getResponse().setComplete(); }} 8.5.3.过滤器执行顺序请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter 请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器： 排序的规则是什么呢？ 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。 GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。 当过滤器的order值一样时，会按照 defaultFilter &gt; 路由过滤器 &gt; GlobalFilter的顺序执行。 详细内容，可以查看源码： org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。 org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链 8.6.域问题8.6.1.什么是跨域问题跨域：域名不一致就是跨域，主要包括： 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com 域名相同，端口不同：localhost:8080和localhost8081 跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题 解决方案：CORS，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html 8.6.2.模拟跨域问题找到课前资料的页面文件： 放入tomcat或者nginx这样的web服务器中，启动并访问。 可以在浏览器控制台看到下面的错误： 从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。 8.6.3.解决跨域问题在gateway服务的application.yml文件中，添加下面的配置： 12345678910111213141516171819spring: cloud: gateway: # 。。。 globalcors: # 全局的跨域处理 add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题 corsConfigurations: '[/**]': allowedOrigins: # 允许哪些网站的跨域请求 - \"http://localhost:8090\" allowedMethods: # 允许的跨域ajax的请求方式 - \"GET\" - \"POST\" - \"DELETE\" - \"PUT\" - \"OPTIONS\" allowedHeaders: \"*\" # 允许在请求中携带的头信息 allowCredentials: true # 是否允许携带cookie maxAge: 360000 # 这次跨域检测的有效期","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"},{"name":"springcloud","slug":"springcloud","permalink":"https://yichenfirst.github.io/tags/springcloud/"}]},{"title":"MySQL的日志","slug":"mysql/日志","date":"2022-11-01T16:00:00.000Z","updated":"2023-07-28T14:16:43.590Z","comments":true,"path":"2022/11/02/mysql/日志/","link":"","permalink":"https://yichenfirst.github.io/2022/11/02/mysql/%E6%97%A5%E5%BF%97/","excerpt":"","text":"MySQL的日志为什么需要undo log在MySQL中执行一条增删改的语句的时候，虽然没有显式的输入begin开启事务和commit提交事务，但是MySQL会隐式开启事务来执行增删改的语句，执行完就自动提交事务。执行一条语句是否自动提交事务，是由autocommit参数决定的，默认是开启的。 那么当一个update语句执行过程中，MySQL发生了崩溃，要怎么回滚到事务之前的数据呢？ 如果我们每次在事务执行过程中，都记录下回滚时需要的信息到一个日志里，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据，我们可以通过这个日志回滚到事务之前的数据。 实现这一机制就是undo log（回滚日志），它保证了事务的ACID特性中的原子性。 undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。如下图： 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作。 不同的操作，需要记录的内容也是不同的，所以不同类型的操作（修改、删除、新增）产生的 undo log 的格式也是不同的，具体的每一个操作的 undo log 的格式我就不详细介绍了，感兴趣的可以自己去查查。 一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id： 通过 trx_id 可以知道该记录是被哪个事务修改的； 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链； 版本链如下图： 另外，undo log还有一个作用，通过ReadView + undo log实现MVCC（多版本并发控制）。 对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列（trx_id 和 roll_pointer）」的比对，如果不满足可见行，就会顺着 undo log 版本链里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 为什么需要redo logMySQL的数据都是存储在磁盘中的，当需要更新数据的时候需要现将磁盘中的数据读取到内存中，在内存中修改这条数据，然后在写回到磁盘中。这样就会导致一个问题，MySQL将内存中的数据修改完成并提交之后，万一机器断电重启，就会导致数据的丢失。 为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB引擎就会先更新内存，然后将本次的修改以redo log的形式记录下来，这个时候就算完成更新了。 后续，InnoDB 引擎会在适当的时候，由后台线程将内存中的数据刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 什么事redo log redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 redo log 和 undo log 有什么区别 这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； 事!务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，如下图： 所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log要写磁盘，数据也要写磁盘，为什么要多此一举？ 写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。 磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。 针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。 可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。 至此， 针对为什么需要 redo log 这个问题我们有两个答案： 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。 产生的redo log是直接写入磁盘的吗 不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图： redo log buffer 默认大小 16 MB，可以通过 innodb_log_Buffer_size 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。 redo log 什么时候刷盘？缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？ 主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。 innodb_flush_log_at_trx_commit 参数控制的是什么？ 单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式「顺序写」到磁盘。 上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。 除此之外，InnoDB 还提供了另外两种策略，由参数 innodb_flush_log_at_trx_commit 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下： 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。 innodb_flush_log_at_trx_commit 为 0 和 2 的时候，什么时候才将 redo log 写入磁盘？ InnoDB 的后台线程每隔 1 秒： 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失; 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 加入了后台现线程后，innodb_flush_log_at_trx_commit 的刷盘时机如下图： 这三个参数的应用场景是什么？ 这三个参数的数据安全性和写入性能的比较如下： 数据安全性：参数 1 &gt; 参数 2 &gt; 参数 0 写入性能：参数 0 &gt; 参数 2&gt; 参数 1 所以，数据安全性和写入性能是熊掌不可得兼的，要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性。 在一些对数据安全性要求比较高的场景中，显然 innodb_flush_log_at_trx_commit 参数需要设置为 1。 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高。 redo log文件写满了怎么办默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：ib_logfile0 和 ib_logfile1 。 在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。 重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。 所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。 我们知道 redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。 redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图： 图中的： write pos 和 checkpoint 的移动都是顺时针方向； write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作； check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录； 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。 所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。 为什么需要binlog前面介绍的 undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。 MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。 binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 为什么有了 binlog， 还要有 redo log？ 这个问题跟 MySQL 的时间线有关系。 最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。 而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。 redo log 和 binlog 有什么区别？这两个日志有四个区别。 1、适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 2、文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 3、写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 4、用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？ 不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 主从复制是怎么实现？MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。 这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。 MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。 具体详细过程如下： MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。 从库是不是越多越好？ 不是的。 因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。 所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构 MySQL 主从复制还有哪些模型？ 主要有三种： 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 binlog 什么时候刷盘？事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。 一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。这是因为有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。 MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图： 虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件： 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。 MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。 而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。 如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值 三个日志讲完了，至此我们可以先小结下，update 语句的执行过程。 当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。 具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个。 为什么需要两阶段提交事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 举个例子，假设 id = 1 这行数据的字段 name 的值原本是 ‘jay’，然后执行 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性； 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性； 两阶段提交的过程是怎样的？从下图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）； commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功； 异常重启会出现什么现象？情况一：一阶段提交之后崩溃了，即写入 redo log，处于 prepare 状态 的时候崩溃了，此时： 由于 binlog 还没写，redo log 处于 prepare 状态还没提交，所以崩溃恢复的时候，这个事务会回滚，此时 binlog 还没写，所以也不会传到备库。 情况二：假设写完 binlog 之后崩溃了，此时： redolog 中的日志是不完整的，处于 prepare 状态，还没有提交，那么恢复的时候，首先检查 binlog 中的事务是否存在并且完整，如果存在且完整，则直接提交事务，如果不存在或者不完整，则回滚事务。 情况三：假设 redolog 处于 commit 状态的时候崩溃了，那么重启后的处理方案同情况二。 由此可见，两阶段提交能够确保数据的一致性。 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计? binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。 也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。 有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？ 放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。 所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 两阶段提交有什么问题两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响： 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 为什么两阶段提交的磁盘 I/O 次数会很高？ binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1： 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘； 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘； 可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。 为什么锁竞争激烈？ 在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。 通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"}]},{"title":"Java中创建线程有几种方式","slug":"java/Java创建线程的方式","date":"2022-10-10T16:00:00.000Z","updated":"2023-07-28T14:00:31.008Z","comments":true,"path":"2022/10/11/java/Java创建线程的方式/","link":"","permalink":"https://yichenfirst.github.io/2022/10/11/java/Java%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Java中创建线程的方式继承Thread类步骤： 创建一个继承于Thread类的子类 重写Thread类的run() –&gt; 将此线程执行的操作声明在run()中 创建Thread类的子类的对象 通过此对象调用start()执行线程 12345678910111213static class MyThread extends Thread{ @Override public void run() { for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); } }}public static void main(String[] args) { System.out.println(Thread.currentThread().getName()); MyThread t1 = new MyThread(); t1.start();} 实现Runnable接口步骤： 创建一个实现了Runnable接口的类 实现类去实现Runnable中的抽象方法：run() 创建实现类的对象 将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象 通过Thread类的对象调用start() 启动线程 调用当前线程的run()–&gt;调用了Runnable类型的target的run() 123456789101112131415static class MyThread implements Runnable{ @Override public void run() { for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); } }}public static void main(String[] args) { System.out.println(Thread.currentThread().getName()); MyThread myThread = new MyThread(); Thread t1 = new Thread(myThread); t1.start();} 实现Callable接口步骤： 创建一个实现Callable的实现类 实现call方法，将此线程需要执行的操作声明在call()中 创建Callable接口实现类的对象 将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象 将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start() 获取Callable中call方法的返回值 实现Callable接口的好处：call()可以有返回值；可以抛出异常，被外面的操作捕获，获取异常信息；Callable支持泛型。 12345678910111213141516171819202122232425static class MyThread implements Callable&lt;Integer&gt; { @Override public Integer call() { int sum = 0; for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); sum += i; } return sum; }}public static void main(String[] args) { System.out.println(Thread.currentThread().getName()); MyThread myThread = new MyThread(); FutureTask futureTask = new FutureTask&lt;&gt;(myThread); Thread t1 = new Thread(futureTask); t1.start(); try{ Integer o = (Integer) futureTask.get(); System.out.println(\"sum: \" + o); } catch (Exception e) { e.printStackTrace(); } } 通过线程池创建步骤： 以方式二或方式三创建好实现了Runnable接口的类或实现Callable的实现类 实现run或call方法 创建线程池 调用线程池的execute方法执行某个线程，参数是之前实现Runnable或Callable接口的对象 12345678910111213141516171819202122static class MyThread1 implements Runnable { @Override public void run() { for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); } }}static class MyThread2 implements Runnable { @Override public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); } }}public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(10); service.execute(new MyThread1()); service.execute(new MyThread2()); service.shutdown();} 使用匿名类12345678910Thread t = new Thread() { @Override public void run(){ for (int i = 0; i &lt; 100; i++) { System.out.println(Thread.currentThread().getName() + \":\" +i); } }};t.start(); 或者 123Thread t = new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + \":\" +i);}).start(); 底层实现以上几种创建线程的方式底层都是通过实现Runnable接口实现的。 ThreadThread类就实现的Runnable接口 CallableCallable实现方式需要使用FutureTask来配合，FutureTask实现了RunnableFuture&lt; V &gt;接口，而RunnableFuture&lt; V &gt;接口继承了Runnable。 在FutureTask的run方法中调用了Callable接口的call()方法。 FutureTask.java 1234567891011121314151617181920212223242526272829303132public void run() { if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try { Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) { V result; boolean ran; try { result = c.call(); ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) set(result); } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); }} 线程池在线程池中工作线程都是构建的Worker对象，而worker类同样实现了Runnable接口。 总结通常在java中创建线程的方法有四种或五种，但是追其底层，创建线程的方法只有一种，就是通过实现Runnable接口。","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"Spring事务传播机制","slug":"spring/spring事务传播机制","date":"2022-09-21T16:00:00.000Z","updated":"2023-07-23T12:35:35.999Z","comments":true,"path":"2022/09/22/spring/spring事务传播机制/","link":"","permalink":"https://yichenfirst.github.io/2022/09/22/spring/spring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Spring事务传播机制什么是事务传播Spring 事务传播机制是指，包含多个事务的方法在相互调用时，事务是如何在这些方法间传播的。 既然是“事务传播”，所以事务的数量应该在两个或两个以上，Spring 事务传播机制的诞生是为了规定多个事务在传播过程中的行为的。比如方法 A 开启了事务，而在执行过程中又调用了开启事务的 B 方法，那么 B 方法的事务是应该加入到 A 事务当中呢？还是两个事务相互执行互不影响，又或者是将 B 事务嵌套到 A 事务中执行呢？所以这个时候就需要一个机制来规定和约束这两个事务的行为，这就是 Spring 事务传播机制所解决的问题。 传播机制都有哪些Spring 事务传播机制可使用 @Transactional(propagation=Propagation.REQUIRED) 来定义，Spring 事务传播机制的级别包含以下 7 种 Propagation.REQUIRED：默认的事务传播级别，它表示如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 Propagation.SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 Propagation.MANDATORY：（mandatory：强制性）如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 Propagation.REQUIRES_NEW：表示创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW 修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 Propagation.NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 Propagation.NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 Propagation.NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 PROPAGATION_REQUIRED。 以上 7 种传播机制，可根据“是否支持当前事务”的维度分为以下 3 类： 事务传播机制使用演示接下来我们演示一下事务传播机制的使用，以下面 3 个最典型的事务传播级别为例： 支持当前事务的 REQUIRED； 不支持当前事务的 REQUIRES_NEW； 嵌套事务 NESTED。 下来我们分别来看。 事务传播机制的示例，需要用到以下两张表： 123456789101112131415-- 用户表CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL, `password` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL, `createtime` datetime DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin ROW_FORMAT=DYNAMIC;-- 日志表CREATE TABLE `log` ( `id` int(11) NOT NULL AUTO_INCREMENT, `content` text NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin; 创建一个 Spring Boot 项目，核心业务代码有 3 个：UserController、UserServcie 以及 LogService。在 UserController 里面调用 UserService 添加用户，并调用 LogService 添加日志。 REQUIRED 使用演示REQUIRED 支持当前事务。 UserController 实现代码如下，其中 save 方法开启了事务： 1234567891011121314151617@RestControllerpublic class UserController { @Resource private UserService userService; @Resource private LogService logService; @RequestMapping(\"/save\") @Transactional public Object save(User user) { // 插入用户操作 userService.save(user); // 插入日志 logService.saveLog(\"用户插入：\" + user.getName()); return true; }} UserService 实现代码如下： 12345678910@Servicepublic class UserService { @Resource private UserMapper userMapper; @Transactional(propagation = Propagation.REQUIRED) public int save(User user) { return userMapper.save(user); }} LogService 实现代码如下： 123456789101112@Servicepublic class LogService { @Resource private LogMapper logMapper; @Transactional(propagation = Propagation.REQUIRED) public int saveLog(String content) { // 出现异常 int i = 10 / 0; return logMapper.saveLog(content); }} 执行结果：程序报错，两张表中都没有插入任何数据。 执行流程描述： 首先 UserService 中的添加用户方法正常执行完成。 LogService 保存日志程序报错，因为使用的是 UserController 中的全局事务，所以整个事务回滚，步骤 1 中的操作也跟着回滚。 所以数据库中没有添加任何数据。 REQUIRED_NEW 使用演示REQUIRED_NEW 不支持当前事务。 UserController 实现代码： 123456789@RequestMapping(\"/save\")@Transactionalpublic Object save(User user) { // 插入用户操作 userService.save(user); // 插入日志 logService.saveLog(\"用户插入：\" + user.getName()); return true;} UserService 实现代码： 123456789101112@Servicepublic class UserService { @Resource private UserMapper userMapper; @Transactional(propagation = Propagation.REQUIRES_NEW) public int save(User user) { System.out.println(\"执行 save 方法.\"); return userMapper.save(user); }} LogService 实现代码： 123456789101112@Servicepublic class LogService { @Resource private LogMapper logMapper; @Transactional(propagation = Propagation.REQUIRES_NEW) public int saveLog(String content) { // 出现异常 int i = 10 / 0; return logMapper.saveLog(content); }} 程序执行结果： User 表中成功添加了一条用户数据，Log 表执行失败，没有加入任何数据，但它并没有影响到 UserController 中的事务执行。 通过以上结果可以看出：LogService 中使用的是单独的事务，虽然 LogService 中的事务执行失败了，但并没有影响 UserController 和 UserService 中的事务。 NESTED 使用演示NESTED 是嵌套事务。 UserController 实现代码如下： 1234567@RequestMapping(\"/save\")@Transactionalpublic Object save(User user) { // 插入用户操作 userService.save(user); return true;} UserService 实现代码如下： 12345678@Transactional(propagation = Propagation.NESTED)public int save(User user) { int result = userMapper.save(user); System.out.println(\"执行 save 方法.\"); // 插入日志 logService.saveLog(\"用户插入：\" + user.getName()); return result;} LogService 实现代码如下： 1234567@Transactional(propagation = Propagation.NESTED)public int saveLog(String content) { // 出现异常 int i = 10 / 0; return logMapper.saveLog(content);} 最终执行结果，用户表和日志表都没有添加任何数据。 执行流程描述： UserController 中调用了 UserService 的添加用户方法，UserService 使用 NESTED 循环嵌套事务，并成功执行了添加用户的方法。 UserService 中调用了 LogService 的添加方法，LogService 使用了 NESTED 循环嵌套事务，但在方法执行中出现的异常，因此回滚了当前事务。 因为 UserService 使用的是嵌套事务，所以发生回滚的事务是全局的，也就是说 UserService 中的添加用户方法也被回滚了，最终执行结果是用户表和日志表都没有添加任何数据。","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"Spirng事务隔离级别","slug":"spring/spring事务隔离级别","date":"2022-09-21T16:00:00.000Z","updated":"2023-07-23T12:35:36.005Z","comments":true,"path":"2022/09/22/spring/spring事务隔离级别/","link":"","permalink":"https://yichenfirst.github.io/2022/09/22/spring/spring%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","excerpt":"","text":"Spring事务隔离级别Spring 中的事务隔离级别和数据库中的事务隔离级别稍有不同，以 MySQL 为例，MySQL 的 InnoDB 引擎中的事务隔离级别有 4 种，而 Spring 中却包含了 5 种事务隔离级别。 什么是事务隔离级别事务隔离级别是对事务 4 大特性中隔离性的具体体现，使用事务隔离级别可以控制并发事务在同时执行时的某种行为。 比如，有两个事务同时操作同一张表，此时有一个事务修改了这张表的数据，但尚未提交事务，那么在另一个事务中，要不要（或者说能不能）看到其他事务尚未提交的数据呢？ 这个问题的答案就要看事务的隔离级别了，不同的事务隔离级别，对应的行为模式也是不一样的（有些隔离级别可以看到其他事务尚未提交的数据，有些事务隔离级别看不到其他事务尚未提交的数据），这就是事务隔离级别的作用。 Spring事务隔离级Sping 中的事务隔离级别有 5 种，它们分别是： DEFAULT：Spring 中默认的事务隔离级别，以连接的数据库的事务隔离级别为准； READ_UNCOMMITTED：读未提交，也叫未提交读，该隔离级别的事务可以看到其他事务中未提交的数据。该隔离级别因为可以读取到其他事务中未提交的数据，而未提交的数据可能会发生回滚，因此我们把该级别读取到的数据称之为脏数据，把这个问题称之为脏读； READ_COMMITTED：读已提交，也叫提交读，该隔离级别的事务能读取到已经提交事务的数据，因此它不会有脏读问题。但由于在事务的执行中可以读取到其他事务提交的结果，所以在不同时间的相同 SQL 查询中，可能会得到不同的结果，这种现象叫做不可重复读； REPEATABLE_READ：可重复读，它能确保同一事务多次查询的结果一致。但也会有新的问题，比如此级别的事务正在执行时，另一个事务成功的插入了某条数据，但因为它每次查询的结果都是一样的，所以会导致查询不到这条数据，自己重复插入时又失败（因为唯一约束的原因）。明明在事务中查询不到这条信息，但自己就是插入不进去，这就叫幻读 （Phantom Read）； SERIALIZABLE：串行化，最高的事务隔离级别，它会强制事务排序，使之不会发生冲突，从而解决了脏读、不可重复读和幻读问题，但因为执行效率低，所以真正使用的场景并不多。 所以，相比于 MySQL 的事务隔离级别，Spring 中多了一种 DEFAULT 的事务隔离级别。 事务隔离级别与问题的对应关系如下： 事务隔离级别 脏读 不可重复读 幻读 读未提交（read uncommitted） √ √ √ 读已提交（read committed） × √ √ 可重复读（repeatable read） × × √ 串行化（serializable） × × × 脏读：一个事务读取到了另一个事务修改的数据之后，后一个事务又进行了回滚操作，从而导致第一个事务读取的数据是错误的。 不可重复读：一个事务两次查询得到的结果不同，因为在两次查询中间，有另一个事务把数据修改了。 幻读：一个事务两次查询中得到的结果集不同，因为在两次查询中另一个事务有新增了一部分数据。 设置事务隔离界别编程式事务12345678910111213141516@Resourseprivate TrabsactionTemplate transactionTemplate;@RequestMapping(\"/add\")public int add(UserInfo userInfo){ transactionTemplate.setIsolationLevel(TranhsactionDefinition.ISOLATION_DEFAULT); return transactionTemplate.execute(status -&gt; { int result = 0; try{ result = userService.addUser(userInfo); } catch(Exception e){ status.setRollbackOnly(); } return result; })} 声明式事务1234567@Transactional(isolation = Isolation.DEFAULT)@RequestMapping(\"/\")public int addUser(UserInfo userInfo){ int result = userService.addUser(userInfo); return result;} 总结Spring 中的事务隔离级别比 MySQL 中的事务隔离级别多了一种，它包含的 5 种隔离级别分别是： Isolation.DEFAULT：默认的事务隔离级别，以连接的数据库的事务隔离级别为准。 Isolation.READ_UNCOMMITTED：读未提交，可以读取到未提交的事务，存在脏读。 Isolation.READ_COMMITTED：读已提交，只能读取到已经提交的事务，解决了脏读，存在不可重复读。 Isolation.REPEATABLE_READ：可重复读，解决了不可重复读，但存在幻读（MySQL 数据库默认的事务隔离级别）。 Isolation.SERIALIZABLE：串行化，可以解决所有并发问题，但性能太低。 但需要注意是 Spring 是事务隔离级别是建立在连接的数据库支持事务的基础上的，如果 Spring 项目连接的数据库不支持事务（或事务隔离级别），那么即使在 Spring 中设置了事务隔离级别，也是无效的设置","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"mysql总结","slug":"面试/mysql总结","date":"2022-08-20T16:00:00.000Z","updated":"2023-07-17T14:19:31.079Z","comments":true,"path":"2022/08/21/面试/mysql总结/","link":"","permalink":"https://yichenfirst.github.io/2022/08/21/%E9%9D%A2%E8%AF%95/mysql%E6%80%BB%E7%BB%93/","excerpt":"","text":"索引事务简述数据库中的 ACID 分别是什么？原子性：每个事务都是不可分割的最小工作单元，事务中的所有操作要么全成功，要么全失败。使用undo log实现回滚。 隔离性：各个事务之间相互隔离，互不干扰。通过锁和MVCC实现隔离 持久性：事务一旦提交，数据会永久的存储在数据库中。使用redo log实现故障恢复。 一致性：比如A向B转账，A减少1000，B就得增加1000，两人的余额总和不能变。一致性通过原子性+隔离性+持久性来保证。 数据库的事务隔离级别有哪些？各有哪些优缺点？ 事务隔离级别要解决的问题（事务并行导致的问题） 脏读 脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。 不可重复读 对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据更新（UPDATE）操作。 幻读 幻读是针对数据插入（INSERT）操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。 事务的隔离级别 读未提交（READ UNCOMMITTED） 读已提交 （READ COMMITTED） 可重复读 （REPEATABLE READ） 串行化 （SERIALIZABLE） 隔离级别 脏读 不可重复度 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 串行化 不可能 不可能 不可能 简述 MySQL MVCC 的实现原理MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo log ，Read View 来实现的 1、隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 2、undo日志 undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录 3、read view 说白了Read View就是事务进行快照读操作的时候生产的读视图，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 我们可以把Read View简单的理解成有三个全局属性 trx_ids 系统当前正在活跃的事务ID集合 up_limit_id 活跃的事务中最小的事务 ID low_limit_id ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1 creator_trx_id 创建这个 ReadView 的事务ID。 如果当前事务的 creator_trx_id 想要读取某个行记录，这个行记录ID 的 DB_TRX_ID，这样会有以下的情况： 如果 trx_id &lt; 活跃的最小事务ID（up_limit_id）,也就是说这个行记录在这些活跃的事务创建前就已经提交了，那么这个行记录对当前事务是可见的。 如果trx_id &gt;= low_limit_id，这个说明行记录在这些活跃的事务之后才创建，说明这个行记录对当前事务是不可见的。 如果 up_limit_id &lt;= trx_id &lt; low_limit_id,说明该记录需要在 trx_ids 集合中，可能还处于活跃状态，因此我们需要在 trx_ids 集合中遍历 ，如果trx_id 存在于 trx_ids 集合中，证明这个事务trx_id还处于活跃状态，不可见，否则 ，trx_id 不存在于 trx_ids 集合中，说明事务trx_id 已经提交了，这行记录是可见的。 【总之就是修改当前行的事务提交了，数据才能被查看】 MySQL中是如何实现事务隔离的读未提交它性能最好，直接读取最新的数据就行。 串行化读的时候加共享锁，也就是其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发写也不能并发读。 读已提交通过MVCC的read view实现，每个查询执行前都会重新生成一个read view。 「读已提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务 可重复读通过MVCC的read view实现，一个事务只有第一次查询才会生成一个read view。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 什么是数据库事务，MySQL 为什么会使用 InnoDB 作为默认选项 数据库事务 数据库事务是访问并可能操作各种数据项的一个数据库操作序列，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。 MySQL 为什么会使用 InnoDB 作为默认选项 InnoDB可靠性比较高，支持事务 InnoDB支持表锁和行锁，MyISAM只支持表级锁。 InnoDB支持外键 快照读与当前读 快照读：其实是基于MVCC+undo log实现的，读取的MVCC快照链路中的某个版本，很可能是历史版本，不用加锁。 当前读：读取的是记录的最新版本，并且返回的数据记录会加上锁，保证其他事务不能并发的修改数据记录。 当前读 当前读的场景： 123456789update .... (更新操作)delete .... (删除操作)insert .... (插入操作)select .... lock in share mode (共享读锁)select .... for update. (写锁) 当前读，读取的诗最新版本，并且对读取的记录加锁，阻塞其他事务同时修改相同记录，避免出现安全问题。 例如：假设要update一条记录，但是另一个事务已经delete这条数据并且commit了，如果不加锁就会产生冲突。所以update的时候肯定要是当前读，得到最新的信息并且锁定相应的记录。 快照读 快照读的场景： 1select .... (普通select操作，不包括当前读的lock in share mode和for update) Read Committed隔离级别：每次select都会生成一个快照读 Read Repeatable隔离级别：开启事务后第一个select语句才是快照读，而不是一开始事务就是快照读。 InnoDB是如何避免幻读的 针对快照读（普通select语句），是通过MVCC方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查不出来这条数据的，所以就很好的避免了幻读问题。 针对当前读（select … for update等语句），是通过next-key lock（临键锁，即记录锁+间隙锁）方式解决了幻读，因为当执行select … for update语句的时候，会加上next-key lock，如果有其他事务在next-key锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法插入成功，所以很好避免了幻读问题。 可重复读隔离级别完全解决幻读了吗没有， 第一个发生幻读现象的场景 id name score 1 小红 50 2 小明 60 3 小林 70 4 小蓝 80 事务A执行查询id=5的记录，此时表中是没有记录的，所以查不出来 12345begin;select * from stu where id = 5;(此时事务B开始执行)update stu set name = \"xiaozhang\" where id = 5;select * from stu where id = 5; 然后事务B插入一条id=5的记录，并提交了事务。 123begin;insert into stu values(5, '小美', 18);commit; 此时，事务A更新id=5这条记录，然后再次查询id=5的记录，事务A就能看到事务B插入的记录了，幻读就是发生在这种违和的场景。 时序图如下： 在可重复读隔离级别下，事务A第一次执行普通的select语句时生成一个read view， 之后事务B向表中插入了一条id=5的记录并提交。接着，事务A对id=5这条记录进行了更新操作，在这个时候，这条新记录的trx_id的值就变成了事务A的事务id，之后事务A在使用普通select语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。 第二个发生幻读的场景 T1时刻：事务A先执行「快照读语句」：select * from user where id &gt; 100得到3条记录 T2时刻：事务B插入一个id=200的记录并提交 T3时刻：事务A再执行「当前读语句」select * from test where id &gt; 100 for update就会得到4条记录，此时也发生了幻读现象。 要避免这特殊场景下发生幻读的现象，就是尽量在开启事务之后，马上执行select … for update这类当前读的语句，因为它会对记录加next-key lock，从而避免其他事务插入一条新记录。 锁MySQL中都有哪些锁 全局锁 表级锁 表锁 元数据锁 意向锁 AUTO-INC锁 行级锁 Record Lock Gap Lock Next-Key Lock 插入意向锁 全局锁全局锁的使用1flush tables with read lock 执行上面的语句后，整个数据库就处于只读状态了，这时其他线程执行一下操作，都会被阻塞： 对数据的增删改操作，比如insert、delete、update等语句 对表结构的更改操作，比如alter table、drop table等语句 1unlock tables 如果要释放全局锁，则要执行这条命令。 当回话断开了，全局锁就会被自动释放。 全局锁的应用场景全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期不一样。 全局锁存在的问题加上全局锁，意味着整个数据库都是只读状态 如果数据库中数据很多，备份就会花费很长时间，而且在备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。 如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建read view，然后整个事务执行期间都在用这个read view，由于mvcc机制，备份期间业务依然可以对数据进行更新操作。 但是对于MyISAM这种不支持事务的引擎，在备份数据库时就要使用全局锁的方式。 表级锁表锁1234lock tables student read // 对student表加读锁，表级别的共享锁lock tables student write // 对student表加写锁，表级别的独占锁unlock tables // 释放当前会话的所有表锁 应该尽量避免在InnoDB引擎的表中使用表锁，因为表的颗粒度太大，会影响并发性能。 元数据锁（MDL）我们不需要显式的使用元数据锁，因为在对数据库表进行操作时，会自动给这个表级上元数据锁。 对一张表进行CRUD操作时，加的是元数据读锁 对一张表做结构变更操作时，加的是元数据写锁； MDL是为了保证当用户对表执行CRUD操作时，防止其他线程对这个表结构做了变更。 当有线程在执行select语句（加MDL读锁）的期间，如果有其他线程要更改表的结构（申请MDL写锁），那么将会阻塞，知道执行完select语句（释放MDL读锁）。 意向锁 在使用InnoDB引擎的表里对某些记录加上共享锁之前，需要在表级别上加上一个意向共享锁 在使用InnoDB引擎的表里对某些记录加上独占锁之前，需要现在表级别加上一个意向独占锁 也就是，当执行插入、更新、删除操作，需要对表加上意向独占锁，然后对该记录加独占锁。 而普通的select是不会加行级锁的，普通的select语句是利用MVCC实现一致性读，是无锁的。 不过，select也是可以对记录加共享锁和独占锁的。 12345// 先表上加意向共享锁，然后对读取的记录加上共享锁select ..... lock in share mode;// 先表上加上意向独占锁，然后对读取的记录加独占锁select ..... for update; 意向独占锁和意向共享锁都是表级锁，不会和行级的独占锁和共享锁发生冲突，而意向锁之间也不会发生冲突，只会和共享表锁（lock tables … read）和独占表锁（lock tables … write）发生冲突。 如果没有意向锁，那么加独占表锁时，就需要遍历表中所有的记录，查看是否有记录存在独占锁，这样效率很低。 有了意向锁，由于对记录加独占锁前，先会加上表级别的意向独占锁，那么在加独占表锁时，直接查该表是否有意向独占锁，如果有意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。 AUTO-INC锁表里的逐主键通常会设置自增的，这是通过对主键字段声明AUTO_INCREMENT属性实现的。 之后就可以在插入数据时，不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过AUTO_INC锁实现的。 AUTO_INC锁时特殊的表锁机制，锁不是在一个事务提交后才释放的，而是执行完插入语句后就会立即释放。 插入数据时，会加一个表级别的AUTO_INC锁，然后为被AUTO-INCRMENT修饰的字段赋值递增的值，等插入语句执行完成后，才会把AUTO_INC锁释放掉。 行级锁InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。 前面也提到，普通的 select 语句是不会对记录加锁的，因为它属于快照读。如果要在查询时对记录加行锁，可以使用下面这两个方式，这种查询会加锁的语句称为锁定读。 12345//对读取的记录加共享锁select ... lock in share mode;//对读取的记录加独占锁select ... for update; 上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。 行级锁的类型主要有三类： Record Lock，记录锁，也就是仅仅把一条记录锁上； Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身； Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 Record LockRecord Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的： 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）; 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。 举个例子，当一个事务执行了下面的语句 12mysql &gt; begin;mysql &gt; select * from t_test where id = 1 for update; 就是对 t_test 表中主键 id 为 1 的这条记录加上 X 型的记录锁，这样其他事务就无法对这条记录进行修改了。 当事务执行 commit 后，事务过程中生成的锁都会被释放。 Gap LockGap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。 假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。 间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。 Next-Key LockNext-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。 next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。 比如，一个事务持有了范围为 (1, 10] 的 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，就会被阻塞。 虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。 ==插入意向锁==日志MySQL的日志为什么需要undo log在MySQL中执行一条增删改的语句的时候，虽然没有显式的输入begin开启事务和commit提交事务，但是MySQL会隐式开启事务来执行增删改的语句，执行完就自动提交事务。执行一条语句是否自动提交事务，是由autocommit参数决定的，默认是开启的。 那么当一个update语句执行过程中，MySQL发生了崩溃，要怎么回滚到事务之前的数据呢？ 如果我们每次在事务执行过程中，都记录下回滚时需要的信息到一个日志里，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据，我们可以通过这个日志回滚到事务之前的数据。 实现这一机制就是undo log（回滚日志），它保证了事务的ACID特性中的原子性。 undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。如下图： 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作。 不同的操作，需要记录的内容也是不同的，所以不同类型的操作（修改、删除、新增）产生的 undo log 的格式也是不同的，具体的每一个操作的 undo log 的格式我就不详细介绍了，感兴趣的可以自己去查查。 一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id： 通过 trx_id 可以知道该记录是被哪个事务修改的； 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链； 版本链如下图： 另外，undo log还有一个作用，通过ReadView + undo log实现MVCC（多版本并发控制）。 对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列（trx_id 和 roll_pointer）」的比对，如果不满足可见行，就会顺着 undo log 版本链里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 为什么需要redo logMySQL的数据都是存储在磁盘中的，当需要更新数据的时候需要现将磁盘中的数据读取到内存中，在内存中修改这条数据，然后在写回到磁盘中。这样就会导致一个问题，MySQL将内存中的数据修改完成并提交之后，万一机器断电重启，就会导致数据的丢失。 为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB引擎就会先更新内存，然后将本次的修改以redo log的形式记录下来，这个时候就算完成更新了。 后续，InnoDB 引擎会在适当的时候，由后台线程将内存中的数据刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 什么事redo log redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 redo log 和 undo log 有什么区别 这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； 事!务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，如下图： 所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log要写磁盘，数据也要写磁盘，为什么要多此一举？ 写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。 磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。 针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。 可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。 至此， 针对为什么需要 redo log 这个问题我们有两个答案： 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。 产生的redo log是直接写入磁盘的吗 不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图： redo log buffer 默认大小 16 MB，可以通过 innodb_log_Buffer_size 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。 redo log 什么时候刷盘？缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？ 主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。 innodb_flush_log_at_trx_commit 参数控制的是什么？ 单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式「顺序写」到磁盘。 上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。 除此之外，InnoDB 还提供了另外两种策略，由参数 innodb_flush_log_at_trx_commit 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下： 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。 innodb_flush_log_at_trx_commit 为 0 和 2 的时候，什么时候才将 redo log 写入磁盘？ InnoDB 的后台线程每隔 1 秒： 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失; 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 加入了后台现线程后，innodb_flush_log_at_trx_commit 的刷盘时机如下图： 这三个参数的应用场景是什么？ 这三个参数的数据安全性和写入性能的比较如下： 数据安全性：参数 1 &gt; 参数 2 &gt; 参数 0 写入性能：参数 0 &gt; 参数 2&gt; 参数 1 所以，数据安全性和写入性能是熊掌不可得兼的，要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性。 在一些对数据安全性要求比较高的场景中，显然 innodb_flush_log_at_trx_commit 参数需要设置为 1。 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高。 redo log文件写满了怎么办默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：ib_logfile0 和 ib_logfile1 。 在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。 重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。 所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。 我们知道 redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。 redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图： 图中的： write pos 和 checkpoint 的移动都是顺时针方向； write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作； check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录； 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。 所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。 为什么需要binlog前面介绍的 undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。 MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。 binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 为什么有了 binlog， 还要有 redo log？ 这个问题跟 MySQL 的时间线有关系。 最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。 而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。 redo log 和 binlog 有什么区别？这两个日志有四个区别。 1、适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 2、文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 3、写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 4、用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？ 不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 主从复制是怎么实现？MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。 这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。 MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。 具体详细过程如下： MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。 从库是不是越多越好？ 不是的。 因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。 所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构 MySQL 主从复制还有哪些模型？ 主要有三种： 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 binlog 什么时候刷盘？事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。 一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。这是因为有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。 MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图： 虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件： 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。 MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。 而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。 如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值 三个日志讲完了，至此我们可以先小结下，update 语句的执行过程。 当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。 具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个。 为什么需要两阶段提交事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 举个例子，假设 id = 1 这行数据的字段 name 的值原本是 ‘jay’，然后执行 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性； 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性； 两阶段提交的过程是怎样的？从下图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）； commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功； 异常重启会出现什么现象？情况一：一阶段提交之后崩溃了，即写入 redo log，处于 prepare 状态 的时候崩溃了，此时： 由于 binlog 还没写，redo log 处于 prepare 状态还没提交，所以崩溃恢复的时候，这个事务会回滚，此时 binlog 还没写，所以也不会传到备库。 情况二：假设写完 binlog 之后崩溃了，此时： redolog 中的日志是不完整的，处于 prepare 状态，还没有提交，那么恢复的时候，首先检查 binlog 中的事务是否存在并且完整，如果存在且完整，则直接提交事务，如果不存在或者不完整，则回滚事务。 情况三：假设 redolog 处于 commit 状态的时候崩溃了，那么重启后的处理方案同情况二。 由此可见，两阶段提交能够确保数据的一致性。 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计? binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。 也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。 有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？ 放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。 所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 两阶段提交有什么问题两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响： 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 为什么两阶段提交的磁盘 I/O 次数会很高？ binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1： 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘； 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘； 可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。 为什么锁竞争激烈？ 在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。 通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。","categories":[{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"}]},{"title":"mysql事务","slug":"mysql/mysql事务","date":"2022-08-14T16:00:00.000Z","updated":"2023-07-17T13:41:27.724Z","comments":true,"path":"2022/08/15/mysql/mysql事务/","link":"","permalink":"https://yichenfirst.github.io/2022/08/15/mysql/mysql%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"简述数据库中的 ACID 分别是什么？原子性：每个事务都是不可分割的最小工作单元，事务中的所有操作要么全成功，要么全失败。使用undo log实现回滚。 隔离性：各个事务之间相互隔离，互不干扰。通过锁和MVCC实现隔离 持久性：事务一旦提交，数据会永久的存储在数据库中。使用redo log实现故障恢复。 一致性：比如A向B转账，A减少1000，B就得增加1000，两人的余额总和不能变。一致性通过原子性+隔离性+持久性来保证。 数据库的事务隔离级别有哪些？各有哪些优缺点？ 事务隔离级别要解决的问题（事务并行导致的问题） 脏读 脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。 不可重复读 对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据更新（UPDATE）操作。 幻读 幻读是针对数据插入（INSERT）操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。 事务的隔离级别 读未提交（READ UNCOMMITTED） 读已提交 （READ COMMITTED） 可重复读 （REPEATABLE READ） 串行化 （SERIALIZABLE） 隔离级别 脏读 不可重复度 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 串行化 不可能 不可能 不可能 简述 MySQL MVCC 的实现原理MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo log ，Read View 来实现的 1、隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 2、undo日志 undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录 3、read view 说白了Read View就是事务进行快照读操作的时候生产的读视图，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 我们可以把Read View简单的理解成有三个全局属性 trx_ids 系统当前正在活跃的事务ID集合 up_limit_id 活跃的事务中最小的事务 ID low_limit_id ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1 creator_trx_id 创建这个 ReadView 的事务ID。 如果当前事务的 creator_trx_id 想要读取某个行记录，这个行记录ID 的 DB_TRX_ID，这样会有以下的情况： 如果 trx_id &lt; 活跃的最小事务ID（up_limit_id）,也就是说这个行记录在这些活跃的事务创建前就已经提交了，那么这个行记录对当前事务是可见的。 如果trx_id &gt;= low_limit_id，这个说明行记录在这些活跃的事务之后才创建，说明这个行记录对当前事务是不可见的。 如果 up_limit_id &lt;= trx_id &lt; low_limit_id,说明该记录需要在 trx_ids 集合中，可能还处于活跃状态，因此我们需要在 trx_ids 集合中遍历 ，如果trx_id 存在于 trx_ids 集合中，证明这个事务trx_id还处于活跃状态，不可见，否则 ，trx_id 不存在于 trx_ids 集合中，说明事务trx_id 已经提交了，这行记录是可见的。 【总之就是修改当前行的事务提交了，数据才能被查看】 MySQL中是如何实现事务隔离的读未提交它性能最好，直接读取最新的数据就行。 串行化读的时候加共享锁，也就是其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发写也不能并发读。 读已提交通过MVCC的read view实现，每个查询执行前都会重新生成一个read view。 「读已提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务 可重复读通过MVCC的read view实现，一个事务只有第一次查询才会生成一个read view。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 什么是数据库事务，MySQL 为什么会使用 InnoDB 作为默认选项 数据库事务 数据库事务是访问并可能操作各种数据项的一个数据库操作序列，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。 MySQL 为什么会使用 InnoDB 作为默认选项 InnoDB可靠性比较高，支持事务 InnoDB支持表锁和行锁，MyISAM只支持表级锁。 InnoDB支持外键 快照读与当前读 快照读：其实是基于MVCC+undo log实现的，读取的MVCC快照链路中的某个版本，很可能是历史版本，不用加锁。 当前读：读取的是记录的最新版本，并且返回的数据记录会加上锁，保证其他事务不能并发的修改数据记录。 当前读 当前读的场景： 123456789update .... (更新操作)delete .... (删除操作)insert .... (插入操作)select .... lock in share mode (共享读锁)select .... for update. (写锁) 当前读，读取的诗最新版本，并且对读取的记录加锁，阻塞其他事务同时修改相同记录，避免出现安全问题。 例如：假设要update一条记录，但是另一个事务已经delete这条数据并且commit了，如果不加锁就会产生冲突。所以update的时候肯定要是当前读，得到最新的信息并且锁定相应的记录。 快照读 快照读的场景： 1select .... (普通select操作，不包括当前读的lock in share mode和for update) Read Committed隔离级别：每次select都会生成一个快照读 Read Repeatable隔离级别：开启事务后第一个select语句才是快照读，而不是一开始事务就是快照读。 InnoDB是如何避免幻读的 针对快照读（普通select语句），是通过MVCC方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查不出来这条数据的，所以就很好的避免了幻读问题。 针对当前读（select … for update等语句），是通过next-key lock（临键锁，即记录锁+间隙锁）方式解决了幻读，因为当执行select … for update语句的时候，会加上next-key lock，如果有其他事务在next-key锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法插入成功，所以很好避免了幻读问题。 可重复读隔离级别完全解决幻读了吗没有， 第一个发生幻读现象的场景 id name score 1 小红 50 2 小明 60 3 小林 70 4 小蓝 80 事务A执行查询id=5的记录，此时表中是没有记录的，所以查不出来 12345begin;select * from stu where id = 5;(此时事务B开始执行)update stu set name = \"xiaozhang\" where id = 5;select * from stu where id = 5; 然后事务B插入一条id=5的记录，并提交了事务。 123begin;insert into stu values(5, '小美', 18);commit; 此时，事务A更新id=5这条记录，然后再次查询id=5的记录，事务A就能看到事务B插入的记录了，幻读就是发生在这种违和的场景。 时序图如下： 在可重复读隔离级别下，事务A第一次执行普通的select语句时生成一个read view， 之后事务B向表中插入了一条id=5的记录并提交。接着，事务A对id=5这条记录进行了更新操作，在这个时候，这条新记录的trx_id的值就变成了事务A的事务id，之后事务A在使用普通select语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。 第二个发生幻读的场景 T1时刻：事务A先执行「快照读语句」：select * from user where id &gt; 100得到3条记录 T2时刻：事务B插入一个id=200的记录并提交 T3时刻：事务A再执行「当前读语句」select * from test where id &gt; 100 for update就会得到4条记录，此时也发生了幻读现象。 要避免这特殊场景下发生幻读的现象，就是尽量在开启事务之后，马上执行select … for update这类当前读的语句，因为它会对记录加next-key lock，从而避免其他事务插入一条新记录。","categories":[{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"},{"name":"总结","slug":"总结","permalink":"https://yichenfirst.github.io/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"count(*)、count(1)、count(主键)、count(列字段)效率比较","slug":"mysql/count函数效率","date":"2022-06-03T16:00:00.000Z","updated":"2023-07-17T14:19:31.079Z","comments":true,"path":"2022/06/04/mysql/count函数效率/","link":"","permalink":"https://yichenfirst.github.io/2022/06/04/mysql/count%E5%87%BD%E6%95%B0%E6%95%88%E7%8E%87/","excerpt":"","text":"count(*)、count(1)、count(主键)、count(列字段) count()是什么 count()是一个聚合函数，函数参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是统计符合条件的记录中，函数指定的参数不为NULL的记录有多少个。 假设count（）函数的参数是字段名，如下 1select count(name) from user 这条语句统计的是user表中，name字段不为NULL的记录。 假设count（）函数的参数是数字1这个表达式，如下： 1select count(1) from user 这条语句是统计user表中，1这个表达式不为NULL的记录。因为1这个表达式永远不为NULL，所以这条语句实际是在统计user中有多少条记录。 count(主键字段) 在通过 count 函数统计有多少个记录时，MySQL 的 server 层会维护一个名叫 count 的变量。 server 层会循环向 InnoDB 读取一条记录，如果 count 函数指定的参数不为 NULL，那么就会将变量 count 加 1，直到符合查询的全部记录被读完，就退出循环。最后将 count 变量的值发送给客户端。 用下面的语句举例： 12// id为主键值select count(id) from user; 如果表里只有主键索引，没有二级索引时，那么，InnoDB 循环遍历聚簇索引，将读取到的记录返回给 server 层，然后读取记录中的 id 值，就会 id 值判断是否为 NULL，如果不为 NULL，就将 count 变量加 1。 但是，如果表里有二级索引时，InnoDB循环遍历的对象就不是聚簇索引，而是二级索引。 因为相同数量的记录，二级索引比聚簇索引占用更少的空间（聚簇索引叶子结点保存所有的数据，二级索引保存主键和索引字段的值），这样遍历二级索引的I/O成本就比遍历聚簇索引的I/O成本小。 count(1) 如果表里只有主键索引，没有二级索引时。 那么，InnoDB 循环遍历聚簇索引（主键索引），将读取到的记录返回给 server 层，但是不会读取记录中的任何字段的值，因为 count 函数的参数是 1，不是字段，所以不需要读取记录中的字段值。参数 1 很明显并不是 NULL，因此 server 层每从 InnoDB 读取到一条记录，就将 count 变量加 1。 可以看到，count(1) 相比 count(主键字段) 少一个步骤，就是不需要读取记录中的字段值，所以通常会说 count(1) 执行效率会比 count(主键字段) 高一点。 但是，如果表里有二级索引时，InnoDB 循环遍历的对象就二级索引了。 count(*) 当使用count( * )时，mysql会当成count(0)处理，所以count( * )的执行过程跟count(1)执行过程基本一致。 而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用key_len 最小的二级索引进行扫描。 只有当没有二级索引的时候，才会采用主键索引来进行统计。 count(字段) count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) 执行效率是最差的。 12// name不是索引，是普通字段select count(name) from user; 对于这个查询来说，会采用全表扫描的方式来计数，所以它的执行效率是比较差的。 性能高低进行排序 count(1)≈count(*)&gt;count(主键ID)&gt;count(column)","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"}]},{"title":"操作系统笔记","slug":"操作系统/操作系统笔记","date":"2022-04-08T16:00:00.000Z","updated":"2023-07-17T13:41:27.794Z","comments":true,"path":"2022/04/09/操作系统/操作系统笔记/","link":"","permalink":"https://yichenfirst.github.io/2022/04/09/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/","excerpt":"","text":"概述特征 并发 同一时间间隔内执行和调度多个程序的能力 宏观上，处理机同时执行多道程序 微观上，处理机在多道程序间高速切换（分时交替执行) 关注单个处理机同一时间段内处理任务数量的能力 共享 即资源共享，系统中的资源供多个并发执行的应用程序共同使用 同时访问方式：同一时段允许多个程序同时访问共享资源 互斥共享方式：也叫独占式，允许多个程序在同一个共享资源上独立而互不干扰的工作 虚拟 使用某种技术把一个物理实体变成多个逻辑上的对应物。 时分复用技术 虚拟处理机：四核八线程 空分服用技术 虚拟磁盘技术： 将一块硬盘虚拟出若干个卷 异步 系统进程用一种走走停停的方式执行，（并不是一下子走完） 进程什么时候以怎样的速度向前推进是不可预知的 发展阶段 手工操作阶段 无操作系统、人工操作、用户独占全机 批处理阶段 单道批处理系统，内存中只有一道程序 多道批处理系统， 分时操作系统 一台主机连接多个带有显示器和键盘的终端，同时允许多个用户通过自己的终端，以交互方式使用计算机，共享主机中的资源。 特征： 多路性：时间片轮转机制 独立性：用户彼此独立 及时性：用户能在短时间内获得相应 交互性：用户可以请求多种服务 缺点： 作业/用户优先级相同，不能优先处理紧急任务 实时操作系统 系统能即时相应外部事件的请求，在规定时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。 微机操作系统 用户空间和内核空间 时钟管理计时：提供系统时间 时钟中断：比如程序切换 中断机制提高多道程序环境下CPU利用率 外中断：中断信号来源于外部设备（被迫中断） 内中断：中断信号来源于当前指令（主动中断） 内中断的三种情况 陷阱/陷入（Trap）：由应用程序主动引发 故障（fault）：由错误条件引发 终止（abort）：由致命错误引发 中断处理过程 当CPU执行到指令3时，产生了一个中断 1、中断产生后，首先进行关中断，CPU对中断做出一个基本响应。保存断点，保存的程序计数器。引出中断服务程序，只是读取中断程序并没有执行。保存现场和屏蔽字，保存CPU寄存器中的数据。开中断。 关中断之后，CPU不会再响应更高级中断请求，相当于一个加锁。开中断相当于解锁。 2、执行中断程序，此时CPU可以并发执行其他的中断 3、关中断，恢复现场和屏蔽字，开中断，中断返回。 原语原语是一个由若干条指令组成的程序段，用来完成某个特定功能，且执行过程不会被中断，具有原子性，运行在内核空间。原语底层通过关中断和开中断实现。 系统调用应用程序调用系统内核程序功能的过程叫做系统调用，通过陷入指令进行的系统调用，也就是通过中断。 陷入指令是在用户空间执行的 进程管理什么是进程进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动，是系统进行资源分配和调度的一个独立单位 几个要点： 进程是程序的一次执行 进程是一个程序及其数据在处理机上顺序执行所发生的活动 进行是程序在一个数据集合上运行的过程 进程是系统进行资源分配和调度的一个独立单位（或基本单位） 进程的结构 控制块 数据段 程序段 进程特征动态性：有创建而生，由撤销而亡 并发性：多个进成可以同时运行 独立性：独立资源分配 异步性：相互独立、互补干扰 什么是线程Thread，进程的轻型实体，也叫“轻量级进程”，是一系列活动按事先设定好的顺序依次执行的过程，是一系列指令的集合 是一条执行路径，不能单独存在，必须包含在进程中 线程是OS中运算调度的最小单位 ==进程与线程比较==线程的实现方式 进程的状态 三种基本状态 就绪状态当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。 执行状态进程已获得CPU，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态;在多处理机系统中，则有多个进程处于执行状态。 阻塞状态正在执行的进程由于发生某事件而暂时无法继续执行时,便放弃处理机而处于暂停状态，亦即进程的执行受到阻塞，把这种暂停状态称为阻塞状态，有时也称为等待状态或封锁状态。致使进程阻塞的典型事件有:请求IO，申请缓冲空间等。通常将这种处于阻塞状态的进程也排成一个队列。有的系统则根据阻塞原因的不同而把处于阻塞状态的进程排成多个队列。 创建和终止 创建状态和终止状态 进程的控制os对进程实现有效的管理，包括创建新进程、撤销已有进程、挂起、阻塞和唤醒、进程切换等多种操作。OS通过原语操作实现进程控制。 原语由若干条指令组成，完成特定的功能，是一种原子操作。 常见原语： 挂起与激活为了系统和用户观察和分析进程 挂起原语：suspend 静止就绪：放外存，不调度 静止阻塞：等待事件 激活原语：active 活动就绪：等待调度 活动阻塞：等待唤醒 进程的调度进程调度也叫处理机调度，即根据一定的算法和原则将处理机资源进行重新分配的过程。 前提：作业/进程数远远大于处理器数 目的：提高资源利用率，减少处理机空闲时间 调度程序：一方面要满足特定系统用户的需求(快速响应)，另一方面要考虑系统整体效率（系统平均周转时间）和调度算法本身的开销。 调度方式剥夺式/抢占式调度 立即暂停当前进程 分配处理机给另一个进程 原则：优先权、短进程优先、时间片原则 非剥夺/非抢占式调度 若有进程请求执行 等待直到当前进程完成或阻塞 缺点：适用于批处理系统，不适合分时、实时系统 调度时机 进程运行完毕 进程时间片用完 进程要求I/O操作 执行某种原语操作 高优先级进程申请运行（剥夺式调度） 调度过程保存镜像：记录进程现场信息 调度算法：确定分配处理机的原则 进程切换：分配处理机给其他进程 处理机回收：从进程回收处理机 调度算法指标CPU利用率 ：忙碌时间/总时间 系统吞吐量： 完成作业数/总时间 周转时间：作业时间-提交时间 等待时间：作业等待处理调度时间 响应时间：提交请求到首次响应间隔 调度算法作业调度：将位于外存后背队列中的某几个作业调入内存排在就绪队列上（作业磁盘到内存或从内存到磁盘的调度） 进程调度：从就绪队列中选取一个进程，分配处理机的过程 先来先服务 算法内容：调度作业/就绪队列中最先入队者，等待操作完成或阻塞 算法原则：按作业/进程达到顺序服务 调度方式：非抢占式调度 使用场景：作业/进程调度 优缺点： 有利于CPU繁忙型作业，充分利用CPU资源 不利于I/O繁忙型作业，操作耗时、其他饥饿 短作业优先 算法内容：所需服务时间最短的作业/进程优先服务 算法原则：追求最少的平均周转时间 调度方式：非抢占 使用场景：作业/进程调度 优缺点： 平均等待/周转时间最少 长作业周转时间会增加或饥饿 估计时间不准确、不能保证急迫任务及时执行 高响应比优先 算法内容：结合先来先服务和短作业优先，综合考虑等待时间和服务时间计算响应比，高的优先调度 算法原则：综合考虑作业/进程的等待事件和服务时间 调度方式：非抢占 使用场景：作业/进程调度 响应比计算： 响应比 = （等待时间+服务时间）/ 服务时间， &gt;= 1（一般来说每个进程的服务时间是确定的，等待时间越长，响应比越大） 只有当前进程放弃执行权（完成/阻塞）时，重新计算所有进程响应比 长作业等待越久响应比越高，更容易获得处理机 优先级调度 算法内容：又叫优先权调度，按作业/进程的优先级进行调度 算法原则：优先级最高(最紧迫)的作业/进程先调度 调度方式：抢占/非抢占 使用场景：作业/进程调度 优先级设置： 静态/动态优先级 系统&gt;用户，交互型&gt;非交互型；I/O型&gt;计算型 低优先级进程可能会产生饥饿 时间片轮转 算法内容：按进程到达就绪队列的顺序，轮流分配一个时间片去执行，时间用完则剥夺 算法原则：公平、轮流为每个进程服务，进程在一定时间内都能得到响应 调度方式：抢占，由时钟中断确定时间到 使用场景：进程调度（作业调度一般只是将作业从磁盘调度到内存一次，不需要多次） 优缺点： 公平，响应快，适用于分时系统 时间片决定因素：系统响应时间、就绪队列进程数量、系统梳理能力 时间片太大，相当于先来先服务；太小，处理机借还频繁，开销增大 多级反馈队列调度算法 算法内容：设置多个优先级排序的就绪队列，优先级从高到底，时间片从小到大。新进程采用队列降级法，进入第一队列，按先来先服务分配时间片，当没有执行完移动到第二级，第三级…，前一队列不为空，不执行后续队列进程。 算法原则：集合前几种优点，时间片轮转+优先级 调度方式：抢占式 使用场景：进程调度 优缺点： 对各类型相对公平；快速响应 终端型（交互型，需要及时处理）作业用户：短作业优先 批处理作业用户：周转时间短 长批处理作业用户：在前几个队列部分执行 进程通信概念：进程之间的信息交换 进程是资源分配的基本单位，各进程内存空间彼此独立 一个进程不能随意访问其他进程的地址空间 方式： 共享存储 消息传递 管道通信 共享存储基于共享数据结构的通信方式(操作系统已经设计好的数据结构) 多个进程共用某个数据结构（OS提供并控制) 由用户（程序员)负责同步处理 低级通信:可以传递少量数据，效率低 基于共享存储区的通信方式 多个进程共用内存中的一块存储区域 由进程控制数据的形式和方式方式 高级通信:可以传递大量数据，效率高 消息传递直接通信:点到点发送 发送和接收时指明双方进程的ID 每个进程维护一个消息缓冲队列 间接通信:广播信箱 以信箱为媒介，作为中间实体 发进程将消息发送到信箱，收进程从信箱读取 可以广播，容易建立双向通信链 管道通信管道 用于连接读/写进程的共享文件，pipe文件 本质是内存中固定大小的缓冲区 半双工通信 同一时段只能单向通信，双工通信需要两个管道 以先进先出(FIFO)方式组织数据传输 通过系统调用read()/write()函数进行读写操作 进程同步协调进程间的相互制约关系，使他们按照预期的方式执行的过程 前提 进程是并发执行的，进程间存在着相互制约关系 并发的进程对系统共享资源进行竞争 进程通信，过程中相互发送的信号称为消息或事件 两种相互制约形式： 间接相互制约关系(互斥)︰进程排他性地访问共享资源 直接相互制约关系(同步)︰进程间的合作，比如管道通信 互斥的访问临界资源访问过程： 1.进入区:尝试进入临界区，成功则加锁( lock) 2.临界区:访问共享资源 3.退出区︰解锁(unlock)，唤醒其它阻塞进程 4.剩余区:其它代码 访问原则： 空闲让进: 临界区空闲，允许一个进程进入 忙则等待: 临界区已有进程，其它进程等待（阻塞状态) 有限等待: 处于等待的进程，等待时间有限 让权等待: 等待时应让出CPU执行权，防止“忙等待” 软件实现互斥的实现方法 硬件实现互斥的实现方法 中断屏蔽方法：关中断/开中断 禁止一切中断，CPU执行完临界区之前不会切换 关中断可能会被滥用 关中断时间长影响效率 不适用于多处理机，无法防止其它处理机调度其它进程访问临界区 只适用于内核进程（该指令运行在内核态) Test-And-Set（TS指令/TSL指令） 读出标志并设置为true，返回旧值，原子操作 违背“让权等待”，会发生忙等 Swap指令 交换两个变量的值，原子操作 违背“让权等待” 信号量信号量是操作系统用来解决并发中的互斥和同步问题的一种方法 PV操作： P操作：wait原语，进程等待 V操作：signal原语，唤醒等待程序 整型信号量：违背让权等待，会发生忙等 记录型信号量：进程进入阻塞状态，不会忙等 内存管理内存管理概念进程运行的基本原理 用户程序到进程的步骤 编译（由用户来完成，不属于操作系统） 链接 静态链接（在运行前链接，适合比较小的程序） 装入时动态链接 运行时动态链接 装入（将文件加载到内存中） 绝对装入（程序员设置装入到内存的哪个位置） 可重定位装入 动态运行时装入 内存分配连续分配管理方式 单一连续分配方式 这是最简单的一种存储管理方式，但只能用于单用户、单任务的操作系统中。采用这种存储管理方式时，可把内存分为系统区和用户区两部分，系统区仅提供给 OS 使用，通常 是放在内存的低址部分；用户区是指除系统区以外的全部内存空间，提供给用户使用。 优点： 实现简单，无外部碎片，不一定需要内存保护 缺点： 只能用户单用户、单任务OS，有内部碎片，存储器利用率低 固定分区分配 固定分区式分配是最简单的一种可运行多道程序的存储管理方式。这是将内存用户空间划分为若干个固定大小的区域，在每个分区中只装入一道作业，这样，把用户空间划分为几个分区，便允许有几道作业并发运行。当有一空闲分区时，便可以再从外存的后备作业队列中选择一个适当大小的作业装入该分区，当该作业结束时，又可再从后备作业队列中找出另一作业调入该分区。 动态分区分配 动态分区分配是根据进程的实际需要，动态地为之分配内存空间。在实现可变分区分配时，将涉及到分区分配中所用的数据结构、分区分配算法和分区的分配与回收操作这样三个问题。 分配方法：不会先划分内存区域，当进程进入内存的时候才会根据进程大小动态的为其建立分区，使分区大小刚好适合进程的需要。 特点：在开始是很好的，进程一次按照顺序存入内存，但是运行久了以后随着进程的消亡，会出现很多成段的内存空间，时间越来越长就会导致很多不可利用的外部碎片，降低内存的利用率。这时需要分配算法来解决 非连续分配管理方式可以将一个进程分散的装入内存分区。根据分区的大小是否固定可以分成分页存储管理(固定)与分段存储管理(不固定)，为了避免两者的缺点，还可以二者混用成段页式存储管理。再根据进程运行作业时是否将作业的的全部代码装入内存，又分为基本分页存储管理(全部装入内存)和请求分页存储管理(非一次全装入内存)。 基本分页存储管理 用户程序的地址空间被划分成若干固定大小的区域，称为“页”，相应地，内存空间分成若干个物理块，页和块的大小相等。可将用户程序的任一页放在内存的任一块中，实现了离散分配。 基本地址变换机构 物理地址 = （页号-&gt;块号）+ 偏移量 页号P = 逻辑地址A/页面长度（大小）L 偏移量W = 逻辑地址A% 页面长度L p = A &gt;&gt; 12；W = A &amp; 4095 基本分段存储管理 分页：用户程序的地址空间被划分成若干固定大小的区域，称为“页”，相应地，内存空间分成若干个物理块，页和块的大小相等。可将用户程序的任一页放在内存的任一块中，实现了离散分配。 分段：将用户程序地址空间分成若干个大小不等的段，每段可以定义一组相对完整的逻辑信息(如主程序，子程序，共享数据在不同的段中)。存储分配时，以段为单位，段与段在内存中可以不相邻接，也实现了离散分配。 分段与分页区别： (1)页是信息的物理单位,分页是为了实现非连续分配,以便解决内存碎片问题,或者说分页是由于系统管理的需要。段是信息的逻辑单位,它含有一组意义相对完整的信息,分段的目的是为了更好地实现共享,满足用户的需要。(2)页的大小固定,由系统确定,将逻辑地址划分为页号和页内地址是由机器硬件实现的。而段的长度却不固定,决定于用户所编写的程序,通常由编译程序在对源程序进行编译时根据信息的性质来划分。(3)分页的作业地址空间是一维的。分段的地址空间是二维的。 段页式管理 先分段，在分页 1个进程——-&gt;1个段表 1个段表项—–&gt;1个页表 1个页表——-&gt;多个物理块 虚拟内存","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"jwt","slug":"token","date":"2022-02-25T16:00:00.000Z","updated":"2023-07-17T13:41:27.714Z","comments":true,"path":"2022/02/26/token/","link":"","permalink":"https://yichenfirst.github.io/2022/02/26/token/","excerpt":"","text":"token结构token有三个部分，用.分隔， Header Payload Signature 通常格式如下： xxxxx.yyyyy.zzzzz Header标头通常由两部分组成：令牌的类型，即 JWT，以及正在使用的签名算法，例如 HMAC SHA256 或 RSA 例如： 1234{ \"alg\": \"HS256\", \"typ\": \"JWT\"} 然后，这个 JSON 被Base64Url编码以形成 JWT 的第一部分。 Payload令牌的第二部分是负载，其中包含声明。声明是关于实体（通常是用户）和附加数据的声明。共有三种类型的声明：注册声明、公共声明和私人声明 例如 12345{ \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true} 然后对有效负载进行Base64Url编码以形成 JSON Web 令牌的第二部分。 ​ 请注意，对于已签名的令牌，此信息虽然受到防篡改保护，但任何人都可以读取。除非加密，否则不要将机密信息放入 JWT 的负载或标头元素中。 Signature要创建签名部分，您必须获取编码的header、编码的payload、密钥、header中指定的算法，并对其进行签名。 例如，如果要使用 HMAC SHA256 算法，则签名将通过以下方式创建： 12345// secret为秘钥HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) JWT如何防止Token篡改JSON Web令牌以紧凑的形式由三部分组成，这些部分由点(.)分隔，分别是：头(Header)、有效载荷(Playload)、签名(Signature)； Header：对TokenUtil.header（含有加密算法）进行Base64Url编码得到jwt的第一部分； Playload：存放有效信息的地方，Base64Url编码得到第二部分； Signature：是整个数据的认证信息。一般根据前两步的数据，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第3部分 此时 signature字段就是关键了，能被解密出明文的，只有header和payload 假如黑客/中间人串改了payload，那么服务器可以通过signature去验证是否被篡改过 在服务端在执行一次 signature = 加密算法(header + “.” + payload, 密钥);, 然后对比 signature 是否一致，如果一致则说明没有被篡改。 所以为什么说服务器的密钥，也就是例子中的secretKey.getBytes()不能被泄漏。只要密钥不被泄露，Signature无法正确，所以就会被服务器识别出来伪造信息。 1234HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret)","categories":[],"tags":[{"name":"token","slug":"token","permalink":"https://yichenfirst.github.io/tags/token/"}]},{"title":"SpringSecurity笔记","slug":"spring/SpringSecurity笔记","date":"2022-02-03T16:00:00.000Z","updated":"2023-07-17T13:41:27.774Z","comments":true,"path":"2022/02/04/spring/SpringSecurity笔记/","link":"","permalink":"https://yichenfirst.github.io/2022/02/04/spring/SpringSecurity%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Springsecurity初体验第一个例子1、创建项目 2、加入spring security依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 3、创建Controller 1234567891011121314package com.yichen.security.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class UserController { @GetMapping(\"/hello\") public String hello(){ return \"hello\"; }} 4、启动项目 日志中生成登录密码，用户名默认为user 5、访问接口 访问”/hello”接口首先会跳转到 “/login”，登录后才会跳转到”/hello” username： user password：c9e14ac9-3d66-4544-a10a-a9194c04544c（日志中显示的，UUID字符串） 配置文件中设置用户名密码12spring.security.user.name=adminspring.security.user.password=admin 使用内存中的用户信息SecurityConfig.java 123456789101112131415161718192021222324252627282930313233package com.yichen.security.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.password.NoOpPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder(){ return NoOpPasswordEncoder.getInstance(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication() .withUser(\"admin\") .password(\"123\").roles(\"admin\") .and() .withUser(\"user1\") .password(\"chen\") .roles(\"admin\");// 添加多个用户信息// auth.inMemoryAuthentication()// .withUser(\"admin2\")// .password(\"123\").roles(\"admin\"); }} 首先我们自定义 SecurityConfig 继承自 WebSecurityConfigurerAdapter，重写里边的 configure 方法。 首先我们提供了一个 PasswordEncoder 的实例，因为目前的案例还比较简单，因此我暂时先不给密码进行加密，所以返回 NoOpPasswordEncoder 的实例即可。 configure 方法中，我们通过 inMemoryAuthentication 来开启在内存中定义用户，withUser 中是用户名，password 中则是用户密码，roles 中是用户角色。 如果需要配置多个用户，用 and 相连。 ​ 在没有 Spring Boot 的时候，我们都是 SSM 中使用 Spring Security，这种时候都是在 XML 文件中配置 Spring Security，既然是 XML 文件，标签就有开始有结束，现在的 and 符号相当于就是 XML 标签的结束符，表示结束当前标签，这是个时候上下文会回到 inMemoryAuthentication 方法中，然后开启新用户的配置。 定制表单登录自定义表单登录页然后接下来我们继续完善前面的 SecurityConfig 类，继续重写它的 configure(WebSecurity web) 和 configure(HttpSecurity http) 方法，如下 SecurityConfig.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.yichen.security.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.password.NoOpPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder(){ return NoOpPasswordEncoder.getInstance(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication() .withUser(\"admin\") .password(\"123\").roles(\"admin\") .and() .withUser(\"user1\") .password(\"chen\") .roles(\"admin\");// auth.inMemoryAuthentication()// .withUser(\"admin2\")// .password(\"123\").roles(\"admin\"); } @Override protected void configure(HttpSecurity http) throws Exception{ http.authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/Login.html\") .loginProcessingUrl(\"/login\") .permitAll() .and() .csrf().disable(); }} resources/static/login.html 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=\"/login\" method=\"post\"&gt; &lt;div class=\"input\"&gt; &lt;label for=\"name\"&gt;用户名&lt;/label&gt; &lt;input type=\"text\" name=\"username\" id=\"name\"&gt; &lt;span class=\"spin\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\"input\"&gt; &lt;label for=\"pass\"&gt;密码&lt;/label&gt; &lt;input type=\"password\" name=\"password\" id=\"pass\"&gt; &lt;span class=\"spin\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\"button login\"&gt; &lt;button type=\"submit\"&gt; &lt;span&gt;登录&lt;/span&gt; &lt;i class=\"fa fa-check\"&gt;&lt;/i&gt; &lt;/button&gt; &lt;/div&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; ​ HttpSecurity 提供了很多配置相关的方法，分别对应命名空间配置中的子标签 。例如，authorizeRequests（）、formLogin（）、httpBasic（）和 csrf（）分别对应 、、 和 标签。调用这些方法之后，除非使用 and（）方法结束当前标签，上下文才会回到 HttpSecurity，否则链式调用的上下文将自动进入对应标签域。 ​ authorizeRequests（）方法实际上返回了一个 URL 拦截注册器，我们可以调用它提供的 anyanyRequest（）、antMatchers（）和 regexMatchers（）等方法来匹配系统的 URL，并为其指定安全策略。 ​ formLogin（）方法和 httpBasic（）方法都声明了需要 Spring Security 提供的表单认证方式，分别返回对应的配置器。其中，formLogin（）.loginPage（\"Login.html\"） 指定自定义的登录页/Login.html，同时，Spring Security 会用/Login.html 注册一个POST路由，用于接收登录请求。 ​ csrf（）方法是 Spring Security 提供的跨站请求伪造防护功能，当我们继承 WebSecurityConfigurer Adapter 时会默认开启 csrf（）方法。 自定义登录参数我们的登录表单中的参数是 username 和 password，注意，默认情况下，这个不能变 1234567&lt;form action=\"/login\" method=\"post\"&gt; &lt;input type=\"text\" name=\"username\" id=\"name\"&gt; &lt;input type=\"password\" name=\"password\" id=\"pass\"&gt; &lt;button type=\"submit\"&gt; &lt;span&gt;登录&lt;/span&gt; &lt;/button&gt;&lt;/form&gt; 当然，这两个参数我们也可以自己配置，自己配置方式如下： 12345678.and().formLogin().loginPage(\"/login.html\").loginProcessingUrl(\"/doLogin\").usernameParameter(\"usern\").passwordParameter(\"passw\").permitAll().and() 12345678910111213141516171819&lt;form action=\"/login.html\" method=\"post\"&gt; &lt;div class=\"input\"&gt; &lt;label for=\"name\"&gt;用户名&lt;/label&gt; &lt;input type=\"text\" name=\"usern\" id=\"name\"&gt; &lt;span class=\"spin\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\"input\"&gt; &lt;label for=\"pass\"&gt;密码&lt;/label&gt; &lt;input type=\"password\" name=\"passw\" id=\"pass\"&gt; &lt;span class=\"spin\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\"button login\"&gt; &lt;button type=\"submit\"&gt; &lt;span&gt;登录&lt;/span&gt; &lt;i class=\"fa fa-check\"&gt;&lt;/i&gt; &lt;/button&gt; &lt;/div&gt;&lt;/form&gt; ==注意修改 input 的 name 属性值和服务端的对应== 登录回调（前后端不分离）在登录成功之后，我们就要分情况处理了，大体上来说，无非就是分为两种情况： 前后端分离登录 前后端不分登录 两种情况的处理方式不一样。本文我们先来卡第二种前后端不分的登录，前后端分离的登录回调我在下篇文章中再来和大家细说。 登录成功 在 Spring Security 中，和登录成功重定向 URL 相关的方法有两个： defaultSuccessUrl successForwardUrl 这两个咋看没什么区别，实际上内藏乾坤。 首先我们在配置的时候，defaultSuccessUrl 和 successForwardUrl 只需要配置一个即可，具体配置哪个，则要看你的需求，两个的区别如下： defaultSuccessUrl 有一个重载的方法，我们先说一个参数的 defaultSuccessUrl 方法。如果我们在 defaultSuccessUrl 中指定登录成功的跳转页面为 /index，此时分两种情况，如果你是直接在浏览器中输入的登录地址，登录成功后，就直接跳转到 /index，如果你是在浏览器中输入了其他地址，例如 http://localhost:8080/hello，结果因为没有登录，又重定向到登录页面，此时登录成功后，就不会来到 /index ，而是来到 /hello 页面。 defaultSuccessUrl 还有一个重载的方法，第二个参数如果不设置默认为 false，也就是我们上面的的情况，如果手动设置第二个参数为 true，则 defaultSuccessUrl 的效果和 successForwardUrl 一致。 successForwardUrl 表示不管你是从哪里来的，登录后一律跳转到 successForwardUrl 指定的地址。例如 successForwardUrl 指定的地址为 /index ，你在浏览器地址栏输入 http://localhost:8080/hello，结果因为没有登录，重定向到登录页面，当你登录成功之后，就会服务端跳转到 /index 页面；或者你直接就在浏览器输入了登录页面地址，登录成功后也是来到 /index。 12345678910.and().formLogin().loginPage(\"/login.html\").loginProcessingUrl(\"/doLogin\").usernameParameter(\"name\").passwordParameter(\"passwd\").defaultSuccessUrl(\"/index\").successForwardUrl(\"/index\").permitAll().and() 注意：实际操作中，defaultSuccessUrl 和 successForwardUrl 只需要配置一个即可。 登录失败 与登录成功相似，登录失败也是有两个方法： failureForwardUrl failureUrl 「这两个方法在设置的时候也是设置一个即可」。failureForwardUrl 是登录失败之后会发生服务端跳转，failureUrl 则在登录失败之后，会发生重定向。 注销登录（前后端不分离）注销登录的默认接口是 /logout，我们也可以配置。 12345678.and().logout().logoutUrl(\"/logout\").logoutSuccessUrl(\"/index\").deleteCookies().clearAuthentication(true).invalidateHttpSession(true).permitAll() 默认注销的 URL 是 /logout，是一个 GET 请求，我们可以通过 logoutUrl 方法来修改默认的注销 URL。 logoutRequestMatcher 方法不仅可以修改注销 URL，还可以修改请求方式，实际项目中，这个方法和 logoutUrl 任意设置一个即可。 logoutSuccessUrl 表示注销成功后要跳转的页面。 deleteCookies 用来清除 cookie。 clearAuthentication 和invalidateHttpSession分别表示清除认证信息和使 HttpSession 失效，默认可以不用配置，默认就会清除。 logout.html 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=\"/logout\" method=\"post\"&gt; &lt;input type=\"submit\" value=\"注销\"/&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 登录回调（前后端分离）之前我们配置登录成功的处理是通过如下两个方法来配置的： defaultSuccessUrl successForwardUrl 这两个都是配置跳转地址的，适用于前后端不分的开发。除了这两个方法之外，还有一个必杀技，那就是 successHandler。 successHandler 的功能十分强大，甚至已经囊括了 defaultSuccessUrl 和 successForwardUrl 的功能。我们来看一下： 12345678.successHandler((req, resp, authentication) -&gt; { Object principal = authentication.getPrincipal(); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(principal)); out.flush(); out.close();}) successHandler 方法的参数是一个 AuthenticationSuccessHandler 对象，这个对象中我们要实现的方法是 onAuthenticationSuccess。 onAuthenticationSuccess 方法有三个参数，分别是： HttpServletRequest HttpServletResponse Authentication 有了前两个参数，我们就可以在这里随心所欲的返回数据了。利用 HttpServletRequest 我们可以做服务端跳转，利用 HttpServletResponse 我们可以做客户端跳转，当然，也可以返回 JSON 数据。 第三个Authentication参数则保存了我们刚刚登录成功的用户信息。 配置完成后，我们再去登录，就可以看到登录成功的用户信息通过 JSON 返回到前端了，如下： 登录回调与注销登录（前后端分离）SecurityConfig.java 12345678910111213141516171819202122232425262728293031323334353637@Overrideprotected void configure(HttpSecurity http) throws Exception{ http.authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login.html\") .successHandler((req, resp, authentication) -&gt; { Object principal = authentication.getPrincipal(); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(principal)); out.flush(); out.close(); }) .failureHandler((req, resp, e) -&gt; { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(e.getMessage()); out.flush(); out.close(); }) .permitAll() .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessHandler((req, resp, authentication) -&gt; { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"注销成功\"); out.flush(); out.close(); }) .permitAll() .and() .csrf().disable().exceptionHandling();} 授权所谓的授权，就是用户如果要访问某一个资源，我们要去检查用户是否具备这样的权限，如果具备就允许访问，如果不具备，则不允许访问。 准备测试用户因为我们现在还没有连接数据库，所以测试用户还是基于内存来配置。 基于内存配置测试用户，我们有两种方式，第一种就是我们本系列前面几篇文章用的配置方式，如下： 12345678910@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication() .withUser(\"admin\") .password(\"123\").roles(\"admin\") .and() .withUser(\"user\") .password(\"123\") .roles(\"user\");} 这是一种配置方式。 由于 Spring Security 支持多种数据源，例如内存、数据库、LDAP 等，这些不同来源的数据被共同封装成了一个 UserDetailService 接口，任何实现了该接口的对象都可以作为认证数据源。 因此我们还可以通过重写 WebSecurityConfigurerAdapter 中的 userDetailsService 方法来提供一个 UserDetailService 实例进而配置多个用户： 1234567@Beanprotected UserDetailsService userDetailsService() { InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(\"admin\").password(\"123\").roles(\"admin\").build()); manager.createUser(User.withUsername(\"user\").password(\"123\").roles(\"user\").build()); return manager;} 准备测试接口123456789101112131415@GetMapping(\"/hello\")public String hello(){ return \"hello\";}@GetMapping(\"/admin/hello\")public String admin() { return \"admin\";}@GetMapping(\"/user/hello\")public String user() { return \"user\";} 这三个测试接口，我们的规划是这样的： /hello 是任何人都可以访问的接口 /admin/hello 是具有admin身份的人才能访问的接口 /user/hello 是具有user身份的人才能访问的接口 所有 user 能够访问的资源，admin 都能够访问 「注意第四条规范意味着所有具备 admin 身份的人自动具备 user 身份。」 配置接下来我们来配置权限的拦截规则，在 Spring Security 的 configure(HttpSecurity http) 方法中，代码如下： 1234567http.authorizeRequests() .antMatchers(\"/admin/**\").hasRole(\"admin\") .antMatchers(\"/user/**\").hasRole(\"user\") .anyRequest().authenticated() .and() ... ... 这里的匹配规则我们采用了 Ant 风格的路径匹配符，Ant 风格的路径匹配符在 Spring 家族中使用非常广泛，它的匹配规则也非常简单 通配符 含义 ** 匹配多层路径 * 匹配一层路径 ? 匹配任意单个字符 上面配置的含义是： 如果请求路径满足 /admin/** 格式，则用户需要具备 admin 角色。 如果请求路径满足 /user/** 格式，则用户需要具备user角色。 剩余的其他格式的请求路径，只需要认证（登录）后就可以访问。 注意代码中配置的三条规则的顺序非常重要，和 Shiro 类似，Spring Security 在匹配的时候也是按照从上往下的顺序来匹配，一旦匹配到了就不继续匹配了，「所以拦截规则的顺序不能写错」。 另一方面，如果你强制将 anyRequest 配置在antMatchers前面，像下面这样： 12345http.authorizeRequests() .anyRequest().authenticated() .antMatchers(\"/admin/**\").hasRole(\"admin\") .antMatchers(\"/user/**\").hasRole(\"user\") .and() 此时项目在启动的时候，就会报错，会提示不能在 anyRequest 之后添加 antMatchers： 这从语义上很好理解，anyRequest 已经包含了其他请求了，在它之后如果还配置其他请求也没有任何意义。 从语义上理解，anyRequest 应该放在最后，表示除了前面拦截规则之外，剩下的请求要如何处理 登录成功后，分别访问 /hello，/admin/hello 以及 /user/hello 三个接口，其中： /hello 因为登录后就可以访问，这个接口访问成功。 /admin/hello 需要 admin 身份，所以访问失败。 /user/hello 需要 user 身份，所以访问成功 角色继承所有 user 能够访问的资源，admin 都能够访问，很明显我们目前的代码还不具备这样的功能。 要实现所有 user 能够访问的资源，admin 都能够访问，这涉及到另外一个知识点，叫做角色继承。 这在实际开发中非常有用。 上级可能具备下级的所有权限，如果使用角色继承，这个功能就很好实现，我们只需要在 SecurityConfig 中添加如下代码来配置角色继承关系即可： 123456@BeanRoleHierarchy roleHierarchy() { RoleHierarchyImpl hierarchy = new RoleHierarchyImpl(); hierarchy.setHierarchy(\"ROLE_admin &gt; ROLE_user\"); return hierarchy;} 注意，在配置时，需要给角色手动加上 ROLE_ 前缀。上面的配置表示 ROLE_admin 自动具备 ROLE_user 的权限。 配置完成后，重启项目，此时我们发现 admin也能访问 /user/hello 这个接口了。 SpringSecurity+mybatis数据库脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for role-- ----------------------------DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` int(11) NOT NULL, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `name_zh` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of role-- ----------------------------INSERT INTO `role` VALUES (1, 'admin', '管理员');INSERT INTO `role` VALUES (2, 'dba', '超级管理员');INSERT INTO `role` VALUES (3, 'user', '用户');-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `password` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES (4, 'user1', '$2a$10$bcKxT0JEe4DaZDgFabtxbuu9t8kqXMoXiry5RWgOeWtCW2yk6FMRS');INSERT INTO `user` VALUES (5, 'admin1', '$2a$10$CzJCvNcmbLDWts9/TRgHcONT/.L4nogyS6PR4QHQtfVsjBYPJOPHK');INSERT INTO `user` VALUES (6, 'dba', '$2a$10$/7aOl1EYM78J1ypa.njp5um9bXTIdU1zkytYGF7tzUc1xsYmBCruO');-- ------------------------------ Table structure for user_role-- ----------------------------DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `id` int(11) NOT NULL, `uid` int(11) NULL DEFAULT NULL, `rid` int(11) NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of user_role-- ----------------------------INSERT INTO `user_role` VALUES (1, 4, 3);INSERT INTO `user_role` VALUES (2, 5, 1);INSERT INTO `user_role` VALUES (3, 6, 2);SET FOREIGN_KEY_CHECKS = 1; 定义实体类User.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.yichen.security.model;import lombok.Data;import org.springframework.security.core.GrantedAuthority;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.UserDetails;import java.util.ArrayList;import java.util.Collection;import java.util.List;@Datapublic class User implements UserDetails { private Integer id; private String username; private String password; private List&lt;Role&gt; roles; @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() { List&lt;SimpleGrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); for (Role role : getRoles()) { authorities.add(new SimpleGrantedAuthority(\"ROLE_\"+role.getName())); } return authorities; } @Override public boolean isAccountNonExpired() { return true; } @Override public boolean isAccountNonLocked() { return true; } @Override public boolean isCredentialsNonExpired() { return true; } @Override public boolean isEnabled() { return true; }} Role.java 123456789101112package com.yichen.security.model;import lombok.Data;@Datapublic class Role { private Integer id; private String name; private String nameZh;} MapperUserMapper.java 1234567891011121314151617181920package com.yichen.security.mapper;import com.yichen.security.model.Role;import com.yichen.security.model.User;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Select;import java.util.List;@Mapperpublic interface UserMapper { @Select(\"select * from user where username=#{username}\") User findUserByUsername(String username); @Select(\"select * from role where id in (select rid from user_role where uid=#{id})\") List&lt;Role&gt; getUserRolesById(Integer id);} Service层UserService.java 123456789101112131415161718192021222324252627package com.yichen.security.service;import com.yichen.security.mapper.UserMapper;import com.yichen.security.model.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.core.userdetails.UsernameNotFoundException;import org.springframework.stereotype.Service;@Servicepublic class UserService implements UserDetailsService { @Autowired UserMapper userMapper; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException { User user = userMapper.findUserByUsername(username); if (user == null) { throw new UsernameNotFoundException(\"用户不存在!\"); } user.setRoles(userMapper.getUserRolesById(user.getId())); System.out.println(user); return user; }} ConfigSecurityConfig.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.yichen.security.config;import com.yichen.security.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.access.hierarchicalroles.RoleHierarchy;import org.springframework.security.access.hierarchicalroles.RoleHierarchyImpl;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired UserService userService; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.userDetailsService(userService); } @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); } @Bean RoleHierarchy roleHierarchy() { RoleHierarchyImpl roleHierarchy = new RoleHierarchyImpl(); String hierarchy = \"ROLE_dba &gt; ROLE_admin \\n ROLE_admin &gt; ROLE_user\"; roleHierarchy.setHierarchy(hierarchy); return roleHierarchy; } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/dba/**\").hasRole(\"dba\") .antMatchers(\"/admin/**\").hasRole(\"admin\") .antMatchers(\"/user/**\").hasRole(\"user\") .anyRequest().authenticated() .and() .formLogin() .permitAll() .and() .csrf().disable(); }} Controller层UserController.java 1234567891011121314151617181920212223242526272829package com.yichen.security.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class UserController { @GetMapping(\"/hello\") public String hello(){ return \"hello\"; } @GetMapping(\"/user/hello\") public String hello2(){ return \"user\"; } @GetMapping(\"/dba/hello\") public String hello3(){ return \"dba\"; } @GetMapping(\"/admin/hello\") public String hello4(){ return \"admin\"; }} 实现效果： 1、dba角色可以访问/user/hello，/admin/hello,，/dba/hello，/hello 2、admin角色可以访问/user/hello，/admin/hello，/hello 3、user角色可以访问/user/hello，/hello","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"},{"name":"springsecurity","slug":"springsecurity","permalink":"https://yichenfirst.github.io/tags/springsecurity/"}]},{"title":"Spring AOP介绍","slug":"spring/Spring AOP介绍","date":"2022-01-06T16:00:00.000Z","updated":"2023-07-17T13:41:27.774Z","comments":true,"path":"2022/01/07/spring/Spring AOP介绍/","link":"","permalink":"https://yichenfirst.github.io/2022/01/07/spring/Spring%20AOP%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"AOP什么是AOP面向切面思想，是Spring的三大核心思想之一（两外两个：IOC-控制反转、DI-依赖注入） AOP优点 在不改变原有功能代码的基础上扩展新的功能实现——OCP原则。 可以简化代码开发提高效率。 可以将非核心业务代码将业务层抽离。 解决问题 解决分离问题 水平分离：展示层-&gt;服务层-&gt;持久层 垂直分离：模块划分（如订单、库存） 切面分离：分离功能性需求和非功能性需求 应用场景 权限控制 缓存控制 审计日志 性能监控 异常处理 分布式追踪… 传统OOP与AOPAOP、OOP在字面上虽然非常类似，但却是面向不同领域的两种设计思想。 OOP（面向对象编程）针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。 而AOP（面向切面编程）则是针对业务处理过程中的切面进行提取，它所面对的是处理过程中的某个步骤或阶段，以获得逻辑过程中各部分之间低耦合性的隔离效果。这两种设计思想在目标上有着本质的差异。 AOP体系简单地去理解，其实AOP要做三类事： 在哪里切入，也就是权限校验等非业务操作在哪些业务代码中执行。 在什么时候切入，是业务代码执行前还是执行后。 切入后做什么事，比如做权限校验、日志记录等。 一些概念详解： Pointcut：切点，决定处理如权限校验、日志记录等在何处切入业务代码中（即织入切面）。切点分为execution方式和annotation方式。前者可以用路径表达式指定哪些类织入切面，后者可以指定被哪些注解修饰的代码织入切面。Advice：处理，包括处理时机和处理内容。处理内容就是要做什么事，比如校验权限和记录日志。处理时机就是在什么时机执行处理内容，分为前置处理（即业务代码执行前）、后置处理（业务代码执行后）等。Aspect：切面，即Pointcut和Advice。Joint point：连接点，是程序执行的一个点。例如，一个方法的执行或者一个异常的处理。在 Spring AOP 中，一个连接点总是代表一个方法执行。Weaving：织入，就是通过动态代理，在目标对象方法中执行处理内容的过程。 实例annotation方式导入AOP依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 目录结构 12345678910com--yichen--------aop-------------annotation------------------TestAspect-------------aspect------------------TestAspect-------------service------------------TestService--------AopApplication TestAspect 123456789101112package com.yichen.aop.annotation;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface TestAspect {} TestAspect.java 123456789101112131415161718192021222324package com.yichen.aop.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;@Aspect@Componentpublic class TestAspect { @Pointcut(\"@annotation(com.yichen.aop.annotation.TestAspect)\") public void test(){ } @Before(\"test()\") public void before(JoinPoint joinPoint) { System.out.println(\"-------【前置通知】-------\" + joinPoint); }} TestService.java 12345678910111213141516171819202122package com.yichen.aop.service;import com.yichen.aop.annotation.TestAspect;import org.springframework.stereotype.Service;@Servicepublic class TestService { @TestAspect public void insert(String str){ System.out.println(\"插入一条数据\"); } public void delete(){ System.out.println(\"删除一条数据\"); } public void update(){ System.out.println(\"更新一条数据\"); }} 运行结果 1234-------【前置通知】-------execution(void com.yichen.aop.service.TestService.insert(String))插入一条数据更新一条数据删除一条数据 execution方式TestAspect.java 1234567891011121314151617181920212223package com.yichen.aop.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;@Aspect@Componentpublic class TestAspect { @Pointcut(\"execution(public * com.yichen.aop.service.TestService.*(..))\") public void test(){ } @Before(\"test()\") public void before(JoinPoint joinPoint) { System.out.println(\"-------【前置通知】-------\" + joinPoint); }} TestService.java 123456789101112131415161718192021package com.yichen.aop.service;import com.yichen.aop.annotation.TestAspect;import org.springframework.stereotype.Service;@Servicepublic class TestService { public void insert(String str){ System.out.println(\"插入一条数据\"); } public void delete(){ System.out.println(\"删除一条数据\"); } public void update(){ System.out.println(\"更新一条数据\"); }} 运行结果 123456-------【前置通知】-------execution(void com.yichen.aop.service.TestService.insert(String))插入一条数据-------【前置通知】-------execution(void com.yichen.aop.service.TestService.update())更新一条数据-------【前置通知】-------execution(void com.yichen.aop.service.TestService.delete())删除一条数据 多个Aspect切面这些切面类执行顺序由@Order注解管理，该注解后的数字越小，所在切面类越先执行。 运行结果 TestAspect.java 12345678910111213141516171819202122232425package com.yichen.aop.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;@Aspect@Component@Order(1)public class TestAspect { @Pointcut(\"execution(public * com.yichen.aop.service.TestService.*(..))\") public void test(){ } @Before(\"test()\") public void before(JoinPoint joinPoint) { System.out.println(\"-------【TestAspect前置通知】-------\" + joinPoint); }} TestAspect2.java 123456789101112131415161718192021222324package com.yichen.aop.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;@Aspect@Component@Order(2)public class TestAspect2 { @Pointcut(\"execution(public * com.yichen.aop.service.TestService.*(..))\") public void test(){ } @Before(\"test()\") public void before(JoinPoint joinPoint) { System.out.println(\"-------【TestAspect2前置通知】-------\" + joinPoint); }} 123456789-------【TestAspect前置通知】-------execution(void com.yichen.aop.service.TestService.insert(String))-------【TestAspect2前置通知】-------execution(void com.yichen.aop.service.TestService.insert(String))插入一条数据-------【TestAspect前置通知】-------execution(void com.yichen.aop.service.TestService.update())-------【TestAspect2前置通知】-------execution(void com.yichen.aop.service.TestService.update())更新一条数据-------【TestAspect前置通知】-------execution(void com.yichen.aop.service.TestService.delete())-------【TestAspect2前置通知】-------execution(void com.yichen.aop.service.TestService.delete())删除一条数据 AOP相关注解@pointcut@Pointcut 注解，用来定义一个切点，即上文中所关注的某件事情的入口，切入点定义了事件触发时机。 1234567891011@Aspect@Componentpublic class LogAspectHandler { /** * 定义一个切面，拦截 com.mutest.controller 包和子包下的所有方法 */ @Pointcut(\"execution(* com.mutest.controller..*.*(..))\") public void pointCut() {}} @Pointcut 注解指定一个切点，定义需要拦截的东西，这里介绍两个常用的表达式：一个是使用 execution()，另一个是使用 annotation()。 以 execution(* com.mutest.controller..*.*(..))) 表达式为例： 第一个 * 号的位置：表示返回值类型，* 表示所有类型。包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包，在本例中指 com.mutest.controller包、子包下所有类的方法。第二个 * 号的位置：表示类名，* 表示所有类。*(..)：这个星号表示方法名， 表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数。 annotation() 表达式： annotation() 方式是针对某个注解来定义切点，比如我们对具有 @PostMapping 注解的方法做切面，可以如下定义切面： 12@Pointcut(\"@annotation(org.springframework.web.bind.annotation.PostMapping)\")public void annotationPointcut() {} 然后使用该切面的话，就会切入注解是 @PostMapping 的所有方法。这种方式很适合处理 @GetMapping、@PostMapping、@DeleteMapping不同注解有各种特定处理逻辑的场景。 @Around@Around注解用于修饰Around增强处理，Around增强处理非常强大，表现在： @Around可以自由选择增强动作与目标方法的执行顺序，也就是说可以在增强动作前后，甚至过程中执行目标方法。这个特性的实现在于，调用ProceedingJoinPoint参数的procedd()方法才会执行目标方法。@Around可以改变执行目标方法的参数值，也可以改变执行目标方法之后的返回值。 Around增强处理有以下特点： 当定义一个Around增强处理方法时，该方法的第一个形参必须是 ProceedingJoinPoint 类型（至少一个形参）。在增强处理方法体内，调用ProceedingJoinPoint的proceed方法才会执行目标方法：这就是@Around增强处理可以完全控制目标方法执行时机、如何执行的关键；如果程序没有调用ProceedingJoinPoint的proceed方法，则目标方法不会执行。 调用ProceedingJoinPoint的proceed方法时，还可以传入一个Object[ ]对象，该数组中的值将被传入目标方法作为实参——这就是Around增强处理方法可以改变目标方法参数值的关键。这就是如果传入的Object[ ]数组长度与目标方法所需要的参数个数不相等，或者Object[ ]数组元素与目标方法所需参数的类型不匹配，程序就会出现异常。 @Around功能虽然强大，但通常需要在线程安全的环境下使用。因此，如果使用普通的Before、AfterReturning就能解决的问题，就没有必要使用Around了。如果需要目标方法执行之前和之后共享某种状态数据，则应该考虑使用Around。尤其是需要使用增强处理阻止目标的执行，或需要改变目标方法的返回值时，则只能使用Around增强处理了。 1234567@Around(value = \"test()\")public Object aroud(ProceedingJoinPoint joinPoint) throws Throwable { System.out.println(\"-------【环绕通知前】-------\" ); Object obj = joinPoint.proceed(); //执行目标方法 System.out.println(\"-------【环绕通知后】--------\"); return obj;} @Before@Before 注解指定的方法在切面切入目标方法之前执行，可以做一些 Log 处理，也可以做一些信息的统计，比如获取用户的请求 URL 以及用户的 IP 地址等等，这个在做个人站点的时候都能用得到，都是常用的方法。例如下面代码 123456789101112131415161718192021222324252627282930313233343536373839404142package com.yichen.aop.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import org.springframework.web.context.request.RequestContextHolder;import org.springframework.web.context.request.ServletRequestAttributes;import javax.servlet.http.HttpServletRequest;import java.util.Arrays;@Aspect@Component@Order(2)public class TestAspect2 { private Logger logger = LoggerFactory.getLogger(TestAspect.class); @Pointcut(\"execution(public * com.yichen.aop.service.TestService.*(..))\") public void test(){ } @Before(\"test()\") public void before(JoinPoint joinPoint) { ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); System.out.println(\"request = \" + request); // 记录下请求内容 logger.info(\"URL : \" + request.getRequestURL().toString()); logger.info(\"HTTP_METHOD : \" + request.getMethod()); logger.info(\"IP : \" + request.getRemoteAddr()); logger.info(\"CLASS_METHOD : \" + joinPoint.getSignature().getDeclaringTypeName() + \".\" + joinPoint.getSignature().getName()); logger.info(\"ARGS : \" + Arrays.toString(joinPoint.getArgs())); System.out.println(\"-------【TestAspect2前置通知】-------\" + joinPoint); }} @After@After 注解和 @Before 注解相对应，指定的方法在切面切入目标方法之后执行，也可以做一些完成某方法之后的 Log 处理。 123456789101112131415161718192021222324@Aspect@Component@Slf4jpublic class LogAspectHandler { /** * 定义一个切面，拦截 com.mutest.controller 包下的所有方法 */ @Pointcut(\"execution(* com.mutest.controller..*.*(..))\") public void pointCut() {} /** * 在上面定义的切面方法之后执行该方法 * @param joinPoint jointPoint */ @After(\"pointCut()\") public void doAfter(JoinPoint joinPoint) { log.info(\"==== doAfter 方法进入了====\"); Signature signature = joinPoint.getSignature(); String method = signature.getName(); log.info(\"方法{}已经执行完\", method); }} @AfterReturning@AfterReturning (目标方法有返回值且正常返回后执行)注解和 @After 有些类似，区别在于 @AfterReturning 注解可以用来捕获切入方法执行完之后的返回值，对返回值进行业务逻辑上的增强处理 @AfterThrowing当被切方法执行过程中抛出异常时，会进入 @AfterThrowing 注解的方法中执行，在该方法中可以做一些异常的处理逻辑。要注意的是 throwing 属性的值必须要和参数一致，否则会报错。该方法中的第二个入参即为抛出的异常。 总结 注解 用途 @Pointcut 定义切入点 @Before 目标方法执行之前执行 @After 目标方法执行之后必定执行，无论是否报错 @AfterReturning 目标方法有返回值且正常返回后执行 @AfterThrowing 目标方法抛出异常后执行 @Around 可以获取到目标方法的入参和返回值 参考： springboot aop的使用 切面AOP实现权限校验：实例演示与注解全解","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"},{"name":"aop","slug":"aop","permalink":"https://yichenfirst.github.io/tags/aop/"}]},{"title":"redis基础","slug":"redis/redis基础","date":"2022-01-06T16:00:00.000Z","updated":"2023-07-17T13:41:27.814Z","comments":true,"path":"2022/01/07/redis/redis基础/","link":"","permalink":"https://yichenfirst.github.io/2022/01/07/redis/redis%E5%9F%BA%E7%A1%80/","excerpt":"","text":"1、什么是NoSQL NoSQL NoSQL = No Only SQL(不仅仅是SQL) 泛指非关系型数据库。 NoSQL特点 解耦 方便扩展（数据之间没有关系，很好扩展） 大数据量性能（Redis一秒写8万次，读取11万次，NoSQL的缓存记录级，是一种细粒度的缓存，性能比较高） 数据类型是多样的 传统RDBMS和NoSQL 12345678传统RDBMS- 结构化组织- SQL- 数据和关系都存在单独的表中- 严格的一致性- 基础的事物- ..... 12345678NoSQL- 不仅仅是数据- 没有固定的查询语言- 键值对存储，列存储，文档存储，图形数据库，社交关系- 最终一致性- CAP定理 和 BSAE（异地多活）- 高性能，高可用，高可扩- ..... 真正在公司的实践：NoSQL + RDBMS ​ NoSQL的四大分类KV键值对: 新浪：Redis 美团：Redis + Tair 阿里、百度 ： Redis + memecache 文档数据库： MongoDB MongoDB是一个基于分布式文件存储的数据库，C++编写，主要用来处理大量的文档 MongoDB是一个介于关系型数据库和非关系型数据库的中间产品，MongoDB是非关系型数据库中功能最丰富，最像关系型数据库的 列存储数据库： HBase 分布式文件系统 图关系数据库： 不是存图形的，放的是关系，比如：朋友圈社交网络，广告推荐 Neo4j，InfoGrid 2、Redis入门概述 什么是Redis Redis（Remote Dictionary Server )，即远程字典服务 是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 Redis能干嘛 1、内存存储、持久化，内存中是断点即失（rdb，aof） 2、效率高 3、发布订阅系统 4、地图信息分析 5、计时器，计数器（浏览量） 6、……. 特性 1、多样的数据类型 2、持久化 3、集群 4、事物 5、…….. Linux安装1、下载安装包 2、解压 1tar -zxvf redis-6.2.3.tar.gz 3、进入redis目录 4、基本的环境安装 123456sudo apt install gccsudo apt-get install make# 进入redis的目录makemake install 5、修改redis后台启动 1sudo apt install vim 6、启动 12redis-server redis.confredis-cli 性能测试redis-benchmark是一个性能测试工具 12# 测试100个并发连接，100000请求redis-benchmark -h localhost -p 6379 -c 100 -n 100000 基础知识12345678910# window Redis启动redis-server redis.windows.confredis-cliC:\\Users\\chen&gt;redis-cli# shutdown断开连接127.0.0.1:6379&gt; shutdown# 退出redisnot connected&gt; exit Redis默认有16个数据库（配置文件中），使用的是第0个数据库 可以使用select进行切换数据库 12345C:\\Users\\chen&gt;redis-cli127.0.0.1:6379&gt; select 3 # 切换数据库OK127.0.0.1:6379[3]&gt; dbsize # 查看db大小(integer) 0 12345127.0.0.1:6379[3]&gt; set name chen # 设置值OK127.0.0.1:6379[3]&gt; dbsize(integer) 1127.0.0.1:6379[3]&gt; get name # 获取值 12127.0.0.1:6379[3]&gt; keys * # 查看数据库中所有的key1) \"name\" 清除当前数据库==flushdb== 1234127.0.0.1:6379[3]&gt; flushdbOK127.0.0.1:6379[3]&gt; dbsize(integer) 0 清除所有数据库的内容==flushall== Redis是单线程的！ Redis是基于内存操作，CPU不是Redis的瓶颈，Redis的瓶颈是根据机器的内存和网络带宽 Redis是C语言写的，官方提供数据为100000+QPS，完全不比同样适用key-value的Menmecache差！ Redis为什么单线程还这么快？ 1、误区1：高性能的服务器一定是多线程？ 2、误区2：多线程（CPU上下文切换！）一定比单线程效率高 核心：redis是将所有的数据全部放在内存中，所以使用单线程操作效率就是最高的，多线程（CPU上下文切换：耗时的操作），对于内存来说，如果没有上下文切换效率就是最高的 3、五大数据类型 翻译：Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 Redis-Key123456789101112127.0.0.1:6379[3]&gt; set name chenOK127.0.0.1:6379[3]&gt; set age 1OK127.0.0.1:6379[3]&gt; keys *1) \"age\"2) \"name\"127.0.0.1:6379[3]&gt; exists name # 判断key是否存在(integer) 1127.0.0.1:6379[3]&gt; exists name1(integer) 0 123456789101112127.0.0.1:6379[3]&gt; expire age 10 # 设置key的过期时间，单位是秒(integer) 1127.0.0.1:6379[3]&gt; ttl age # 查看当前key的剩余时间(integer) 8127.0.0.1:6379[3]&gt; ttl age(integer) 6127.0.0.1:6379[3]&gt; ttl age(integer) 3127.0.0.1:6379[3]&gt; ttl age(integer) 1127.0.0.1:6379[3]&gt; ttl age(integer) -2 12345127.0.0.1:6379[3]&gt; set name chenOK127.0.0.1:6379[3]&gt; type name # 判断key的类型string127.0.0.1:6379[3]&gt; String123456789101112131415################################################################### append# strlen127.0.0.1:6379[3]&gt; set name chenOK127.0.0.1:6379[3]&gt; append name 1 # 追加字符串，如果不存在则创建一个空串，再追加(integer) 5127.0.0.1:6379[3]&gt; get name\"chen1\"127.0.0.1:6379[3]&gt; append name ,hello(integer) 11127.0.0.1:6379[3]&gt; get name\"chen1,hello\"127.0.0.1:6379[3]&gt; strlen name # 获取字符串长度(integer) 11 1234567891011121314151617################################################################### incr# decr127.0.0.1:6379[3]&gt; set views 0OK127.0.0.1:6379[3]&gt; type viewsstring127.0.0.1:6379[3]&gt; incr views # 加1(integer) 1127.0.0.1:6379[3]&gt; incr views(integer) 2127.0.0.1:6379[3]&gt; get views\"2\"127.0.0.1:6379[3]&gt; decr views # 减1(integer) 1127.0.0.1:6379[3]&gt; get views\"1\" 1234567891011################################################################### incrby# decrby127.0.0.1:6379[3]&gt; get views\"1\"127.0.0.1:6379[3]&gt; incrby views 10 # 指定增量(integer) 11127.0.0.1:6379[3]&gt; get views\"11\"127.0.0.1:6379[3]&gt; decrby views 5 #(integer) 6 123456789101112131415################################################################### getrange# setrange127.0.0.1:6379[3]&gt; set key1 hello,chenOK127.0.0.1:6379[3]&gt; get key1\"hello,chen\"127.0.0.1:6379[3]&gt; getrange key1 0 3 # 截取字符串\"hell\"127.0.0.1:6379[3]&gt; getrange key1 0 -1 # 查看全部字符串\"hello,chen\"127.0.0.1:6379[3]&gt; setrange key1 1 xx # 替换指定位置开始的字符串(integer) 10127.0.0.1:6379[3]&gt; get key1\"hxxlo,chen\" 1234567891011121314################################################################### setex (set with expire) # 设置过期时间# setnx (set if not exists) # 不存在设置（在分布式锁中常常使用） 127.0.0.1:6379[3]&gt; setex key2 20 \"redis\" # 设置 key2 20s后过期OK127.0.0.1:6379[3]&gt; setnx key2 \"mongodb\" # key2存在则设置失败(integer) 0127.0.0.1:6379[3]&gt; ttl key2(integer) -2127.0.0.1:6379[3]&gt; setnx key2 \"mongodb\" # key2过期不存在，设置成功(integer) 1127.0.0.1:6379[3]&gt; get key2\"mongodb\" 1234567891011121314151617################################################################### mset # mget127.0.0.1:6379[3]&gt; mset k1 v1 k2 v2 k3 v3OK127.0.0.1:6379[3]&gt; keys *1) \"k1\"2) \"k2\"3) \"k3\"127.0.0.1:6379[3]&gt; mget k1 k2 k31) \"v1\"2) \"v2\"3) \"v3\"127.0.0.1:6379[3]&gt; msetnx k4 v4 k1 v1 # msetnx原子性操作，一起成功或一起失败(integer) 0127.0.0.1:6379[3]&gt; get k4(nil) 12345678910################################################################### 对象set user:1{name:zhangsan, age:3} # 设置一个user:1对象，值为json字符127.0.0.1:6379[3]&gt; msetnx user:1:name chen user:1:age 3(integer) 1127.0.0.1:6379[3]&gt; mget user:1:name user:1:age1) \"chen\"2) \"3\"127.0.0 12345678910################################################################### getset127.0.0.1:6379[3]&gt; getset db redis # 如果不存在值，则返回null(nil)127.0.0.1:6379[3]&gt; get db\"redis\"127.0.0.1:6379[3]&gt; getset db mongodb # 存在值，获取原来的值，并设置新的值\"redis\"127.0.0.1:6379[3]&gt; get db\"mongodb\" String类似的使用场景：value可以是字符串也可以是字符串 计数器 统计多单位数量 粉丝数 对象缓存 List在redis中，list可以实现栈，队列和阻塞队列 所有list命令都是以l开头 123456789101112131415161718192021222324################################################################### 添加# lpush# rpush127.0.0.1:6379[3]&gt; lpush array one # 将值插入到列表的头部（左）(integer) 1127.0.0.1:6379[3]&gt; lpush array two(integer) 2127.0.0.1:6379[3]&gt; lpush array three(integer) 3127.0.0.1:6379[3]&gt; lrange array 0 -1 # 查看list的值1) \"three\" # 倒序2) \"two\"3) \"one\"127.0.0.1:6379[3]&gt; lrange array 0 11) \"three\"2) \"two\"127.0.0.1:6379[3]&gt; rpush array right # 将值插入到列表的尾部（右）(integer) 4127.0.0.1:6379[3]&gt; lrange array 0 -11) \"three\"2) \"two\"3) \"one\"4) \"right\" 1234567891011###################################################### 移除# lpop# rpop127.0.0.1:6379[3]&gt; lpop array\"three\"127.0.0.1:6379[3]&gt; rpop array\"right\"127.0.0.1:6379[3]&gt; lrange array 0 -11) \"two\"2) \"one\" 12345678910################################################################# 获取值# lindex127.0.0.1:6379[3]&gt; lrange array 0 -11) \"two\"2) \"one\"127.0.0.1:6379[3]&gt; lindex array 1\"one\"127.0.0.1:6379[3]&gt; lindex array 0\"two\" 12345################################################################# 获取长度# llen127.0.0.1:6379[3]&gt; llen array(integer) 2 123456789101112131415161718192021222324252627################################################################# 移除固定值# lrem127.0.0.1:6379[3]&gt; lpush list 1(integer) 1127.0.0.1:6379[3]&gt; lpush list 2(integer) 2127.0.0.1:6379[3]&gt; lpush list 3(integer) 3127.0.0.1:6379[3]&gt; lpush list 3(integer) 4127.0.0.1:6379[3]&gt; lrange list 0 -11) \"3\"2) \"3\"3) \"2\"4) \"1\"127.0.0.1:6379[3]&gt; lrem list 1 1 # 移除list集合中指定个数的value(integer) 1127.0.0.1:6379[3]&gt; lrange list 0 -11) \"3\"2) \"3\"3) \"2\"127.0.0.1:6379[3]&gt; lrem list 2 3 # 移除list中两个3(integer) 2127.0.0.1:6379[3]&gt; lrange list 0 -11) \"2\"127.0.0.1:6379[3]&gt; 123456789101112131415################################################################# trim 修剪127.0.0.1:6379[3]&gt; lpush mylist \"1\"(integer) 1127.0.0.1:6379[3]&gt; lpush mylist \"2\"(integer) 2127.0.0.1:6379[3]&gt; lpush mylist \"3\"(integer) 3127.0.0.1:6379[3]&gt; lpush mylist \"4\"(integer) 4127.0.0.1:6379[3]&gt; ltrim mylist 1 2 # 通过下标截取指定长度OK127.0.0.1:6379[3]&gt; lrange mylist 0 -11) \"3\"2) \"2\" 123456789101112131415161718192021222324################################################################# rpoplpush 移除到列表中最后一个命令元素，将它移动到新的列表中127.0.0.1:6379[3]&gt; lpush list 1(integer) 1127.0.0.1:6379[3]&gt; lpush list 2(integer) 2127.0.0.1:6379[3]&gt; lpush list 3(integer) 3127.0.0.1:6379[3]&gt; lpush list 4(integer) 4127.0.0.1:6379[3]&gt; lrange list 0 -11) \"4\"2) \"3\"3) \"2\"4) \"1\"127.0.0.1:6379[3]&gt; rpoplpush list mylist\"1\"127.0.0.1:6379[3]&gt; lrange list 0 -11) \"4\"2) \"3\"3) \"2\"127.0.0.1:6379[3]&gt;127.0.0.1:6379[3]&gt; lrange mylist 0 -11) \"1\" 12345678910################################################################# lset 将列表中指定下标的值替换为另一个值，不存在会报错，更新操作127.0.0.1:6379[3]&gt; lset list 0 item(error) ERR no such key127.0.0.1:6379[3]&gt; lpush list 1(integer) 1127.0.0.1:6379[3]&gt; lset list 0 itemOK127.0.0.1:6379[3]&gt; lrange list 0 01) \"item\" 1234567891011121314151617181920################################################################# linsert 将某个value插入到某个元素的前面或后面127.0.0.1:6379[3]&gt; lpush list hello(integer) 1127.0.0.1:6379[3]&gt; lpush list word(integer) 2127.0.0.1:6379[3]&gt; linsert list before \"word\" \"other\"(integer) 3127.0.0.1:6379[3]&gt; lrange list 0 -11) \"other\"2) \"word\"3) \"hello\"127.0.0.1:6379[3]&gt; linsert list after \"word\" \"other\"(integer) 4127.0.0.1:6379[3]&gt; lrange list 0 -11) \"other\"2) \"word\"3) \"other\"4) \"hello\"127.0.0.1:6379[3]&gt; 小节 实际是一个链表 如果key不存在，创建新的链表 如果key存在，新增内容 如果移除的所有的值，空链表，也代表不存在 在两边插入或改动值，效率最高，中间元素，相对来说效率会低一些 Setset中的值是不重复的 123456789101112131415############################################################## sadd 添加# smembers 查看全部元素# sismember 判断某个值是否存在127.0.0.1:6379[3]&gt; sadd set name1(integer) 1127.0.0.1:6379[3]&gt; sadd set name2(integer) 1127.0.0.1:6379[3]&gt; smembers set1) \"name1\"2) \"name2\"127.0.0.1:6379[3]&gt; sismember set name1(integer) 1127.0.0.1:6379[3]&gt; sismember set name3(integer) 0 123456789################################################################# scard 长度# srem 移除127.0.0.1:6379[3]&gt; scard set(integer) 2127.0.0.1:6379[3]&gt; srem set name1(integer) 1127.0.0.1:6379[3]&gt; smembers set1) \"name2\" 12345678910111213#################################################################srandmember 随机抽取一个元素127.0.0.1:6379[3]&gt; srandmember set\"name6\"127.0.0.1:6379[3]&gt; srandmember set\"name3\"127.0.0.1:6379[3]&gt; srandmember set\"name5\"127.0.0.1:6379[3]&gt; srandmember set\"name5\"127.0.0.1:6379[3]&gt; srandmember set 2 # 随机抽取指定个数元素1) \"name5\"2) \"name6\" 12345################################################################# 随机删除key127.0.0.1:6379[3]&gt; spop set\"name3\" 123456################################################################### 讲一个指定的值，移动到另外一个set集合中127.0.0.1:6379[3]&gt; smove set set2 name2(integer) 1127.0.0.1:6379[3]&gt; smembers set21) \"name2\" 1234567891011121314151617181920212223242526###################################################################### 交集# 并集# 差集127.0.0.1:6379[3]&gt; sadd key1 a(integer) 1127.0.0.1:6379[3]&gt; sadd key1 b(integer) 1127.0.0.1:6379[3]&gt; sadd key1 c(integer) 1127.0.0.1:6379[3]&gt; sadd key2 b(integer) 1127.0.0.1:6379[3]&gt; sadd key2 c(integer) 1127.0.0.1:6379[3]&gt; sadd key2 d(integer) 1127.0.0.1:6379[3]&gt; sdiff key1 key2 # 差集1) \"a\"127.0.0.1:6379[3]&gt; sinter key1 key2 # 交集1) \"c\"2) \"b\"127.0.0.1:6379[3]&gt; sunion key1 key2 # 并集1) \"c\"2) \"b\"3) \"d\"4) \"a\" 微博，A用户将所有关注的人放在一个set集合中 共同关注，共同爱好，推荐好友 HashMap集合，key-map，本质和String没太大区别，还是一个简单的key-value 123456789101112131415161718192021####################################################################127.0.0.1:6379[3]&gt; hset hash1 name chen # set一个具体key-value(integer) 1127.0.0.1:6379[3]&gt; hget hash1 name # 获取1个字段值\"chen\"127.0.0.1:6379[3]&gt; hmset hash1 name xing name2 chen # set多个key-valueOK127.0.0.1:6379[3]&gt; hmget hash1 name name2 # # 获取多个字段值1) \"xing\"2) \"chen\"127.0.0.1:6379[3]&gt; hgetall hash1 # 获取全部数据1) \"name\"2) \"xing\"3) \"name2\"4) \"chen\"127.0.0.1:6379[3]&gt; hdel hash1 name # 删除hash指定key字段，对应的value也删除了(integer) 1127.0.0.1:6379[3]&gt; hgetall hash11) \"name2\"2) \"chen\"127.0.0.1: 1234#################################################################### hlen127.0.0.1:6379[3]&gt; hlen hash1(integer) 1 1234567##################################################################### hexists 判断hash指定字段是否存在127.0.0.1:6379[3]&gt; hexists hash1 name(integer) 0127.0.0.1:6379[3]&gt; hexists hash1 name2(integer) 1127.0.0.1:637 1234567################################################################# hkeys 获取所有的key# hvals 获取所有的value127.0.0.1:6379[3]&gt; hkeys hash11) \"name2\"127.0.0.1:6379[3]&gt; hvals hash11) \"chen\" 1234################################################################# hincr hdecr 加1或减1# hincrby hdecrby 指定增量# hsetnx hash变更的数据，user name age，尤其是用户信息之类，经常变动的信息 hash更适合于对象的存储，String更适合字符串的存储 Zset（有序集合）在set的基础上，增加一个值，set k1 v1, zset k1 score1 v1 12345678910111213141516127.0.0.1:6379[3]&gt; zadd set 1 one # 添加一个值(integer) 1127.0.0.1:6379[3]&gt; zadd set 2 two(integer) 1127.0.0.1:6379[3]&gt; zadd set 3 three(integer) 1127.0.0.1:6379[3]&gt; zadd set 0 zero(integer) 1127.0.0.1:6379[3]&gt; zadd set 5 five(integer) 1127.0.0.1:6379[3]&gt; zrange set 0 -11) \"zero\"2) \"one\"3) \"two\"4) \"three\"5) \"five\" 123456789101112131415161718192021222324##################################################################### zrangebyscore 排序127.0.0.1:6379[3]&gt; zrangebyscore set -inf +inf # 从小到大1) \"zero\"2) \"one\"3) \"two\"4) \"three\"5) \"five\"127.0.0.1:6379[3]&gt; zrevrange set 0 -1 # 从大到小1) \"five\"2) \"three\"3) \"two\"4) \"zero\"127.0.0.1:6379[3]&gt; zrangebyscore set -inf +inf withscores 1) \"zero\" 2) \"0\" 3) \"one\" 4) \"1\" 5) \"two\" 6) \"2\" 7) \"three\" 8) \"3\" 9) \"five\"10) \"5\" 123456789####################################################################### zrem 移除127.0.0.1:6379[3]&gt; zrem set one(integer) 1127.0.0.1:6379[3]&gt; zrange set 0 -11) \"zero\"2) \"two\"3) \"three\"4) \"five\" 1234########################################################################### zcard 长度127.0.0.1:6379[3]&gt; zcard set(integer) 4 123456######################################################################### zcount 获取每个区间成员数量127.0.0.1:6379[3]&gt; zcount set 1 3(integer) 2127.0.0.1:6379[3]&gt; zcount set 1 5(integer) 3 1 4、三种特殊数据类型geospatial地理位置 geoadd 123456789101112# geoadd 添加地理位置# 规则： 两级无法直接添加# 参数：key值 （纬度，经度，名称）127.0.0.1:6379&gt; geoadd china:city 116.40 39.90 beijing(integer) 1127.0.0.1:6379&gt; geoadd china:city 121.47 31.23 shanghai(integer) 1127.0.0.1:6379&gt; geoadd china:city 106.50 29.53 chongqing 114.05 22.52 shenzhen(integer) 2127.0.0.1:6379&gt; geoadd china:city 120.16 30.24 hangzhou 108.96 34.26 xian(integer) 2 geopos 12345678127.0.0.1:6379&gt; geopos china:city beijing # 获取指定城市的经度和纬度1) 1) \"116.39999896287918091\" 2) \"39.90000009167092543\"127.0.0.1:6379&gt; geopos china:city chongqing shenzhen1) 1) \"106.49999767541885376\" 2) \"29.52999957900659211\"2) 1) \"114.04999762773513794\" 2) \"22.5200000879503861\" geodist 两人之间的距离 1234127.0.0.1:6379&gt; geodist china:city shanghai beijing\"1067378.7564\"127.0.0.1:6379&gt; geodist china:city shanghai beijing km\"1067.3788\" georadius 123456789101112131415161718192021222324127.0.0.1:6379&gt; georadius china:city 110 30 1000 km # 以110 30这个经纬度为中心，寻找方圆1000km内的城市1) \"chongqing\"2) \"xian\"3) \"shenzhen\"4) \"hangzhou\"127.0.0.1:6379&gt; georadius china:city 110 30 500 km 1) \"chongqing\"2) \"xian\"127.0.0.1:6379&gt; georadius china:city 110 30 500 km withcoord # 显示城市的经纬度1) 1) \"chongqing\" 2) 1) \"106.49999767541885376\" 2) \"29.52999957900659211\"2) 1) \"xian\" 2) 1) \"108.96000176668167114\" 2) \"34.25999964418929977\"127.0.0.1:6379&gt; georadius china:city 110 30 500 km withdist # 显示城市到中心的距离（110 30）1) 1) \"chongqing\" 2) \"341.9374\"2) 1) \"xian\" 2) \"483.8340\"127.0.0.1:6379&gt; georadius china:city 110 30 500 km withdist count 1 # 显示指定数量1) 1) \"chongqing\" 2) \"341.9374\" georadiusbymember 12345# 找出位于指定元素周围的其他命令127.0.0.1:6379&gt; georadiusbymember china:city beijing 1000 km1) \"beijing\"2) \"xian\" geohash 1234# 将二维的经纬度转换为一维的字符串，如果两个字符串越接近，那么距离越近127.0.0.1:6379&gt; geohash china:city beijing chongqing1) \"wx4fbxxfke0\"2) \"wm5xzrybty0\" geo 底层的实现原理其实就是Zset，可以使用Zset命令操作geo 1234567891011121314151617127.0.0.1:6379&gt; zrange china:city 0 -1 # 查看全部元素1) \"chongqing\"2) \"xian\"3) \"shenzhen\"4) \"hangzhou\"5) \"shanghai\"6) \"beijing\"127.0.0.1:6379&gt; zrem china:city beijing # 移除指定元素(integer) 1127.0.0.1:6379&gt; zrange china:city 0 -11) \"chongqing\"2) \"xian\"3) \"shenzhen\"4) \"hangzhou\"5) \"shanghai\" Hyperloglog 什么是基数？ A {1,3,5,7,8,7} B{1,3,5,7,8} 基数（不重元素） = 5，可以接受误差 简介 Redis Hyperloglog 基数统计的算法 优点：占用的内存固定，2^64不同的元素，只需12KB内存 网站的UV（一个人访问一个网站，但还是算做一个人） 传统方式，set保存用户的id，然后可以统计set中的元素数据作为标准判断（占用量大内存） 0.81%错误率 12345678910111213127.0.0.1:6379&gt; pfadd key a b c d e f g h i j # 创建第一组元素(integer) 1127.0.0.1:6379&gt; pfcount key # 统计元素基数数量(integer) 10 127.0.0.1:6379&gt; pfadd key2 i j z x c v b n m #(integer) 1127.0.0.1:6379&gt; pfcount key2(integer) 9127.0.0.1:6379&gt; pfmerge key3 key key2 # 合并两组OK127.0.0.1:6379&gt; pfcount key3(integer) 15 Bitmap 位存储 统计用户信息，活跃，不活跃，登录，未登录，打卡 Bitmaps位图，数据结构，都是操作二进制位来进行记录 使用bitmaps来记录周一到周日的打卡 123456789101112131415127.0.0.1:6379&gt; setbit sign 0 1(integer) 0127.0.0.1:6379&gt; setbit sign 1 0(integer) 0127.0.0.1:6379&gt; setbit sign 2 0(integer) 0127.0.0.1:6379&gt; setbit sign 3 0(integer) 0127.0.0.1:6379&gt; setbit sign 4 1(integer) 0127.0.0.1:6379&gt; setbit sign 5 1(integer) 0127.0.0.1:6379&gt; setbit sign 6 1(integer) 0 查看某一天是否打卡 1234127.0.0.1:6379&gt; getbit sign 3(integer) 0127.0.0.1:6379&gt; getbit sign 6(integer) 1 统计打卡天数 12127.0.0.1:6379&gt; bitcount sign # 统计(integer) 4 5、事务Redis事务本质：一组命令的集合！一个事物中的所有命令都会被序列化，在事物执行过程中，会顺序执行！ 1---------队列 set set set 执行---------------- Redis事物没有隔离级别的概念 所有命令在事物中，并没有被执行，只有发起执行命令的时候才会执行 ==Redis单条命令是保证原子性的，但是事务不保证原子性== redis事物： 开启事务（multi） 命令入队（） 执行事务（exec） 正常执行事物 12345678910111213127.0.0.1:6379&gt; multi # 开启事务OK127.0.0.1:6379(TX)&gt; set k1 v1 # 命令入队QUEUED127.0.0.1:6379(TX)&gt; set k2 v2QUEUED127.0.0.1:6379(TX)&gt; get k2QUEUED127.0.0.1:6379(TX)&gt; exec # 执行事物1) OK2) OK3) \"v2\" 放弃事务 12345678910111213127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 v1QUEUED127.0.0.1:6379(TX)&gt; set k2 v2QUEUED127.0.0.1:6379(TX)&gt; set k4 v4QUEUED127.0.0.1:6379(TX)&gt; discard # 放弃事务，都不会被执行OK127.0.0.1:6379&gt; get k4(nil) 编译型异常（代码问题），事物汇总所有的命令都不会被执行 12345678910111213127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 v1 QUEUED127.0.0.1:6379(TX)&gt; set k2 v3QUEUED127.0.0.1:6379(TX)&gt; getset k2 # 错误的命令(error) ERR wrong number of arguments for 'getset' command127.0.0.1:6379(TX)&gt; set k4 v4QUEUED127.0.0.1:6379(TX)&gt; exec # 执行错误，所有命令都不会被执行(error) EXECABORT Transaction discarded because of previous errors. 运行时错误（1/0），如果事物队列中存在语法性错误，其他命令会正常执行，错误命令抛出异常 1234567891011121314151617181920127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 v1QUEUED127.0.0.1:6379(TX)&gt; incr k1QUEUED127.0.0.1:6379(TX)&gt; set k2 v2QUEUED127.0.0.1:6379(TX)&gt; set k3 v3QUEUED127.0.0.1:6379(TX)&gt; get k3QUEUED127.0.0.1:6379(TX)&gt; exec1) OK2) (error) ERR value is not an integer or out of range # 报错，其他名称成功3) OK4) OK5) \"v3\"127.0.0.1:6379&gt; get k2\"v2\" 监控 锁：Redis可以实现乐观锁，watch 悲观锁： 认为什时候都会出问题，无论做什么都会加锁 乐观锁： 认为什么时候都不会出问题，无论做什么都不会加锁 正常执行成功 12345678910111213141516127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; set out 0OK127.0.0.1:6379&gt; watch money # 监视money对象OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; decrby money 20QUEUED127.0.0.1:6379(TX)&gt; incrby out 20QUEUED127.0.0.1:6379(TX)&gt; exec1) (integer) 802) (integer) 20 测试多线程修改值，使用watch可以当做redis的乐观锁操作 1234567891011127.0.0.1:6379&gt; watch moneyOK127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; decrby money 10QUEUED127.0.0.1:6379(TX)&gt; incrby out 10QUEUED127.0.0.1:6379(TX)&gt; exec # 执行之前，另外一个线程修改了money值，就会导致事务执行失败(nil) 1234567891011121314127.0.0.1:6379&gt; unwatch # 如果执行失败，就先解锁，放弃监视OK127.0.0.1:6379&gt; watch money # 获取最新的值，再次监视OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; decrby money 10QUEUED127.0.0.1:6379(TX)&gt; incrby out 10QUEUED127.0.0.1:6379(TX)&gt; exec # 比对监视的值，是否发生了变化，变化了就会执行失败1) (integer) 9902) (integer) 40 6、jedis使用java来操作Redis 1、导入依赖 123456789101112&lt;!-- 导入jedis--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt;&lt;!-- fastjson--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt; &lt;/dependency&gt; 2、测试 连接数据库 操作命令 断开连接 12345public class Test { public static void main(String[] args) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); System.out.println(jedis.ping()); } 常用APIString List Set Hash Zset 测试 事务 正常执行 123456789101112131415161718192021public static void main(String[] args) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); Transaction multi = jedis.multi(); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"hello\", \"world\"); jsonObject.put(\"name\", \"chen\"); String result = jsonObject.toJSONString(); try { multi.set(\"user1\", result); multi.set(\"user2\", result); multi.exec(); } catch (Exception e){ e.printStackTrace(); multi.discard(); }finally { System.out.println(jedis.get(\"user1\")); System.out.println(jedis.get(\"user2\")); jedis.close(); }} 失败 1234567891011121314151617181920212223public static void main(String[] args) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); jedis.flushDB(); Transaction multi = jedis.multi(); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"hello\", \"world\"); jsonObject.put(\"name\", \"chen\"); String result = jsonObject.toJSONString(); try { multi.set(\"user1\", result); multi.set(\"user2\", result); int i = 1 / 0; multi.exec(); } catch (Exception e){ e.printStackTrace(); multi.discard(); }finally { System.out.println(jedis.get(\"user1\")); System.out.println(jedis.get(\"user2\")); jedis.close(); }} 远程连接Redis将保护模式关闭 注释 bind 127.0.0.1 7、springboot整合springboot操作数据：spring-data jpa jdbc mongodb redis SpringData也是和SpringData齐名的项目 说明：在springboot2.x之后，原来使用的jedis被替换为了lettuce jedis： 采用直连，多个线程操作的话，是不安全的，如果想要避免不安全，使用jedis pool连接池！更像BIO模式 lettuce：采用netty，实例可以在多个线程中进行共享，不存在线程不安全的情况，更像NIO模式 源码分析： RedisAutoConfiguration.java 123456789101112131415161718192021222324public class RedisAutoConfiguration { public RedisAutoConfiguration() { } @Bean @ConditionalOnMissingBean( name = {\"redisTemplate\"} ) @ConditionalOnSingleCandidate(RedisConnectionFactory.class) //可以自己定义一个RedisTemplate类替换这个 public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} 1、导入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 2、配置连接 123spring.redis.database=0spring.redis.port=6379spring.redis.host=127.0.0.1 3、测试 12345678910111213141516171819@AutowiredRedisTemplate redisTemplate;@Testvoid contextLoads() { // RedisTemplate 操作不同的数据类型，api和执行一样 // opsForValue 操作字符串，类似String // opsForList 操作list，类似List // opsForSet // opsForHash // opsForZset // opsForGeo // opsForHyperLogLog redisTemplate.opsForValue().set(\"key1\",\"逸辰\"); System.out.println(redisTemplate.opsForValue().get(\"key1\"));} ==pojo类需要序列化，不然redis保存对象会报错== 配置自定义RedisTemplate 123456789101112131415161718192021222324252627282930313233@Configurationpublic class RedisConfig { @Bean @SuppressWarnings(\"all\") public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory){ // 一般直接使用&lt;String, Object&gt; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(factory); //json序列化配置 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); //String的序列化 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); //key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); //hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化，方式采用jackson序列化方式 template.setValueSerializer(jackson2JsonRedisSerializer); // hash 的 key 采用jackson序列化方式 template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; }} Redis的工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582package com.example.demo.utils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Component;import org.springframework.util.CollectionUtils;import java.util.Collection;import java.util.List;import java.util.Map;import java.util.Set;import java.util.concurrent.TimeUnit;/** * @author 曹真豪 * @date 2021/5/31 9:39 */@Componentpublic final class RedisUtil { @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; // =============================common============================ /** * 指定缓存失效时间 * @param key 键 * @param time 时间(秒) */ public boolean expire(String key, long time) { try { if (time &gt; 0) { redisTemplate.expire(key, time, TimeUnit.SECONDS); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 根据key 获取过期时间 * @param key 键 不能为null * @return 时间(秒) 返回0代表为永久有效 */ public long getExpire(String key) { return redisTemplate.getExpire(key, TimeUnit.SECONDS); } /** * 判断key是否存在 * @param key 键 * @return true 存在 false不存在 */ public boolean hasKey(String key) { try { return redisTemplate.hasKey(key); } catch (Exception e) { e.printStackTrace(); return false; } } /** * 删除缓存 * @param key 可以传一个值 或多个 */ @SuppressWarnings(\"unchecked\") public void del(String... key) { if (key != null &amp;&amp; key.length &gt; 0) { if (key.length == 1) { redisTemplate.delete(key[0]); } else { redisTemplate.delete((Collection&lt;String&gt;) CollectionUtils.arrayToList(key)); } } } // ============================String============================= /** * 普通缓存获取 * @param key 键 * @return 值 */ public Object get(String key) { return key == null ? null : redisTemplate.opsForValue().get(key); } /** * 普通缓存放入 * @param key 键 * @param value 值 * @return true成功 false失败 */ public boolean set(String key, Object value) { try { redisTemplate.opsForValue().set(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 普通缓存放入并设置时间 * @param key 键 * @param value 值 * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * @return true成功 false 失败 */ public boolean set(String key, Object value, long time) { try { if (time &gt; 0) { redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); } else { set(key, value); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 递增 * @param key 键 * @param delta 要增加几(大于0) */ public long incr(String key, long delta) { if (delta &lt; 0) { throw new RuntimeException(\"递增因子必须大于0\"); } return redisTemplate.opsForValue().increment(key, delta); } /** * 递减 * @param key 键 * @param delta 要减少几(小于0) */ public long decr(String key, long delta) { if (delta &lt; 0) { throw new RuntimeException(\"递减因子必须大于0\"); } return redisTemplate.opsForValue().increment(key, -delta); } // ================================Map================================= /** * HashGet * @param key 键 不能为null * @param item 项 不能为null */ public Object hget(String key, String item) { return redisTemplate.opsForHash().get(key, item); } /** * 获取hashKey对应的所有键值 * @param key 键 * @return 对应的多个键值 */ public Map&lt;Object, Object&gt; hmget(String key) { return redisTemplate.opsForHash().entries(key); } /** * HashSet * @param key 键 * @param map 对应多个键值 */ public boolean hmset(String key, Map&lt;String, Object&gt; map) { try { redisTemplate.opsForHash().putAll(key, map); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * HashSet 并设置时间 * @param key 键 * @param map 对应多个键值 * @param time 时间(秒) * @return true成功 false失败 */ public boolean hmset(String key, Map&lt;String, Object&gt; map, long time) { try { redisTemplate.opsForHash().putAll(key, map); if (time &gt; 0) { expire(key, time); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 向一张hash表中放入数据,如果不存在将创建 * * @param key 键 * @param item 项 * @param value 值 * @return true 成功 false失败 */ public boolean hset(String key, String item, Object value) { try { redisTemplate.opsForHash().put(key, item, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 向一张hash表中放入数据,如果不存在将创建 * * @param key 键 * @param item 项 * @param value 值 * @param time 时间(秒) 注意:如果已存在的hash表有时间,这里将会替换原有的时间 * @return true 成功 false失败 */ public boolean hset(String key, String item, Object value, long time) { try { redisTemplate.opsForHash().put(key, item, value); if (time &gt; 0) { expire(key, time); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 删除hash表中的值 * * @param key 键 不能为null * @param item 项 可以使多个 不能为null */ public void hdel(String key, Object... item) { redisTemplate.opsForHash().delete(key, item); } /** * 判断hash表中是否有该项的值 * * @param key 键 不能为null * @param item 项 不能为null * @return true 存在 false不存在 */ public boolean hHasKey(String key, String item) { return redisTemplate.opsForHash().hasKey(key, item); } /** * hash递增 如果不存在,就会创建一个 并把新增后的值返回 * * @param key 键 * @param item 项 * @param by 要增加几(大于0) */ public double hincr(String key, String item, double by) { return redisTemplate.opsForHash().increment(key, item, by); } /** * hash递减 * * @param key 键 * @param item 项 * @param by 要减少记(小于0) */ public double hdecr(String key, String item, double by) { return redisTemplate.opsForHash().increment(key, item, -by); } // ============================set============================= /** * 根据key获取Set中的所有值 * @param key 键 */ public Set&lt;Object&gt; sGet(String key) { try { return redisTemplate.opsForSet().members(key); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 根据value从一个set中查询,是否存在 * * @param key 键 * @param value 值 * @return true 存在 false不存在 */ public boolean sHasKey(String key, Object value) { try { return redisTemplate.opsForSet().isMember(key, value); } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将数据放入set缓存 * * @param key 键 * @param values 值 可以是多个 * @return 成功个数 */ public long sSet(String key, Object... values) { try { return redisTemplate.opsForSet().add(key, values); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 将set数据放入缓存 * * @param key 键 * @param time 时间(秒) * @param values 值 可以是多个 * @return 成功个数 */ public long sSetAndTime(String key, long time, Object... values) { try { Long count = redisTemplate.opsForSet().add(key, values); if (time &gt; 0) expire(key, time); return count; } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 获取set缓存的长度 * * @param key 键 */ public long sGetSetSize(String key) { try { return redisTemplate.opsForSet().size(key); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 移除值为value的 * * @param key 键 * @param values 值 可以是多个 * @return 移除的个数 */ public long setRemove(String key, Object... values) { try { Long count = redisTemplate.opsForSet().remove(key, values); return count; } catch (Exception e) { e.printStackTrace(); return 0; } } // ===============================list================================= /** * 获取list缓存的内容 * * @param key 键 * @param start 开始 * @param end 结束 0 到 -1代表所有值 */ public List&lt;Object&gt; lGet(String key, long start, long end) { try { return redisTemplate.opsForList().range(key, start, end); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 获取list缓存的长度 * * @param key 键 */ public long lGetListSize(String key) { try { return redisTemplate.opsForList().size(key); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 通过索引 获取list中的值 * * @param key 键 * @param index 索引 index&gt;=0时， 0 表头，1 第二个元素，依次类推；index&lt;0时，-1，表尾，-2倒数第二个元素，依次类推 */ public Object lGetIndex(String key, long index) { try { return redisTemplate.opsForList().index(key, index); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 将list放入缓存 * * @param key 键 * @param value 值 */ public boolean lSet(String key, Object value) { try { redisTemplate.opsForList().rightPush(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * @param key 键 * @param value 值 * @param time 时间(秒) */ public boolean lSet(String key, Object value, long time) { try { redisTemplate.opsForList().rightPush(key, value); if (time &gt; 0) expire(key, time); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * * @param key 键 * @param value 值 * @return */ public boolean lSet(String key, List&lt;Object&gt; value) { try { redisTemplate.opsForList().rightPushAll(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, List&lt;Object&gt; value, long time) { try { redisTemplate.opsForList().rightPushAll(key, value); if (time &gt; 0) expire(key, time); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 根据索引修改list中的某条数据 * * @param key 键 * @param index 索引 * @param value 值 * @return */ public boolean lUpdateIndex(String key, long index, Object value) { try { redisTemplate.opsForList().set(key, index, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 移除N个值为value * * @param key 键 * @param count 移除多少个 * @param value 值 * @return 移除的个数 */ public long lRemove(String key, long count, Object value) { try { Long remove = redisTemplate.opsForList().remove(key, count, value); return remove; } catch (Exception e) { e.printStackTrace(); return 0; } }} 8、redis.conf 单位 对单位大小写不敏感 包含 网络 123bind 127.0.0.1 # 绑定的ipprojected-mode yes # 保护模式port 6379 # 端口设置 通用general 123456789101112daemonize yes # 以守护进程的方式运行，默认是no，需要设置为yes# 日志# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel noticelogfile \"\" # 日志文件的位置名databases 16 # 数据库的数量，默认是16个数据库 快照snapshotting 持久化， 在规定的时间内，执行了多少次操作，则会持久化到文件.rdb.aof 1234567891011121314# 如果900s内，如果至少有10个key进行了修改，则进行持久化操作save 900 1# 如果300s内，如果至少有10个key进行了修改，则进行持久化操作save 300 10# 如果60s内，如果至少有10000个key进行了修改，则进行持久化操作save 60 10000stop-writes-on-bgsave-error yes # 持久化出错，是否继续工作rdbcompression yes # 是否压缩rdb文件，需要消耗一些cpu资源rdbchecksum yes # 保存rdb文件时，进行错误的校验dir ./ # rdb文件保存的目录 安全security 可以设置密码 1234# 命令行设置config set requirepass \"123456\"config get requirepassauth 123456 # 登录 限制clients 123456789maxclients 10000 # 设置能连接上redis最大客户端的数量maxmemory &lt;bytes&gt; # redis配置最大的内存容量maxmemory-policy noeviction # 内存达到上限之后的处理策略，# 1、volatile-lru：只对设置了过期时间的key进行LRU（默认值）# 2、allkeys-lru：删除LRU算法的key# 3、volatile-random：随机删除即将过期的key# 4、allkeys-random：随机删除# 5、volatile-ttl：删除即将过期的# 6、noeviction：永不过期，返回错误 append only mode aof配置 12345678appendonly no # 默认不开启，默认使用rdb模式appendfilename \"appendonly.aof\" # 持久化文件的名字# appendfsync always # 每次修改都会同步（sync），消耗性能appendfsync everysec # 每一秒执行一次sync，可能会丢失这1s的数据# appendfsync no # 不执行sync，这个时候操作系统自己同步数据，速度最快 9、Redis持久化由于Redis的数据都存放在内存中，如果没有配置持久化，redis重启后数据就全丢失了，于是需要开启redis的持久化功能，将数据保存到磁盘上，当redis重启后，可以从磁盘中恢复数据。redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。 RDB（Redis DataBase）RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 ==rdb保存的文件dump.rdb== 触发规则 save的规则满足的情况下，会自动触发rdb规则 执行flushall命令，则会触发rdb规则 退出redis，会触发rdb规则 备份会自动生成dump.rdb 恢复rdb文件 将dump.rdb文件放在redis启动目录就可以 优点： 适合大规模的数据恢复 如果对数据完成性不高 缺点： 需要一定的时间间隔，如果redis意外宕机了，最后一次的数据就没了 fork进程的时候，会占用一定的内容空间 AOFAOF（append only file）持久化：以独立日志的方式记录每次写命令， 重启时再重新执行AOF文件中的命令达到恢复数据的目的。AOF的主要作用 是解决了数据持久化的实时性，目前已经是Redis持久化的主流方式 ==Aof保存的是appendonly.aof文件== 默认是不开启的 aof文件错误，使用 ==redis-check-aof –fix==工具修复 优点： 每次修改都同步，文件完整性更加好 每秒同步一次，可能丢失一秒的数据 缺点： 相对于数据文件来说，aof远远大于rdb，修复速度也比rdb慢 aof的运行效率也要比rdb慢，所以redis默认使用rdb持久化 10、Redis发布订阅 命令 订阅端： 123456789101112127.0.0.1:6379&gt; subscribe chen # 订阅一个频道Reading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"chen\"3) (integer) 1# 等待读取推送的信息1) \"message\"2) \"chen\"3) \"hello,chen\"1) \"message\"2) \"chen\"3) \"hello,redis\" 发送端： 123456127.0.0.1:6379&gt; publish chen hello,chen # 发布者发布消息到频道(integer) 1127.0.0.1:6379&gt; publish chen hello,redis(integer) 1127.0.0.1:6379&gt; 原理 使用场景： 实时消息系统 实施聊天室 订阅 复杂场景可以使用消息中间件rabbitMQ","categories":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"}]},{"title":"spring笔记","slug":"spring/spring笔记","date":"2021-10-10T16:00:00.000Z","updated":"2023-07-17T13:41:27.804Z","comments":true,"path":"2021/10/11/spring/spring笔记/","link":"","permalink":"https://yichenfirst.github.io/2021/10/11/spring/spring%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Spring5框架概述1、Spring是轻量级的开源的JavaEE框架 2、Spring可以解决企业应用开发的复杂性 3、Spring有两个核心部分： IOC：控制反转，把创建对象过程交给spring进行管理 AOP：面向切面，不修改源代码进行功能增强 4、Spring特点 方便解耦，简化开发 AOP编程支持 方便程序测试 方便和其他框架进行整合 方便进行事务操作 降低API开发难度 IOC容器实例目录结构 1234567java com yichen spring BeanTestresources BeanTest.xml BeanTest.java 1234567891011121314151617181920212223package com.yichen.spring;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class BeanTest { private String user = \"admin\"; public void fun(){ System.out.println(\"user: \" + user); } @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); BeanTest beanTest = context.getBean(\"test\", BeanTest.class); System.out.println(\"beanTest.user = \" + beanTest.user); beanTest.fun(); }} BeanTest.xml 12345678&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置 User 对象创建--&gt; &lt;bean id=\"test\" class=\"com.yichen.spring.BeanTest\"&gt;&lt;/bean&gt;&lt;/beans&gt; IOC概念与原理什么是IOC 控制反转，把对象创建和对象之间的调用过程，交给 Spring 进行管理 使用 IOC 目的：为了耦合度降低 IOC底层原理用到技术：xml解析、工厂模式、反射 原始方式中，UserService与UserDao的耦合度太高了 工厂模式中，UserService与UserDao的耦合度降低，但UserService和UserDao与UserFactory间的还存在耦合 可以使用IOC技术进一步降低耦合度 当UserDao发生发生改变时（比如路径变动），不需要修改UserFactory，只修改xml配置文件就好，。 BeanFactory接口IOC 思想基于 IOC 容器完成，IOC 容器底层就是对象工厂 Spring提供IOC容器实现两种方式 BeanFactory：IOC 容器基本实现，是 Spring 内部的使用接口，不提供开发人员进行使用，加载配置文件时候不会创建对象，在获取对象（使用）才去创建对象 ApplicationContext：BeanFactory 接口的子接口，提供更多更强大的功能，一般由开发人 员进行使用，加载配置文件时候就会把在配置文件对象进行创建 ApplicationContext接口实现类： FileSystemXMLApplicationContext FileSystemXmlApplicationContext 默认是去项目的路径下加载，可以是相对路径，也可以是绝对路径，若是绝对路径，“file:” 前缀可以缺省。 123456789101112131415161718@Testpublic void testBean(){ //classes目录 BeanFactory beanFactory=new FileSystemXmlApplicationContext(\"classpath:applicationContext.xml\"); //项目路径相对路径 BeanFactory beanFactory=new FileSystemXmlApplicationContext(\"src\\\\main\\\\resources\\\\applicationContext.xml\"); //多配置文件 BeanFactory beanFactory=new FileSystemXmlApplicationContext(new String[]{\"src\\\\main\\\\resources\\\\applicationContext.xml\"}); //绝对目录 BeanFactory beanFactory=new FileSystemXmlApplicationContext(new String[]{\"E:\\\\Workspace\\\\idea_workspace\\\\spring\\\\springtest\\\\src\\\\main\\\\resources\\\\applicationContext.xml\"}); TestBean bean= (TestBean) beanFactory.getBean(\"testBean\"); assertEquals(\"testStr\",bean.getTestStr());} ClassPathXMLApplicationContext ClassPathXmlApplicationContext 默认会去 classPath 路径下找。classPath 路径指的就是编译后的 classes 目录。 123456789101112131415161718@Testpublic void testBean(){ //单配置文件方式一 BeanFactory beanFactory=new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //单配置文件方式二 BeanFactory beanFactory=new ClassPathXmlApplicationContext(\"classpath:applicationContext.xml\"); //多个配置文件 BeanFactory beanFactory=new ClassPathXmlApplicationContext(new String[]{\"applicationContext.xml\"}); //绝对路径需加“file:”前缀 BeanFactory beanFactory = new ClassPathXmlApplicationContext(\"file:E:\\Workspace\\idea_workspace\\spring\\springtest\\src\\main\\resources\\applicationContext.xml\"); TestBean bean= (TestBean) beanFactory.getBean(\"testBean\"); assertEquals(\"testStr\",bean.getTestStr());} IOC操作Bean管理什么是Bean管理 spring创建对象 spring注入属性 Bean管理操作的两种方式1&lt;bean id=\"test\" class=\"com.yichen.spring.BeanTest\"&gt;&lt;/bean&gt; 在spring配置文件中，使用bean标签，标签里面添加对应属性，就可以实现对象创建 在bean标签属性 id属性： 唯一标识 class属性： 类全路径（包类路径） 创建对象时候，默认也是执行无参数构造方法完成对象创建 基于xml方式注入属性DI：依赖注入，就是注入属性 使用 set 方法进行注入123456789101112131415161718192021222324252627282930package com.yichen.spring;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Book { private String bname; private String bauthor; public void setBname(String bname) { this.bname = bname; } public void setBauthor(String bauthor) { this.bauthor = bauthor; } public void testDemo(){ System.out.println(\"bname = \" + bname); System.out.println(\"bauthor = \" + bauthor); } @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); Book book = context.getBean(\"book\", Book.class); System.out.println(\"book = \" + book); book.testDemo(); }} 在 spring 配置文件配置对象创建，配置属性注入 12345678&lt;bean id=\"book\" class=\"com.yichen.spring.Book\"&gt; &lt;!--使用 property 完成属性注入 name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=\"bname\" value=\"易筋经\"&gt;&lt;/property&gt; &lt;property name=\"bauthor\" value=\"达摩老祖\"&gt;&lt;/property&gt;&lt;/bean&gt; 运行结果 1234book = com.yichen.spring.Book@544a2ea6bname = 易筋经bauthor = 达摩老祖 通过有参构造进行注入12345678910111213141516171819202122package com.yichen.spring;/** * 使用有参构造进行注入 */public class Order { String oname; String address; public Order(String oname, String address) { this.oname = oname; this.address = address; } public void testDemo(){ System.out.println(\"oname = \" + oname); System.out.println(\"address = \" + address); }} 1234&lt;bean id=\"orders\" class=\"com.yichen.spring.Order\"&gt; &lt;constructor-arg name=\"oname\" value=\"电脑\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"address\" value=\"China\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; OrderTest.java 12345678910111213141516package com.yichen.spring;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class OrderTest { @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); Order order = context.getBean(\"orders\", Order.class); System.out.println(\"order = \" + order); order.testDemo(); }} xml 注入其他类型属性 null 1234&lt;!--null 值--&gt;&lt;property name=\"address\"&gt; &lt;null/&gt;&lt;/property&gt; 属性值包含特殊符号 12345678&lt;!--属性值包含特殊符号 1 把&lt;&gt;进行转义 &amp;lt; &amp;gt; 2 把带特殊符号内容写到 CDATA--&gt;&lt;property name=\"address\"&gt; &lt;value&gt;&lt;![CDATA[&lt;&lt;南京&gt;&gt;]]&gt;&lt;/value&gt;&lt;/property&gt; 注入属性-外部 bean 创建两个类 service 类和 dao 类 在 service 调用 dao 里面的方法 在 spring 配置文件中进行配置 UserDao.java 123public class UserDao{ void update();} UserDaoImpl.java 1234567public class UserDao implements UserDao{ @Override public void update(){ System.out.println(\"dao update..............\") }} UserService.java 123456789101112public class UserService { //创建 UserDao 类型属性，生成 set 方法 private UserDao userDao; public void setUserDao(UserDao userDao) { this.userDao = userDao; } public void add() { System.out.println(\"service add...............\"); userDao.update(); }} 123456789&lt;!--1 service 和 dao 对象创建--&gt;&lt;bean id=\"userService\" class=\"com.yichen.spring.service.UserService\"&gt; &lt;!--注入 userDao 对象 name 属性：类里面属性名称 ref 属性：创建 userDao 对象 bean 标签 id 值 --&gt; &lt;property name=\"userDao\" ref=\"userDaoImpl\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"userDaoImpl\" class=\"com.yichen.spring.dao.UserDaoImpl\"&gt;&lt;/bean&gt; 123456789101112131415package com.yichen.spring;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class OrderTest { @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); UserService userService = context.getBean(\"userService\", UserService.class); userService.add(); }} 运行结果 12service add...............dao update.............. 注入属性-内部 bean（1）一对多关系：部门和员工 一个部门有多个员工，一个员工属于一个部门。部门是一，员工是多 （2）在实体类之间表示一对多关系，员工表示所属部门，使用对象类型属性进行表示 1234567891011121314151617181920212223/部门类public class Dept { private String dname; public void setDname(String dname) { this.dname = dname; }}//员工类public class Emp { private String ename; private String gender; //员工属于某一个部门，使用对象形式表示 private Dept dept; public void setDept(Dept dept) { this.dept = dept; } public void setEname(String ename) { this.ename = ename; } public void setGender(String gender) { this.gender = gender; }} 12345678910111213&lt;!--内部 bean--&gt;&lt;bean id=\"emp\" class=\"com.atguigu.spring5.bean.Emp\"&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=\"ename\" value=\"lucy\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"女\"&gt;&lt;/property&gt; &lt;!--设置对象类型属性--&gt; &lt;property name=\"dept\"&gt; &lt;bean id=\"dept\" class=\"com.atguigu.spring5.bean.Dept\"&gt; &lt;property name=\"dname\" value=\"安保部\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 注入属性-级联赋值第一种写法 12345678910&lt;bean id=\"emp\" class=\"com.atguigu.spring5.bean.Emp\"&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=\"ename\" value=\"lucy\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"女\"&gt;&lt;/property&gt; &lt;!--级联赋值--&gt; &lt;property name=\"dept\" ref=\"dept\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"dept\" class=\"com.atguigu.spring5.bean.Dept\"&gt; &lt;property name=\"dname\" value=\"财务部\"&gt;&lt;/property&gt;&lt;/bean&gt; 第二种写法 12345678910111213&lt;!--级联赋值--&gt;&lt;bean id=\"emp\" class=\"com.atguigu.spring5.bean.Emp\"&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=\"ename\" value=\"lucy\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"女\"&gt;&lt;/property&gt; &lt;!--级联赋值--&gt; &lt;property name=\"dept\" ref=\"dept\"&gt;&lt;/property&gt; &lt;property name=\"dept.dname\" value=\"技术部\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"dept\" class=\"com.atguigu.spring5.bean.Dept\"&gt; &lt;property name=\"dname\" value=\"财务部\"&gt;&lt;/property&gt;&lt;/bean&gt; &lt;property name=\"dept.dname\" value=\"技术部\"&gt;&lt;/property&gt; 需要在Emp类中生成Dept的get方法 xml注入集合属性 注入数组类型属性 注入 List 集合类型属性 注入 Map 集合类型属性 注入 Set集合类型属性 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.yichen.spring.test1;import java.util.List;import java.util.Map;import java.util.Set;public class Stu { /** * 1 数组类型属性 */ String[] courses; /** * 2 list 集合类型属性 */ List&lt;String&gt; list; /** * 3 map 集合类型属性 */ Map&lt;String, String&gt; maps; /** * 4 map 集合类型属性 */ Set&lt;String&gt; sets; public void setCourses(String[] courses) { this.courses = courses; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMaps(Map&lt;String, String&gt; maps) { this.maps = maps; } public void setSets(Set&lt;String&gt; sets) { this.sets = sets; }} 12345678910111213141516171819202122232425262728293031&lt;bean id=\"stu\" class=\"com.yichen.spring.test1.Stu\"&gt; &lt;!--数组类型属性注入--&gt; &lt;property name=\"courses\"&gt; &lt;array&gt; &lt;value&gt;java 课程&lt;/value&gt; &lt;value&gt;数据库课程&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list 类型属性注入--&gt; &lt;property name=\"list\"&gt; &lt;list&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;小三&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map 类型属性注入--&gt; &lt;property name=\"maps\"&gt; &lt;map&gt; &lt;entry key=\"JAVA\" value=\"java\"&gt;&lt;/entry&gt; &lt;entry key=\"PHP\" value=\"php\"&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set 类型属性注入--&gt; &lt;property name=\"sets\"&gt; &lt;set&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; 在集合里面设置对象类型值 1234567891011121314&lt;!--创建多个 course 对象--&gt;&lt;bean id=\"course1\" class=\"com.atguigu.spring5.collectiontype.Course\"&gt; &lt;property name=\"cname\" value=\"Spring5 框架\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"course2\" class=\"com.atguigu.spring5.collectiontype.Course\"&gt; &lt;property name=\"cname\" value=\"MyBatis 框架\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--注入 list 集合类型，值是对象--&gt;&lt;property name=\"courseList\"&gt; &lt;list&gt; &lt;ref bean=\"course1\"&gt;&lt;/ref&gt; &lt;ref bean=\"course2\"&gt;&lt;/ref&gt; &lt;/list&gt;&lt;/property&gt; 把集合注入部分提取出来 （1）在 spring 配置文件中引入名称空间 util 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:util=\"http://www.springframework.org/schema/util\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd\"&gt; （2）使用 util 标签完成 list 集合注入提取 1234567891011&lt;!--1 提取 list 集合类型属性注入--&gt;&lt;util:list id=\"bookList\"&gt; &lt;value&gt;易筋经&lt;/value&gt; &lt;value&gt;九阴真经&lt;/value&gt; &lt;value&gt;九阳神功&lt;/value&gt;&lt;/util:list&gt;&lt;!--2 提取 list 集合类型属性注入使用--&gt;&lt;bean id=\"book\" class=\"com.atguigu.spring5.collectiontype.Book\"&gt; &lt;property name=\"list\" ref=\"bookList\"&gt;&lt;/property&gt;&lt;/bean&gt; FactoryBean1、Spring 有两种类型 bean，一种普通 bean，另外一种工厂 bean（FactoryBean） 2、普通 bean：在配置文件中定义 bean 类型就是返回类型 3、工厂 bean：在配置文件定义 bean 类型可以和返回类型不一样 第一步 创建类，让这个类作为工厂 bean，实现接口 FactoryBean 第二步 实现接口里面的方法，在实现的方法中定义返回的 bean 类型 1234567891011121314151617public class MyBean implements FactoryBean&lt;Course&gt; { //定义返回 bean @Override public Course getObject() throws Exception { Course course = new Course(); course.setCname(\"abc\"); return course; } @Override public Class&lt;?&gt; getObjectType() { return null; } @Override public boolean isSingleton() { return false; }} 1&lt;bean id=\"myBean\" class=\"com.atguigu.spring5.factorybean.MyBean\"&gt;&lt;/bean&gt; 123456@Testpublic void test3() { ApplicationContext context =new ClassPathXmlApplicationContext(\"bean3.xml\"); Course course = context.getBean(\"myBean\", Course.class); System.out.println(course);} bean作用域在 Spring 里面，设置创建 bean 实例是单实例还是多实例 在 Spring 里面，默认情况下，bean 是单实例对象 12345678@Testpublic void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); Book book1 = context.getBean(\"book\", Book.class); Book book2 = context.getBean(\"book\", Book.class); System.out.println(\"book1 = \" + book1); System.out.println(\"book2 = \" + book2);} 运行结果 12book1 = com.yichen.spring.Book@17211155book2 = com.yichen.spring.Book@17211155 地址相同，说明是单实例对象 如何设置单实例还是多实例 （1）在 spring 配置文件 bean 标签里面有属性（scope）用于设置单实例还是多实例 （2）scope 属性值 第一个值 默认值，singleton，表示是单实例对象 第二个值 prototype，表示是多实例对象 1234&lt;bean id=\"book\" class=\"com.yichen.spring.Book\" scope=\"prototype\"&gt; &lt;property name=\"bname\" value=\"易筋经\"&gt;&lt;/property&gt; &lt;property name=\"bauthor\" value=\"达摩老祖\"&gt;&lt;/property&gt;&lt;/bean&gt; 运行结果 12book1 = com.yichen.spring.Book@b3d7190book2 = com.yichen.spring.Book@5fdba6f9 singleton 和 prototype 区别 第一 singleton 单实例，prototype 多实例 第二 创建对象的时间不一样 设置 scope 值是 singleton 时候，加载 spring 配置文件时候就会创建单实例对象 设置 scope 值是 prototype 时候，不是在加载 spring 配置文件时候创建对象，在调用 getBean 方法时候创建多实例对象 bean生命周期 通过构造器创建 bean 实例（无参数构造） 为 bean 的属性设置值和对其他 bean 引用（调用 set 方法） 调用 bean 的初始化的方法（需要进行配置初始化的方法） bean 可以使用了（对象获取到了） 当容器关闭时候，调用 bean 的销毁的方法（需要进行配置销毁的方法） 123456789101112131415161718192021222324252627282930313233343536package com.yichen.spring.test2;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Orders { //无参数构造 public Orders() { System.out.println(\"第一步 执行无参数构造创建 bean 实例\"); } private String oname; public void setOname(String oname) { this.oname = oname; System.out.println(\"第二步 调用 set 方法设置属性值\"); } //创建执行的初始化的方法 public void initMethod() { System.out.println(\"第三步 执行初始化的方法\"); } //创建执行的销毁的方法 public void destroyMethod() { System.out.println(\"第五步 执行销毁的方法\"); } @Test public void testBean3() { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"BeanTest.xml\"); Orders orders = context.getBean(\"test2-orders\", Orders.class); System.out.println(\"第四步 获取创建 bean 实例对象\"); System.out.println(orders); //手动让 bean 实例销毁 context.close(); }} 123&lt;bean id=\"test2-orders\" class=\"com.yichen.spring.test2.Orders\" init-method=\"initMethod\" destroy-method=\"destroyMethod\"&gt; &lt;property name=\"oname\" value=\"手机\"&gt;&lt;/property&gt;&lt;/bean&gt; 运行结果 1234567第一步 执行无参数构造创建 bean 实例第一步 执行无参数构造创建 bean 实例第二步 调用 set 方法设置属性值第三步 执行初始化的方法第四步 获取创建 bean 实例对象com.yichen.spring.test2.Orders@10d59286第五步 执行销毁的方法 bean 的后置处理器，bean 生命周期有七步 通过构造器创建 bean 实例（无参数构造） 为 bean 的属性设置值和对其他 bean 引用（调用 set 方法） 把 bean 实例传递 bean 后置处理器的方法 postProcessBeforeInitialization 调用 bean 的初始化的方法（需要进行配置初始化的方法） 把 bean 实例传递 bean 后置处理器的方法 postProcessAfterInitialization bean 可以使用了（对象获取到了） 当容器关闭时候，调用 bean 的销毁的方法（需要进行配置销毁的方法） 12345678910111213141516171819package com.yichen.spring.test2;import org.springframework.beans.BeansException;import org.springframework.beans.factory.config.BeanPostProcessor;public class MyBeanPost implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"在初始化之前执行的方法\"); return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"在初始化之后执行的方法\"); return bean; }} 12345&lt;bean id=\"test2-orders\" class=\"com.yichen.spring.test2.Orders\" init-method=\"initMethod\" destroy-method=\"destroyMethod\"&gt; &lt;property name=\"oname\" value=\"手机\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--配置为当前xml中所有bean配置后置处理器--&gt;&lt;bean id=\"myBeanPost\" class=\"com.yichen.spring.test2.MyBeanPost\"&gt;&lt;/bean&gt; 12345678第一步 执行无参数构造创建 bean 实例第二步 调用 set 方法设置属性值在初始化之前执行的方法第三步 执行初始化的方法在初始化之后执行的方法第四步 获取创建 bean 实例对象com.yichen.spring.test2.Orders@10dba097第五步 执行销毁的方法 xml自动装配1、什么是自动装配 根据指定装配规则（属性名称或者属性类型），Spring 自动将匹配的属性值进行注入 2、演示自动装配过程 根据属性名称自动注入 byName: 会自动在容器上下文中查找和自己对象set方法后面的值对应的bean id，如steDept 的 Dept 1234567891011&lt;!--实现自动装配 bean 标签属性 autowire，配置自动装配 autowire 属性常用两个值： byName 根据属性名称注入 ，注入值 bean 的 id 值和类属性名称一样 byType 根据属性类型注入--&gt;&lt;bean id=\"emp\" class=\"com.yichen.spring.autowire.Emp\" autowire=\"byName\"&gt; &lt;!--&lt;property name=\"dept\" ref=\"dept\"&gt;&lt;/property&gt;--&gt;&lt;/bean&gt;&lt;bean id=\"dept\" class=\"com.atguigu.spring5.autowire.Dept\"&gt;&lt;/bean&gt; 根据属性类型自动注入 byName: 会自动在容器上下文中查找和自己对象属性的类型相同的bean（全局唯一） 1234567891011&lt;!--实现自动装配 bean 标签属性 autowire，配置自动装配 autowire 属性常用两个值： byName 根据属性名称注入 ，注入值 bean 的 id 值和类属性名称一样 byType 根据属性类型注入--&gt;&lt;bean id=\"emp\" class=\"com.yichen.spring.autowire.Emp\" autowire=\"byType\"&gt; &lt;!--&lt;property name=\"dept\" ref=\"dept\"&gt;&lt;/property&gt;--&gt;&lt;/bean&gt;&lt;bean id=\"dept\" class=\"com.yichen.spring.autowire.Dept\"&gt;&lt;/bean&gt; 小结： byName：需要保证所有bean的id唯一，并且这个bean需要和自动注入的属性的set方法的值一致 byType： 需要保证所有bean的class唯一，并且这个bean需要和自动注入的属性的类型一致 外部属性文件（1）创建外部属性文件，properties 格式文件 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root （2）把外部 properties 属性文件引入到 spring 配置文件中 引入 context 名称空间 jdbc.properties 1234567891011121314151617181920212223&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!--在 spring 配置文件使用标签引入外部属性文件--&gt; &lt;!--引入外部属性文件--&gt; &lt;context:property-placeholder location=\"classpath:jdbc.properties\"/&gt; &lt;!--配置连接池--&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"${prop.driverClass}\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"${prop.url}\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"${prop.userName}\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"${prop.password}\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 基于注解方式注入属性 什么是注解 注解是代码特殊标记，格式：@注解名称(属性名称=属性值, 属性名称=属性值..) 使用注解，注解作用在类上面，方法上面，属性上面 使用注解目的：简化 xml 配置 Spring 针对 Bean 管理中创建对象提供注解 @Component @Service @Controller @Repository 上面四个注解功能是一样的，都可以用来创建 bean 实例 基于注解方式实现对象创建第一步 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.2.6.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步 开启组件扫描 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!--开启组件扫描 1 如果扫描多个包，多个包使用逗号隔开 2 扫描包上层目录--&gt; &lt;context:component-scan base-package=\"com.atguigu\"&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 第三步 创建类，在类上面添加创建对象注解 12345678910111213141516171819202122232425package com.yichen.spring.test3;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.stereotype.Component;//在注解里面 value 属性值可以省略不写，//默认值是类名称，首字母小写//UserService -- userService@Component(value = \"userService\")public class UserService { public void add() { System.out.println(\"service add.......\"); } @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"Bean1.xml\"); UserService userService = context.getBean(\"userService\", UserService.class); System.out.println(\"beanTest.user = \" + userService); userService.add(); }} 开启组件扫描细节配置 1234567891011121314151617&lt;!--示例 1 use-default-filters=\"false\" 表示现在不使用默认 filter，自己配置 filter context:include-filter ，设置扫描哪些内容 只扫描controller注解--&gt;&lt;context:component-scan base-package=\"com.atguigu\" use-defaultfilters=\"false\"&gt; &lt;context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt;&lt;/context:component-scan&gt;&lt;!--示例 2 下面配置扫描包所有内容 context:exclude-filter： 设置哪些内容不进行扫描--&gt;&lt;context:component-scan base-package=\"com.atguigu\"&gt; &lt;context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt;&lt;/context:component-scan&gt; 基于注解方式实现属性注入 @Autowired：根据属性类型进行自动装配 第一步 把 service 和 dao 对象创建，在 service 和 dao 类添加创建对象注解 第二步 在 service 注入 dao 对象，在 service 类添加 dao 类型属性，在属性上面使用注解 只能注入接口只有一个实现类的情况 ，当有多个实现类时使用@Autowired注解会报错 123456789101112131415161718192021222324252627282930313233package com.yichen.spring.test3;import org.junit.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.stereotype.Component;import org.springframework.stereotype.Service;@Service(value = \"userService\")public class UserService { /** * 定义 dao 类型属性 * 不需要添加 set 方法 * 添加注入属性注解 */ @Autowired private UserDao userDao; public void add() { System.out.println(\"service add.......\"); userDao.add(); } @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"Bean1.xml\"); UserService userService = context.getBean(\"userService\", UserService.class); System.out.println(\"beanTest.user = \" + userService); userService.add(); }} 123456package com.yichen.spring.test3;public interface UserDao { void add();} 123456789101112package com.yichen.spring.test3;import org.springframework.stereotype.Repository;@Repositorypublic class UserDaoImpl implements UserDao{ @Override public void add() { System.out.println(\"dao add...............\"); }} @Qualifier：根据名称进行注入 这个@Qualifier 注解的使用，和上面@Autowired 一起使用，解决多个接口实现类注入问题 1234567891011package com.yichen.spring.test3;import org.springframework.stereotype.Repository;@Repositorypublic class UserServiceImpl2 implements UserDao{ @Override public void add() { System.out.println(\"UserDaoImpl1 add...............\"); }} 12345678910111213141516171819202122232425262728293031package com.yichen.spring.test3;import org.junit.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.stereotype.Component;import org.springframework.stereotype.Service;@Service(value = \"userService\")public class UserService { @Qualifier(value = \"userDaoImpl\") @Autowired private UserDao userDao; public void add() { System.out.println(\"service add.......\"); userDao.add(); } @Test public void test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"Bean1.xml\"); UserService userService = context.getBean(\"userService\", UserService.class); System.out.println(\"beanTest.user = \" + userService); userService.add(); }} @Resource：可以根据类型注入，可以根据名称注入 不推荐使用，@Resource在javax包中 12345import javax.annotation.Resource //@Resource //根据类型进行注入 @Resource(name = \"userDaoImpl1\") //根据名称进行注入 private UserDao userDao; @Value：注入普通类型属性 12@Value(value = \"abc\")private String name; 完全注解开发（1）创建配置类，替代 xml 配置文件 1234@Configuration //作为配置类，替代 xml 配置文件@ComponentScan(basePackages = {\"com.atguigu\"}) public class SpringConfig {} （2）编写测试类 12345678910@Testpublic void testService2() { //加载配置类 ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); UserService userService = context.getBean(\"userService\", UserService.class); System.out.println(userService); userService.add();} AOP见springboot/aop.md 底层原理AOP 底层使用动态代理 有接口情况，使用JDK代理 没有接口情况，使用CGLIB动态代理","categories":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"}]},{"title":"哲学家进餐问题","slug":"操作系统/哲学家进餐问题","date":"2021-09-26T16:00:00.000Z","updated":"2023-07-28T14:16:43.587Z","comments":true,"path":"2021/09/27/操作系统/哲学家进餐问题/","link":"","permalink":"https://yichenfirst.github.io/2021/09/27/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90%E9%97%AE%E9%A2%98/","excerpt":"","text":"哲学家进餐问题问题描述有四位哲学家围在一张圆桌旁，他们一生都在吃东西和思考。 有四只筷子供他们使用，哲学家需要拿到一双筷子之后才能吃饭；吃完饭后会放下筷子继续思考。 这时就会有一个问题，如果哲学家同时拿起左手边的筷子，所有哲学家都会因为拿不到右手边的筷子不能吃饭而饿死。 所以需要一种方案保证哲学家可以交替吃饭和思考，而不会被饿死。 代码模拟在哲学家问题中，哲学家存在两种行为，分别是思考和吃东西。可以将哲学家看做一个线程，在线程中的run方法内分别实现思考与吃东西的方法。而吃东西需要同时依赖左手边的筷子和右手边的筷子，同时一根筷子只能被一个哲学家拥有，可以使用锁来保证这一点。所以首先定义哲学家类，Philosophers。 Philosophers.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Random;import java.util.concurrent.TimeUnit;public class Philosophers extends Thread{ private String pName; // 哲学家名称 private int index; // 哲学家的编号 private Chopsticks left; // 哲学家左手边的筷子 private Chopsticks right; // 哲学家右手边的筷子 private static final Random random = new Random(1); public Philosophers(String pName, int index, Chopsticks left, Chopsticks right){ this.pName = pName; this.index = index; this.left = left; this.right = right; } @Override public void run() { try { while(true){ think(); // 思考 eat(); // 吃东西 } } catch (InterruptedException e) { throw new RuntimeException(e); } } // 模拟哲学家思考 public void think() throws InterruptedException { TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); } // 模拟哲学家吃东西 public void eat() throws InterruptedException { synchronized (left) { synchronized (right){ TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); System.out.println(pName + \"进餐\"); } } }} Chopsticks.java 12public class Chopsticks {} Main.java 123456789101112131415161718192021public class Main { public static void main(String[] args) { Chopsticks c1 = new Chopsticks(); Chopsticks c2 = new Chopsticks(); Chopsticks c3 = new Chopsticks(); Chopsticks c4 = new Chopsticks(); Philosophers p1 = new Philosophers(\"p1\", 0, c1, c2); Philosophers p2 = new Philosophers(\"p2\", 1, c2, c3); Philosophers p3 = new Philosophers(\"p3\", 2, c3, c4); Philosophers p4 = new Philosophers(\"p4\", 3, c4, c1); p1.start(); p2.start(); p3.start(); p4.start(); }} 启动程序后可以发现出现了死锁问题，所有哲学家都无法进餐。 解决方案找一个哲学家反方向那筷子之前我们一直假定所有哲学家都是先拿左手边的筷子，然后再拿右手边的筷子。当有一个哲学家先拿右手边的筷子，就可以解决死锁问题。 123456789101112假设有四个哲学家p1，p2，p3，p4p1左手边的筷子是c1，右手边的筷子是c2p2左手边的筷子是c2，右手边的筷子是c3p3左手边的筷子是c3，右手边的筷子是c4p4左手边的筷子是c4，右手边的筷子是c1如果p1先拿右手边的筷子，其余哲学家先拿左手边的筷子，会出现以下情况：p1持有了c2p2要拿自己左手边的筷子p2，由于p2已经被p1持有，p2阻塞等待p3持有c3p4持有c4,同时p4可以持有c1，然后进食（或p1可以持有c1，发生进食） 我们可以看到，由于p2无法拿到筷子，导致3个哲学家竞争4根筷子，所以必定有一个哲学家可以持有两个两根筷子发生进食。而持有的筷子释放后，又可以让其他的哲学家持有而发生进食，也就不会发生死锁。 12345678910111213141516171819// 模拟哲学家吃东西public void eat() throws InterruptedException { if(index == 0) { synchronized (right) { synchronized (left){ TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); System.out.println(pName + \"进餐\"); } } } else { synchronized (left) { synchronized (right){ TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); System.out.println(pName + \"进餐\"); } } }} 这种方法其实是一种单线程的方式，只有一个哲学家进食完之后其他的哲学家才能进食，效率比较低。我们可以让编号为奇数的哲学家先拿左手边的筷子，偶数的哲学家拿右手边的筷子，提高进食增加效率。 12345678910111213141516171819// 模拟哲学家吃东西public void eat() throws InterruptedException { if(index % 2 == 0) { synchronized (right) { synchronized (left){ TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); System.out.println(pName + \"进餐\"); } } } else { synchronized (left) { synchronized (right){ TimeUnit.MILLISECONDS.sleep(random.nextInt(100)); System.out.println(pName + \"进餐\"); } } }} 限制进食人数比如四个哲学家，最多只让三个哲学家吃东西，这样最少有一个哲学家可以拿到两根筷子进食，而不会发生死锁。 可以在上述代码的基础上，使用信号量Semaphore来解决。 设置一个信号量，保证最多只有三个哲学家能同时进餐 1private static final Semaphore maxDiners = new Semaphore(3); 然后修改eat方法 1234567891011// 模拟哲学家吃东西public void eat() throws InterruptedException { maxDiners.acquire(); synchronized (right) { synchronized (left){ TimeUnit.MILLISECONDS.sleep(random.nextInt(1)); System.out.println(pName + \"进餐\"); } } maxDiners.release();}","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"vue组件与路由","slug":"前端/vue组件与路由","date":"2021-09-10T16:00:00.000Z","updated":"2023-07-17T13:41:27.784Z","comments":true,"path":"2021/09/11/前端/vue组件与路由/","link":"","permalink":"https://yichenfirst.github.io/2021/09/11/%E5%89%8D%E7%AB%AF/vue%E7%BB%84%E4%BB%B6%E4%B8%8E%E8%B7%AF%E7%94%B1/","excerpt":"","text":"es6的新特性1、变量声明 var 、let 、 const var 作用: 使用这个关键字声明变量是全局变量，容易出现变量混淆。 let 作用: 用来声明一个局部变量 。作用范围：从定义开始，到新定义的代码块结束 。 [推荐使用] const 作用：用来定义一个常量 。作用：变量一旦被定义不能被修改 。特殊说明：修饰number、字符串时变量的值不能修改， 修饰对象的时候对象的地址不能修改。 123456789101112131415&lt;script &gt; function test(){ for(let i = 0; i &lt; 10; i++){ console.log(\"function inner:\",i); } //console.log(\"function out:\",i); const name = \"xiaochen\"; //常量 const user = {name:\"小三\"}; //const 修饰对象，对象地址不能改变 对象中的属性可以随意修改 console.log(user.name); user.name = \"小明\"; console.log(user.name); } test(); //调用函数&lt;/script&gt; 2、字符串模板 通过反引号使用字符串模板 12345678910111213let html = \"&lt;div&gt;\\n\"+ \"&lt;h1&gt;我是标签&lt;/h1&gt;\\n\"+ \"&lt;input type=\\\"text\\\" name=\\\"name\\\"&gt;\\n\"+ \" &lt;button&gt;我是按钮&lt;/button&gt;\\n\"+ \"&lt;/div&gt;\\n\"; let html1 = `&lt;div&gt; &lt;h1&gt;我是标签&lt;/h1&gt; &lt;input type=\"text\" name=\"name\"&gt; &lt;button&gt;我是按钮&lt;/button&gt; &lt;/div&gt;`; console.log(html); console.log(html1); 3、es6 function(){} 匿名函数自己存在自己this，简化写法为()=&gt;(箭头函数) , 箭头函数没有自己this。 4、对象定义方式 es5对象定义方式: 12345let name = \"小陈\";let age = 23;let bir = \"2012-12-12\";const user = {name:name,age:age,bir:bir} ; //es5console.log(user); es6中对象的扩展：当对象属性名和属性赋值名一致时，可以省略变量名不写。 12345let name = \"小陈\";let age = 23;let bir = \"2012-12-12\";const stu = {name,age,bir} ; //es6console.log(stu); Vue中组件(component)1、Vue标准开发模式1.1、vue推荐开发方式是sPA : Single Page (Web)Application （单页面应用） vue推荐开发方式是基于单页面应用单页面web应用 1.2、什么是sPA单页面应用 单页面应用:就是日后项目中只有一张页面 1.3、为什么vue推荐开发方式sPA的开发方式? 引入vue js文件 在现有页面中创建vue实例对象一个页面中只能存在一个vue实例一个。 ==vue推荐开发方式要求:一个应用中只能存在一个vue实例== 1.4、使用现有手段严格遵循sPA存在问题? 现有开发方式导致项目中唯一一个页面中代码越来越多不利后续维护 现有开发方式导致项目中唯一一个页面中完成全部业务功能,导致当前页面每次加载速度非常慢 1.5、为了严格遵循sPa开发方式在vue中提供了vue组件component 组件: 组件减少vue根实例代码量 一个组负贲完成项目中一个功能或者一组功实现业务功能隔离 组件还可以在vue实现复用 2、 组件作用组件作用: 用来减少Vue实例对象中代码量,日后在使用Vue开发过程中,可以根据 不同业务功能将页面中划分不同的多个组件,然后由多个组件去完成整个页面的布局,便于日后使用Vue进行开发时页面管理,方便开发人员维护。组件还可以实现共享和复用。 3 、组件使用3.1 、全局组件注册 说明:全局组件注册给Vue实例,日后可以在任意Vue实例的范围内使用该组件 123456789101112131415161718192021222324252627//1.开发全局组件&lt;script&gt; //开发全局组件 //参数1. 组件名称 参数2：指定组件内容的配置对象 Vue.component('login',{ template:\"&lt;div&gt;&lt;h3&gt;用户登录组件&lt;/h3&gt;&lt;/div&gt;\", //template:模板 用来书写组件中html代码 注意：template属性必须存在一个根容器，有且只有一个根容器 }); Vue.component('register',{ template: \"&lt;div&gt;&lt;h3&gt;用户注册组件&lt;/h3&gt;&lt;/div&gt;\" }); const app = new Vue({ el: \"#app\", data:{ }, methods:{ }, computed:{ } });&lt;/script&gt;//2.使用全局组件 在Vue实例范围内&lt;div id=\"app\"&gt; &lt;!--使用全局组件：根据组件名称使用全局组件--&gt; &lt;login&gt;&lt;/login&gt; &lt;register&gt;&lt;/register&gt;&lt;/div&gt; 1234# 注意: - 1.Vue.component用来开发全局组件 参数1: 组件的名称 参数2: 组件配置{} template:''用来书写组件的html代码 template中必须有且只有一个root元素 - 2.使用时需要在Vue的作用范围内根据组件名使用全局组件 - 3.如果在注册组件过程中使用 驼峰命名组件的方式 在使用组件时 必须将驼峰的所有单词小写加入-线进行使用 3.2 、局部组件注册 说明:通过将组件注册给对应Vue实例中一个components属性来完成组件注册,这种方式不会对Vue实例造成累加 第一种开发方式 1234567891011121314151617181920212223//局部组件登录模板声明//定义登录组件配置对象const login = { //定义一个登录组件 template:\"&lt;div&gt;&lt;h3&gt;用户登录局部组件&lt;/h3&gt;&lt;/div&gt;\"}const app = new Vue({ el: \"#app\", data:{ }, methods:{ }, computed:{ }, components:{ //在这里注册组件都为局部组件 login, //es5 login:login es6 login, register:{ template:\"&lt;div&gt;&lt;h3&gt;用户注册局部组件&lt;/h3&gt;&lt;/div&gt;\" } }});//局部组件使用 在Vue实例范围内&lt;login&gt;&lt;/login&gt;&lt;register&gt;&lt;/register&gt; 第二种开发方式 1234567891011121314151617181920212223242526272829//1.声明局部组件模板 template 标签 注意:在Vue实例作用范围外声明 &lt;template id=\"loginTemplate\"&gt; &lt;div&gt; &lt;h3&gt;用户登录局部组件&lt;/h3&gt; &lt;input type=\"text\"&gt; &lt;/div&gt; &lt;/template&gt;//2.定义变量用来保存模板配置对象 //定义登录组件配置对象 const login = { //定义一个登录组件 template:\"#loginTemplate\" };//3.注册组件 const app = new Vue({ el: \"#app\", data:{ }, methods:{ }, computed:{ }, components:{ //在这里注册组件都为局部组件 login, //注册登录组件 } }); //4.局部组件使用 在Vue实例范围内 &lt;login&gt;&lt;/login&gt; 4、Prop的使用作用:props用来给组件传递相应静态数据或者是动态数据的 4.1、 通过在组件上声明静态数据传递给组件内部12345678910111213141516171819202122232425262728//1.声明组件模板配置对象 const login = { template: \"&lt;div&gt;&lt;h3&gt;登录界面-{{title}}-{{count}}-{{age}}&lt;/h3&gt;&lt;/div&gt;\", data(){ //注意：在props中定义数据，不能在data中重复定义，如果重复定义，优先使用 props中数据为主 return { loginTitle : this.title, }; }, props:['title','count','age'], //props作用 用来接收使用组件时通过组件标签传递的数据 };//2.注册组件// 如何实现父组件向子组件内部传递数据，并将数据在子组件中进行展示?注意：在vue中父组件向子组件传递数据可以使用 prop 属性完成数据传递 const app = new Vue({ el:\"#app\", data:{ msg: \"Vue 中组件开发\", }, methods:{}, computed:{}, components:{ login, //注册组价 } });//3.通过组件完成数据传递 &lt;login title=\"欢迎访问我们的系统!!!\" count=\"11\" age=\"23\"&gt;&lt;/login&gt; 1234# 总结: 1.使用组件时可以在组件上定义多个属性以及对应数据 2.在组件内部可以使用props数组生命多个定义在组件上的属性名 日后可以在组件中通过{{ 属性名 }} 方式获取组件中属性值 3.在props中定义数据，不能在data中重复定义，如果重复定义，优先使用props中数据为主 4.2、通过在组件上声明动态数据传递给组件内部12345678910111213141516171819202122232425//1.声明组件模板对象 const login = { template: \"&lt;div&gt;&lt;h3&gt;登录界面-{{title}}&lt;/h3&gt;&lt;/div&gt;\", data(){ return { }; }, props:['title'], //props作用 用来接收使用组件时通过组件标签传递的数据 };//2.注册局部组件 const app = new Vue({ el:\"#app\", data:{ msg: \"Vue 中组件开发\", name:\"小李\", }, methods:{}, computed:{}, components:{ login, //注册组价 } });//3.使用组件 &lt;login :title=\"name\"&gt;&lt;/login&gt; //使用v-bind形式将数据绑定Vue实例中data属性,日后data属性发生变化,组件内部数据跟着变化 4.3、prop的单向数据流 单向数据流:所有的 prop 都使得其父子 prop 之间形成了一个单向下行绑定：父级 prop 的更新会向下流动到子组件中，但是反过来则不行。 所有的 prop 都使得其父子 prop 之间形成了一个单向下行绑定：父级 prop 的更新会向下流动到子组件中，但是反过来则不行。这样会防止从子组件意外改变父级组件的状态，从而导致你的应用的数据流向难以理解。 额外的，每次父级组件发生更新时，子组件中所有的 prop 都将会刷新为最新的值。这意味着你不应该在一个子组件内部改变 prop。如果你这样做了，Vue 会在浏览器的控制台中发出警告。—摘自官网 5、组件中定义数据和事件使用1. 组件中定义属于组件的数据12345678910//定义局部用户列表组件配置对象const users = { template:\"&lt;div&gt;&lt;h3&gt;用户列表-{{count}}-{{name}}&lt;/h3&gt;&lt;/div&gt;\", //用来定义组件html内容 data(){ //用来给当前组件定义一系列数据 注意：在组件中定义的数据只能在当前组件中可用 return { count:0, name:\"小李\", }; }}; 2.组件中事件定义1234567891011121314151617181920212223242526272829//定义局部用户列表组件配置对象const users = { template:\"&lt;div&gt;&lt;h3&gt;用户列表-{{count}}-{{name}}-{{countSqrt}}&lt;/h3&gt;&lt;button @click='changeCount'&gt;+&lt;/button&gt;&lt;ul&gt;&lt;li v-for='item in items'&gt;{{item}}&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;\", //用来定义组件html内容 data(){ //用来给当前组件定义一系列数据 注意：在组件中定义的数据只能在当前组件中可用 return { count:0, name:\"小李\", //items:[\"山西\",\"北京\",\"天津\"], items:[], }; }, methods:{ //用来给当前组件定义一系列事件 changeCount(){ this.count++; } }, computed:{ //用来给当前组件定义一系列计算属性，用来对页面中结果进行二次计算处理时候 countSqrt(){ return this.count*this.count; } }, created(){ //组件已经注入了data、methods、computed 相关数据方法 //发送请求 /*axios.get(\"/xxx\").then(res=&gt;{ this.items = res.data; });*/ this.items=[\"山西\",\"北京\",\"天津\"]; },}; 123# 总结 1.组件中定义事件和直接在Vue中定义事件基本一致 直接在组件内部对应的html代码上加入@事件名=函数名方式即可 2.在组件内部使用methods属性用来定义对应的事件函数即可,事件函数中this 指向的是当前组件的实例 6、向子组件中传递事件并在子组件中调用该事件在子组件中调用传递过来的相关事件必须使用 this.$emit(‘函数名’) 方式调用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;信息：{{msg}} 年龄：{{age}}&lt;/h1&gt; &lt;login @aa=\"findAll\" @test=\"test\"&gt;&lt;/login&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;script&gt; //如何将父组件中事件传递给子组件中 1.在使用事件时使用@别名=“传递事件名” 2.在组件中调用传递的事件时 this.$emit('别名') //定义一个登录组件配置对象 const login = { template: \"&lt;div&gt;&lt;h3&gt;用户登录-{{msg}}&lt;/h3&gt;&lt;button @click='test'&gt;点我触发事件&lt;/button&gt;&lt;/div&gt;\", data(){ return { msg:\"我是子组件的信息\", age:23, user:{id:2,name:\"小陈\",age:23,bir:\"2012-02-09\"}, }; }, methods:{ test(){ alert('子组件中定义的事件...'); //调用父组件中findAll事件 aa 事件名： // this.$emit('aa'); //$emit用来调用父组件中相关事件 this.$emit('test',this.msg,this.age,this.user); //$emit用来调用父组件中相关事件,并传递参数 } } }; const app = new Vue({ el:\"#app\", data:{ msg: \"Vue 中组件开发\", age:23, user:{id:1,name:\"小吴\",age:12}, }, methods:{ findAll(){ alert('父组件中定义的事件...'); }, test(msg,age,user) { alert('父组件中定义test的事件....'+msg); console.log(\"msg:\",msg); console.log(\"age\",age); console.log(\"user\",user); this.msg = msg; this.age = age; } }, computed:{}, components:{ login, //注册组件 } });&lt;/script&gt; 7、插槽的使用怎么理解插槽？ slot相当于一个空标签，通过vue可以实现动态改变值和样式，把一块区域内容抽了出来可以实现复用，就和Java里封装的工具类一样。 插槽就是子组件中的提供给父组件使用的一个占位符。 我们在构建页面过程中一般会把用的比较多的公共的部分抽取出来作为一个单独的组件，但是在实际使用这个组件的时候却又不能完全的满足需求，我希望在这个组件中添加一点东西，这时候我们就需要用到插槽来分发内容。 通俗易懂的讲，slot具有“占坑”的作用，在子组件占好了位置，那父组件使用该子组件标签时，新添加的DOM元素就会自动填到这个坑里面 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!--vue使用组件--&gt; &lt;login&gt;&lt;h5 slot=\"aa\"&gt;我是用户自定义aa内容&lt;/h5&gt;&lt;h5 slot=\"bb\"&gt;我是用户自定义bb内容&lt;/h5&gt;&lt;/login&gt; &lt;hr&gt; &lt;login&gt;&lt;button&gt;+++&lt;/button&gt;&lt;/login&gt; &lt;hr&gt; &lt;login&gt;&lt;/login&gt; &lt;hr&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;script&gt; //slot：插槽 作用：用来扩展自定义组件让组件变得更加灵活，用来个性化定制自己的组件 注意：插槽必须配合组件才能使用 const login = { template: \"&lt;div&gt;&lt;slot name='aa'&gt;&lt;span&gt;默认扩展&lt;/span&gt;&lt;/slot&gt;&lt;h3&gt;用户登录界面&lt;/h3&gt;&lt;slot name='bb'&gt;&lt;span&gt;默认扩展&lt;/span&gt;&lt;/slot&gt;&lt;/div&gt;\", }; const app = new Vue({ el:\"#app\", data:{ msg:\"slot 插槽的使用\" }, methods:{ }, computed:{}, components: { login, }, });&lt;/script&gt; Vue中路由 (Vue Router)1、路由路由:根据请求的路径按照一定的路由规则进行请求的转发从而帮助我们实现统一请求的管理 2、作用 用来在vue中实现组件之间的动态切换 3、使用路由3.1、引入路由 12345&lt;!--cdn方式--&gt;&lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"https://unpkg.com/vue-router/dist/vue-router.js\"&gt;&lt;/script&gt; //vue 路由js&lt;!--本地引入--&gt;&lt;script src=\"js/vue-router.js\"&gt;&lt;/script&gt; 3.2、创建组件对象 12345678//创建组件配置对象const users = { template:\"&lt;div&gt;&lt;h3&gt;用户管理&lt;/h3&gt;&lt;/div&gt;\"};const emps = { template: \"&lt;div&gt;&lt;h3&gt;员工管理&lt;/h3&gt;&lt;/div&gt;\"} 3.3、定义路由对象的规则 123456const router = new VueRouter({ routes:[ {path:'/users',component:users}, //用来定义具体得的某个组件路由规则 path:用来指定对应请求路径 component:指定路径对应的组件 {path:'/emps', component:emps } //用来定义一些规则 ]}); 3.4、将路由对象注册到vue实例 12345678910const app = new Vue({ el:\"#app\", data:{ msg:\"vue 中router(路由)的使用\", }, methods:{}, computed:{}, components:{}, //注册局部组件 router:router, //用来注册路由配置}); 3.5、在页面中显示路由的组件 12&lt;!--指定路由组件在哪里展示 router-view 标签：作用：用来展示路由对应组件显示的位置--&gt;&lt;router-view&gt;&lt;/router-view&gt; 3.6、根据链接切换路由 12&lt;a href=\"#/users\"&gt;用户管理&lt;/a&gt;&lt;a href=\"#/emps\"&gt;员工管理&lt;/a&gt; 4、router-link使用**作用:**用来替换我们在切换路由时使用a标签切换路由 **好处:**就是可以自动给路由路径加入#不需要手动加入 123&lt;!--router-link 标签： 作用：用来替换a标签 好处：书写路径时不需要显示加入# to: 用来指定路由路径 tag：默认为a标签 用来指定router-link底层渲染标签--&gt;&lt;router-link to=\"/users\" tag=\"a\"&gt;用户管理(link)&lt;/router-link&gt;&lt;router-link to=\"/emps\" tag=\"a\"&gt;员工管理(link)&lt;/router-link&gt; 123# 总结: 1.router-link 用来替换使用a标签实现路由切换 好处是不需要书写#号直接书写路由路径 2.router-link to属性用来书写路由路径 tag属性:用来将router-link渲染成指定的标签 5、默认路由**作用:**用来在第一次进入界面是显示一个默认的组件 12345678const router = new VueRouter({ routes:[ //{ path:'/',component:login}, {path:'/',redirect:'users' }, //默认路由规则 redirect:(路由路径重定向) {path:'/users',component:users}, //用户组件路由规则 {path:'/emps', component:emps } //员工组件路由规则 ]}); 6、路由动态切换两种方式方式一：通过使用标签方式直接进行路由切换 12345&lt;!--1.通过使用标签方式直接进行路由切换--&gt;&lt;a href=\"#/users\"&gt;用户管理&lt;/a&gt;&lt;a href=\"#/emps\"&gt;员工管理&lt;/a&gt;&lt;router-link to=\"/users\" tag=\"a\"&gt;用户管理(link)&lt;/router-link&gt;&lt;router-link to=\"/emps\" tag=\"a\"&gt;员工管理(link)&lt;/router-link&gt; 方式二：通过js代码的方式进行动态切换路由 1234567891011121314151617181920&lt;!--2.通过js代码的方式进行动态切换路由 this.router.push(\"切换的路由路径\")--&gt;&lt;button @click=\"test\"&gt;测试动态路由&lt;/button&gt;const app = new Vue({ el:\"#app\", data:{ msg:\"vue 中router(路由)的使用 route-link标签的使用\", }, methods:{ test(){ console.log(\"test\"); //this.$router.push(\"/emps\"); //代表切换路由路径 //this.$router.push({path:'/emps'}); //切换路由 this.$router.push({name:\"emps\"}); //名称方式切换路由 [推荐] }, }, computed:{}, components:{}, //注册局部组件 router, //注册路由}); 7、路由中参数传递第一种方式传递参数 传统方式 1、通过?号形式拼接参数 12&lt;!--1.获取？后传递参数 this.route.query.key.?后面参数key--&gt;&lt;router-link to=\"/users?deptid=21name=李四\"&gt;用户管理&lt;/router-link&gt; 2、组件中获取参数 1234567891011121314const users = { template:\"&lt;div&gt;&lt;h1&gt;用户管理&lt;/h1&gt;&lt;/div&gt;\", data(){ return {}; }, methods: {}, created(){ //获取路由路径中参数 1.获取queryString(deptid=21)中传递参数 //this.$route 当前路由对象 //this.$router 路由管理器对象 console.log(\"deptid:\",this.$route.query.deptid); console.log(\"name:\",this.$route.query.name); }}; 第二种方式传递参数 restful 1、通过使用路径方式传递参数 1234567&lt;!--2.获取路由路径中参数 rest 方式参数获取 this.$route.params.路径中别名--&gt;&lt;router-link to=\"/emps/11/王五\"&gt;员工管理&lt;/router-link&gt;const router = new VueRouter({ routes:[ {path:'/emps/:id/:name', name:'emps',component:emps } //员工组件路由规则 ]}); 2、组件中获取参数 1234567891011const emps = { template: \"&lt;div&gt;&lt;h1&gt;员工管理&lt;/h1&gt;&lt;/div&gt;\", data(){ return {}; }, methods: {}, created() { console.log(\"id:\",this.$route.params.id); //获取路径中的参数 console.log(\"name:\",this.$route.params.name); //获取路径中的参数 }} 完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!--切换路由--&gt; &lt;!--1.获取？后传递参数 this.route.query.key.?后面参数key--&gt; &lt;router-link to=\"/users?deptid=21name=李四\"&gt;用户管理&lt;/router-link&gt; &lt;!--2.获取路由路径中参数 rest 方式参数获取 this.$route.params.路径中别名--&gt; &lt;router-link to=\"/emps/11/王五\"&gt;员工管理&lt;/router-link&gt; &lt;!--router-view 显示路由组件--&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;script src=\"js/vue-router.js\"&gt;&lt;/script&gt;&lt;script&gt; //创建组件配置对象 const users = { template:\"&lt;div&gt;&lt;h1&gt;用户管理&lt;/h1&gt;&lt;/div&gt;\", data(){ return {}; }, methods: {}, created(){ //获取路由路径中参数 1.获取queryString(deptid=21)中传递参数 //this.$route 当前路由对象 //this.$router 路由管理器对象 console.log(\"deptid:\",this.$route.query.deptid); console.log(\"name:\",this.$route.query.name); } }; const emps = { template: \"&lt;div&gt;&lt;h1&gt;员工管理&lt;/h1&gt;&lt;/div&gt;\", data(){ return {}; }, methods: {}, created() { console.log(\"id:\",this.$route.params.id); //获取路径中的参数 console.log(\"name:\",this.$route.params.name); //获取路径中的参数 } } //创建路由对象并定义路由规则 const router = new VueRouter({ routes:[ {path:'/',redirect:'users' }, //默认路由规则 redirect:(路由路径重定向) {path:'/users',name:'users',component:users}, //用户组件路由规则 name:路由名称 必须唯一 {path:'/emps/:id/:name', name:'emps',component:emps } //员工组件路由规则 ] }); const app = new Vue({ el:\"#app\", data:{ msg:\"vue 中router(路由)的使用 route-link标签的使用\", }, methods:{ }, computed:{}, components:{}, //注册局部组件 router, //注册路由 });&lt;/script&gt; 8、嵌套路由8.1、声明最外层和内层路由 123456789101112131415161718192021222324&lt;template id=\"product\"&gt; &lt;div&gt; &lt;h1&gt;商品管理&lt;/h1&gt; &lt;router-link to=\"/product/add\"&gt;商品添加&lt;/router-link&gt; &lt;router-link to=\"/product/edit\"&gt;商品编辑&lt;/router-link&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;//声明组件模板const product={ template:'#product'};const add = { template:'&lt;h4&gt;商品添加&lt;/h4&gt;'};const edit = { template:'&lt;h4&gt;商品编辑&lt;/h4&gt;'}; 8.2、创建路由对象含有嵌套路由 123456789101112const router = new VueRouter({ routes:[ { path:'/product', component:product, children:[ {path:'add',component: add}, {path:'edit',component: edit}, ] }, ] }); 8.3、注册路由对象 123456const app = new Vue({ el: \"#app\", data: {}, methods: {}, router,//定义路由对象}); 8.4、测试路由 12&lt;router-link to=\"/product\"&gt;商品管理&lt;/router-link&gt;&lt;router-view&gt;&lt;/router-view&gt; 案例练习： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!--路由链接--&gt; &lt;router-link to=\"/products\"&gt;商品管理&lt;/router-link&gt; &lt;!--显示路由组件--&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;template id=\"productsTemplate\"&gt; &lt;div&gt; &lt;div&gt;&lt;h3&gt;商品列表&lt;/h3&gt;&lt;/div&gt; &lt;a href=\"#/products/add\"&gt;添加商品信息&lt;/a&gt; &lt;table border=\"1\"&gt; &lt;tr&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;名称&lt;/th&gt; &lt;th&gt;价格&lt;/th&gt; &lt;th&gt;生产日期&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;短袖&lt;/td&gt; &lt;td&gt;60.88&lt;/td&gt; &lt;td&gt;2021-06-09&lt;/td&gt; &lt;!--&lt;th&gt;&lt;a href=\"\"&gt;删除&lt;/a&gt;&lt;a href=\"#/products/edit\"&gt;修改&lt;/a&gt;&lt;/th&gt;--&gt; &lt;td&gt;&lt;a href=\"\"&gt;删除&lt;/a&gt;&lt;a href=\"javascript:;\" @click.prevent=\"editRow({id:1,name:'小陈'})\"&gt;修改&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;!--router-view 用来展示子路由组件--&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;script src=\"js/vue-router.js\"&gt;&lt;/script&gt;&lt;script&gt; //创建组件配置对象 const products = { template:\"#productsTemplate\", data(){ return {}; }, methods:{ editRow(user){ console.log(user); this.$router.push({path:'/products/edit',query:user}); //切换路由路径 通过query(?) } } }; //创建添加商品信息子组件 const add = { template: \"&lt;div from action=''&gt;商品名称: &lt;input type='text'&gt;&lt;br&gt;商品价格: &lt;input type='text'&gt;&lt;br&gt; &lt;input type='button' value='保存商品'&gt;&lt;/div&gt;\" } const edit = { template: \"&lt;div from action=''&gt;商品名称: &lt;input type='text' v-model='user.id'&gt;&lt;br&gt;商品价格: &lt;input type='text' v-model='user.name'&gt;&lt;br&gt; &lt;input type='button' value='确认修改'&gt;&lt;/div&gt;\", data() { return { user:{}, }; }, methods:{}, created(){ console.log(\"edit: \",this.$route.query); this.user = this.$route.query; } } //创建路由对象 const router = new VueRouter({ routes:[ {path:'/',redirect:'/products' }, //默认路由 { path:'/products', name:'products', component:products, children:[ //定义子路由 注意：子路由的path属性不能使用'/'开头 {path:\"add\",component:add}, {path:\"edit\",component:edit}, ] }, ] }); const app = new Vue({ el:\"#app\", data:{ msg:\"vue 中router 中嵌套路由的使用\", }, methods:{}, computed:{}, components:{ products, }, router, //注册路由 });&lt;/script&gt; 运行截图：","categories":[],"tags":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/tags/vue/"}]},{"title":"ElasticSearch概述","slug":"ElasticSearch","date":"2021-09-01T16:00:00.000Z","updated":"2023-07-17T13:41:27.774Z","comments":true,"path":"2021/09/02/ElasticSearch/","link":"","permalink":"https://yichenfirst.github.io/2021/09/02/ElasticSearch/","excerpt":"","text":"ElasticSearch概述Elasticsearch是一个基于==Lucene==的搜索服务器。它提供了一个==分布式多用户==能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。Elasticsearch用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Lucene简介Lucene是apache软件基金会4 jakarta项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。Lucene是一套用于全文检索和搜寻的开源程式库，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。人们经常提到信息检索程序库，虽然与搜索引擎有关，但不应该将信息检索程序库与搜索引擎相混淆。 Lucene是一个全文检索引擎的架构。那什么是全文搜索引擎？ 全文搜索引擎是名副其实的搜索引擎，国外具代表性的有Google、Fast/AllTheWeb、AltaVista、Inktomi、Teoma、WiseNut等，国内著名的有百度（Baidu）。它们都是通过从互联网上提取的各个网站的信息（以网页文字为主）而建立的数据库中，检索与用户查询条件匹配的相关记录，然后按一定的排列顺序将结果返回给用户，因此他们是真正的搜索引擎。 从搜索结果来源的角度，全文搜索引擎又可细分为两种，一种是拥有自己的检索程序（Indexer），俗称“蜘蛛”（Spider）程序或“机器人”（Robot）程序，并自建网页数据库，搜索结果直接从自身的数据库中调用，如上面提到的7家引擎；另一种则是租用其他引擎的数据库，并按自定的格式排列搜索结果，如Lycos引擎。 ElasticSearch与SolrSolr简介Solr 是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化 Solr可以独立运行，运行在Jetty、Tomcat等这些Servlet容器中，Solr 索引的实现方法很简单，用 POST 方法向 Solr 服务器发送一个描述 Field 及其内容的 XML 文档，Solr根据xml文档添加、删除、更新索引 。Solr 搜索只需要发送 HTTP GET 请求，然后对 Solr 返回Xml、json等格式的查询结果进行解析，组织页面布局。Solr不提供构建UI的功能，Solr提供了一个管理界面，通过管理界面可以查询Solr的配置和运行情况。 solr是基于lucene开发企业级搜索服务器，实际上就是封装了lucene。 Solr是一个独立的企业级搜索应用服务器，它对外提供类似于Web-service的API接口。用户可以通过http请求，向搜索引擎服务器提交一定格式的文件，生成索引；也可以通过提出查找请求，并得到返回结果。 ElasticSearch vs Solr 总结（1）es基本是开箱即用，非常简单。Solr安装略微复杂。 （2）Solr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能。 （3）Solr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。 （4）Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供，例如图形化界面需要kibana友好支撑 （5）Solr 查询快，但更新索引时慢（即插入删除慢），用于电商等查询多的应用； ES建立索引快（即查询慢），即实时性查询快，用于facebook新浪等搜索。 Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。 （6）Solr比较成熟，有一个更大，更成熟的用户、开发和贡献者社区，而 Elasticsearch相对开发维护者较少，更新太快，学习使用成本较高。 ES安装及head插件安装ES安装ELK（Elasticsearch , Logstash, Kibana）版本要一致，解压即用（需要nodejs环境） 下载地址：https://www.elastic.co/start ​ https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-windows-x86_64.zip 熟悉目录结构 12345678bin 启动文件config 配置文件 log4j2 日志配置文件 jvm.option 虚拟机相关配置文件 elasticsearch.yml elasticsearch的配置文件 默认9200端口！跨域！lib 相关jar包models 功能模块plugins 插件 ik 启动，访问9200双击bin/elasticsearch.bat 安装可视化界面es head插件下载地址：https://github.com/mobz/elasticsearch-head cnpm install 安装 cnpm run start 启动，访问9100 出现跨域问题 在elasticsearch的配置文件elasticsearch.yml中添加 12http.cors.enabled: truehttp.cors.allow-origin: \"*\" 再次访问9100 ELK介绍ELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana , 它们都是开源软件。 Elasticsearch是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。主要负责将日志索引并存储起来，方便业务方检索查询。 Logstash 主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。是一个日志收集、过滤、转发的中间件，主要负责将各条业务线的各类日志统一收集、过滤后，转发给 Elasticsearch 进行下一步处理。 Kibana 也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。 Kibana安装下载下载地址：https://www.elastic.co/downloads/kibana ​ https://artifacts.elastic.co/downloads/kibana/kibana-7.6.2-windows-x86_64.zip 解压即用 启动，访问5601 汉化在config/kibana.yml中添加 1i18n.locale: \"zh-CN\" ES核心概念 概述 elasticsearch是面向文档的 关系型数据库和elasticsearch对比 Relational DB Elasticsearch（一切JSON） 数据库（database） 索引（indices） 表（tables） types（会被弃用） 行（rows） documents 字段（columns） fields 物理设计： elasticsearch在后台把每个索引划分成多个分片，每份分片可以再集群中的不同服务器间迁移 逻辑设计： 一个索引类型中，包含多个文档。当我们索引一片文档时，可以通过这样的顺序找到：索引&gt;类型&gt;文档ID，ID是个字符串。 文档 文档是ElasticSearch中的最顶层结构被序列化程JSON数据，并做唯一表示存储 文档里面存储的是一条条数据 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档唯一标识 _index索引类似关系型数据口中的数据库，存储和索引数据的地方。实际上数据和索引时被存储在分片中，可以理解为分片是库的一部分。 _typetype可以理解成关系型数据库中的表，表内存储的对象有着相同的结构，即存储在通过type中的数据对象结构相同。type会有自己的映射，类似表的字段。_type的命名规则可以大写，小写不能包含下划线或逗号。 _idid时文档的唯一表示，由ElasticSearch自动生成 索引 就是数据库 倒排索引 IK分词器中文分词器：IK分词器 安装https://github.com/medcl/elasticsearch-analysis-ik 解压到elasticsearch的plugins中 elasticsearch-plugin list查看加载的插件 在kibana中测试ik分词器 ik分词器中包含两个算法：ik_smart, ik_max_word ik_smart：最少划分 ik_max_word: 为最细粒度划分！穷尽词库的可能！ ik分词器的配置自己的词典 在新创建.dic文件中添加自定义词组，并在配置文件中添加新创建的.dic文件 Rest风格说明 method url地址 描述 PUT localhost:9200/索引名称/类型名称/文档 id 创建文档（指定文档 id） POST localhost:9200/索引名称/类型名称 创建文档（随机文档 id） POST localhost:9200/索引名称/类型名称/文档 id/_update 修改文档 DELETE localhost:9200/索引名称/类型名称/文档 id 删除文档 GET localhost:9200/索引名称/类型名称/文档 id 查询文档通过文档 id POST localhost:9200/索引名称/类型名称/_search 查询所有数据 基础测试 索引操作1、创建一个索引 12PUT /索引名/类型名/文档ID{请求体} 完成了自动增加了索引！数据也成功添加了。 字段类型 一级分类 二级分类 具体类型 核心类型 字符串类型 string,text,keyword 整数类型 integer,long,short,byte 浮点类型 double,float,half_float,scaled_float 逻辑类型 boolean 日期类型 date 范围类型 range 二进制类型 binary 复合类型 数组类型 array 对象类型 object 嵌套类型 nested 地理类型 地理坐标类型 geo_point 地理地图 geo_shape 特殊类型 IP类型 ip 范围类型 completion 令牌计数类型 token_count 附件类型 attachment 抽取类型 percolator 指定字段类型 获得规则 通过get请求查看索引信息 查看默认信息 如果自己的文档没有指定，那么es就会默认配置字段类型 通过get _cat/可以获得es的当前信息 修改 使用PUT覆盖之前的值 使用POST修改 删除 12DELETE test1 #删除索引DELETE test3/_doc/1 #删除文档 文档操作 基本操作 12345678PUT chen/_doc/3{ \"name\": \"李四\", \"age\": 33, \"desc\": \"mmmmmm\", \"tags\": [\"唱歌\", \"交友\"]} 查询 1GET /chen/_doc/1 更新数据（PUT， POST） 推荐POST（见上节） 简单的条件查询 1GET chen/_doc/_search?q=name:chen 复杂搜索 12345678GET chen/_doc/_search{ \"query\":{ \"match\":{ \"name\": \"chen\" } }} 结果过滤，只查询name，desc字段 123456789GET chen/_doc/_search{ \"query\":{ \"match\":{ \"name\": \"chen\" } }, \"_source\":[\"name\", \"desc\"] } 排序 12345678910111213GET chen/_doc/_search{ \"query\":{ \"match\":{ \"name\": \"chen\" } }, \"sort\": [{ \"age\": { \"order\": \"asc\" } }] } 分页 123456789101112131415GET chen/_doc/_search{ \"query\":{ \"match\":{ \"name\": \"chen\" } }, \"sort\": [{ \"age\": { \"order\": \"asc\" } }] , \"from\": 0, \"size\":1} 布尔查询 must命令想当与and 123456789101112131415161718GET chen/_doc/_search{ \"query\":{ \"bool\":{ \"must\":[ { \"match\":{ \"name\":\"chen\" }, \"match\":{ \"age\":23 } } ] } }} should命令相当于or must_not相当于not springboot集成ES问题： 修改配置文件 修改成本地版本 ElasticSearchClientConfig.java 1234567891011121314@Configurationpublic class ElasticSearchClientConfig { @Bean public RestHighLevelClient restHighLevelClient(){ RestHighLevelClient client = new RestHighLevelClient( RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\") ) ); return client; }} API索引操作1、创建索引 12345678910111213@Testvoid contextLoads() throws IOException { //创建索引请求 CreateIndexRequest request = new CreateIndexRequest(\"chen1\"); //执行创建请求 CreateIndexResponse createIndexResponse = restHighLevelClient.indices().create(request, RequestOptions.DEFAULT); System.out.println(createIndexResponse); } 2、判断索引是否存在 123456789// 测试获取索引,判断索引存不存在@Testvoid testExistIndex() throws IOException { GetIndexRequest request = new GetIndexRequest(\"chen1\"); boolean exists = restHighLevelClient.indices().exists(request, RequestOptions.DEFAULT); System.out.println(exists);} 3、删除索引 12345678//测试删除索引@Testvoid deleteIndex() throws IOException { DeleteIndexRequest request = new DeleteIndexRequest(\"chen1\"); AcknowledgedResponse delete = restHighLevelClient.indices().delete(request, RequestOptions.DEFAULT); System.out.println(delete);} 文档操作1、添加文档 12345678910111213141516171819//添加文档 @Test void testAddDocument() throws IOException { //创建对象 User user = new User(\"chen\", 13); //创建请求 IndexRequest request = new IndexRequest(\"chen1\"); //规则 request.id(\"1\"); //讲数据放入请求， json, 在xml中导入fastjson request.source(JSON.toJSONString(user), XContentType.JSON); //客户端发送请求 IndexResponse indexResponse = restHighLevelClient.index(request, RequestOptions.DEFAULT); System.out.println(indexResponse.toString()); System.out.println(indexResponse.status()); } 2、判断文档是否存在 123456789101112//判断文档是否存在 @Test void testExistsDocument() throws IOException { GetRequest getRequest = new GetRequest(\"chen1\", \"1\"); //不返回_source上下文 getRequest.fetchSourceContext(new FetchSourceContext(false)); getRequest.storedFields(\"_none_\"); boolean exists = restHighLevelClient.exists(getRequest, RequestOptions.DEFAULT); System.out.println(exists); } 3、修改文档 12345678910111213//修改文档信息@Testvoid testUpdateDocument() throws IOException { UpdateRequest updateRequest = new UpdateRequest(\"chen1\", \"1\"); User user = new User(\"逸辰\", 12); updateRequest.doc(JSON.toJSONString(user), XContentType.JSON); UpdateResponse update = restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); System.out.println(update);} 4、删除文档 1234567//删除文档信息 @Test void testDeleteDocument() throws IOException { DeleteRequest request = new DeleteRequest(\"chen1\", \"1\"); DeleteResponse deleteResponse = restHighLevelClient.delete(request, RequestOptions.DEFAULT); System.out.println(deleteResponse); } 5、获得文档信息 12345678//获得文档信息@Testvoid testGetDocument() throws IOException { GetRequest getRequest = new GetRequest(\"chen1\", \"1\"); GetResponse getResponse = restHighLevelClient.get(getRequest, RequestOptions.DEFAULT); System.out.println(getResponse.getSourceAsString());} 6、批量插入数据 1234567891011121314151617181920212223242526272829303132333435363738//批量插入数据 @Test void testBulkRequest() throws IOException { BulkRequest bulkRequest = new BulkRequest(); bulkRequest.timeout(\"10s\"); ArrayList&lt;User&gt; userList = new ArrayList&lt;&gt;(); userList.add(new User(\"chen11\", 72)); userList.add(new User(\"chen12\", 52)); userList.add(new User(\"chen13\", 42)); userList.add(new User(\"chen14\", 32)); userList.add(new User(\"chen15\", 2)); userList.add(new User(\"chen16\", 1)); userList.add(new User(\"chen17\", 12)); userList.add(new User(\"chen18\", 19)); userList.add(new User(\"chen19\", 12)); userList.add(new User(\"chen111\", 9)); userList.add(new User(\"chen122\", 18)); userList.add(new User(\"chen133\", 12)); userList.add(new User(\"chen144\", 17)); userList.add(new User(\"chen155\", 12)); userList.add(new User(\"chen166\", 16)); userList.add(new User(\"chen177\", 12)); userList.add(new User(\"chen188\", 17)); userList.add(new User(\"chen199\", 13)); userList.add(new User(\"chen100\", 14)); for (int i = 0; i &lt; userList.size(); i++) { bulkRequest.add( new IndexRequest(\"chen1\") .id(\"\"+(i+1)) .source(JSON.toJSONString(userList.get(i)), XContentType.JSON)); } BulkResponse bulkResponse = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT); System.out.println(bulkResponse.toString()); System.out.println(bulkResponse.hasFailures()); } 7、查询 12345678910111213141516171819202122//查询 @Test void testSearch() throws IOException { SearchRequest searchRequest = new SearchRequest(\"chen1\"); //构建查询条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //查询条件，可以使用QueryBuilders工具来实现 //QueryBuilders.termQuery 精确 //QueryBuilders.matchAllQuery 匹配所有 TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(\"name\", \"chen\"); sourceBuilder.query(termQueryBuilder); searchRequest.source(sourceBuilder); SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); System.out.println(searchResponse.getHits().toString()); }","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://yichenfirst.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://yichenfirst.github.io/tags/ElasticSearch/"}]},{"title":"Vue安装教程","slug":"前端/vue安装教程","date":"2021-08-30T16:00:00.000Z","updated":"2023-07-17T13:41:27.774Z","comments":true,"path":"2021/08/31/前端/vue安装教程/","link":"","permalink":"https://yichenfirst.github.io/2021/08/31/%E5%89%8D%E7%AB%AF/vue%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/","excerpt":"","text":"一、node.js安装和配置1.下载安装node.js 下载地址：http://nodejs.cn/download/ 根据电脑位数选择.msi安装包进行安装 查看npm版本，在cmd中输入npm -v 2.配置默认安装目录和缓存日志目录 1）、创建默认安装目录和缓存日志目录 2）、执行命令，将npm的全局模块目录和缓存目录配置到我们刚才创建的两个目录： 12npm config set prefix \"D:\\Program Files\\nodejs\\node_global\"npm config set cache \"D:\\Program Files\\nodejs\\node_cache\" 123npm config get prefix 查看npm全局安装包保存路径npm config get cache 查看npm安装包缓存路径npm list -global 查看全局安装目录 3.node.js环境配置（根据自己的node安装路径配置） 1）、【系统变量】下新建【NODE_PATH】 2)、【系统变量】下的【path】添加上node的路径【D:\\Program Files\\nodejs】 4.配置淘宝镜像源 查看npm下载源 1npm config get registry 将npm的模块下载仓库从默认的国外站点改为国内的站点，加快下载速度。一般使用淘宝的镜像源（https://registry.npm.taobao.org）： 1）、临时使用 1npm --registry https://registry.npm.taobao.org install cluster 2)、永久使用 有两种配置 （1）、直接修改npm的默认配置 1npm config set registry https://registry.npm.taobao.org （2）、安装cnpm 1npm install -g cnpm --registry=https://registry.npm.taobao.org 二、安装vue及脚手架1.安装vue.js 12npm install vue -gcnpm install vue -g 根据自己的淘宝镜像源设置选择命令，其中-g是全局安装，指安装到global全局目录去 查看安装vue信息 12npm info vuecnpm info vue 查看安装的vue版本 1npm list vue 2.安装webpack模板 1npm install webpack -g webpack 4x以上，webpack将命令相关的内容都放到了webpack-cli，所以还需要安装webpack-cli：npm install --global webpack-cli，安装成功后可使用webpack -v查看版本号 3.安装脚手架vue-cli 2.x 1npm install vue-cli -g 安装vue-router 1npm install -g vue-router vue-cli2创建vue项目 1）、创建项目（最好在cd到D盘的某个位置，即项目的路径，否则项目会新建在C:\\Users\\Administrator\\，也可以直接在想要的项目路径下输入cmd） 1vue init webpack 项目名 Project name（工程名）:回车(含大写字母给我报错了，我给改了my-vue)\\ Project description（工程介绍）：回车 Author：作者名 ：回车 Vue build ==&gt; （是否安装编译器）runtime-compiler、 runtime-only 都是打包方式，第二个效率更高； Install vue-router ==&gt; 是否要安装 vue-router，项目中肯定要使用到路由，所以Y 回车； Use ESLint to lint your code ==&gt; 是否需要ESLint检测代码，目前我们不需要所以 n 回车； Set up unit tests ==&gt; 是否安装 单元测试工具 目前我们不需要 所以 n 回车； Setup e2e tests with Nightwatch ==&gt;是否需要端到端测试工具目前我们不需要所以n回车； Should we run npm install for you after the project has been created? (recommended) (Use arrow keys)==&gt; 安装依赖npm install 回车； 省略部分截图 最终结果如下，项目初始化成功： 按照提示进入项目目录，运行项目 12cd hellonpm run dev 在浏览器地址栏中输入http://localhost:8080，打开网址如下： 2）、创建项目中的一些问题 （1）、输入vue init webpack 项目名， 提示vue不是内部命令或外部命令，也还不是可执行的程序或批处理文件 在控制台输入npm config list 将prefix处目录加入path中，重新打开一个cmd创建此项目。 （2）、vue-cli2创建项目提示乱码： 在cmd中输入CHCP 65001，按回车键即可将编码格式设成utf-8，再创建就不会乱码了。这样只是临时修改，重新打开一个cmd创建项目还是会乱码。可以直接将cmd的编码格式改成utf-8。 参考： https://blog.csdn.net/dream_summer/article/details/108867317","categories":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/tags/vue/"}]},{"title":"mysql基础","slug":"mysql/mysql","date":"2021-08-30T16:00:00.000Z","updated":"2023-07-17T13:41:27.804Z","comments":true,"path":"2021/08/31/mysql/mysql/","link":"","permalink":"https://yichenfirst.github.io/2021/08/31/mysql/mysql/","excerpt":"","text":"1、Mysql1.1连接数据库命令行连接 12mysql -uroot -pchen --连接数据库show databases; -- 查看所有数据库 1use demo; -- 切换数据库demo 1show tables; -- 查看数据库中所有表 1describe book; -- 查看表的详细信息 12create database test; -- 创建一个test数据库exit; --退出连接 2、操作数据库2.1 、操作数据库1.创建数据库 1create database if not exists test; -- 如果不存在，则创建 2.删除数据库 1drop database if exists test; -- 如果存在，删除数据库 3.使用数据库 12-- 如果表名或字段名是一个关键字，可以使用``use `user`; 4.查看数据库 1show batabases 2.2、数据库的列类型 数值 tinyint 十分小的数据 1个字节 smallint 较小的数据 2个字节 mediumint 中等大小的数据 3个字节 int 标准的整数 4个字节 big 较大的数据 8个字节 float 浮点数 4个字节 double 浮点数 8个字节 decimal 字符串形式浮点数 金融计算，一般使用 字符串 char 字符串固定大小 0~255 varchar 可变字符串 0~65535 tinytext 微型文本 2^8 -1 text 文本串 2^16 - 1 时间和日期 java.util.Date date YYYY-MM-DD 日期格式 time HH：mm：ss 时间格式 datetime YYYY-MM-DD HH：mm：ss timestamp 时间戳 1970.1.1到现在毫秒数 year 年份表示 null 没有值，未知 2.3 、数据库字段属性==unsigned：== 无符号整数 声明该列不能为复数 ==zerofill：== 0填充 不足的位数，使用0来填充 ==自增：== 通常用来设置唯一的主键，必须为整数类型 ==非空：== Null not Null 不赋值，报错 ==默认：== 设置默认值 2.4、创建数据库123456789101112131415161718192021222324-- 目标 : 创建一个school数据库-- 创建学生表(列,字段)-- 学号int 登录密码varchar(20) 姓名,性别varchar(2),出生日期(datatime),家庭住址,email-- 创建表之前 , 一定要先选择数据库CREATE TABLE IF NOT EXISTS `student` (`id` int(4) NOT NULL AUTO_INCREMENT COMMENT '学号',`name` varchar(30) NOT NULL DEFAULT '匿名' COMMENT '姓名',`pwd` varchar(20) NOT NULL DEFAULT '123456' COMMENT '密码',`sex` varchar(2) NOT NULL DEFAULT '男' COMMENT '性别',`birthday` datetime DEFAULT NULL COMMENT '生日',`address` varchar(100) DEFAULT NULL COMMENT '地址',`email` varchar(50) DEFAULT NULL COMMENT '邮箱',PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8-- 查看数据库的定义SHOW CREATE DATABASE school;-- 查看数据表的定义SHOW CREATE TABLE student;-- 显示表结构DESC student; -- 设置严格检查模式(不能容错了)SET sql_mode='STRICT_TRANS_TABLES'; 格式 12345678CREATE TABLE [IF NOT EXISTS] `表名`( `字段名`[属性][索引][注释], `字段名`[属性][索引][注释], ······· `字段名`[属性][索引][注释], `字段名`[属性][索引][注释], `字段名`[属性][索引][注释],)[表类型][字符集设置][注释] 2.5 、数据表的类型1234-- 数据库引擎INNODB 默认使用MYISAM 早年使用 INNODB MYISAM 事物支持 支持 不支持 数据行锁定 支持 不支持 外键约束 支持 不支持 全文索引 不支持 支持 表空间大小 较大，约2倍 较小 常规操作： MYISAM 节约空间 速度较快 INNODB 安全性高，事物处理， 多表多用户操作 在物理空间存在的位置 所有数据库文件都存在data目录文件下 本质是文件存储 MySQL引擎在物理文件上的区别 INNODB在数据库表只有一个*.frm文件，以及上级目录下的ibdata1文件 MYISAM对应文件 *.frm 表结构的定义文件 *.MYD 数据文件（data） *.MYI 索引文件（index） 设数据库表的字符集编码 1CHARSET=utf8 mysql默认编码不支持中文 或在my.ini中配置默认编码 1character-set-server=utf8 2.6 、修改和删除1.修改表 123456789101112-- 修改表明 alter table 旧表名 rename as 新表名alter table student rename as student1-- 增加表字段 alter table 表名 add 字段名 列属性alter table student1 add age int-- 修改表的字段alter table student1 modify age varchar(11) -- 修改约束alter table student1 change age age1 int -- 字段重命名-- 删除表字段alter table student1 drop age1 2.删除表 12-- 删除表（如果表存在再删除）drop table if exists student1 3、MySQL的数据管理3.1、外键 不推荐使用 数据库就是单纯的表，只用来存数据，只有行（数据）和列（字段） 可以通多程序实现外键 3.2、DML语言数据库意义：数据存储、数据管理 DML语言：数据操作语言 insert update delete 3.3、添加123456-- 插入语句-- insert into 表名([字段1， 字段2， 字段3]) values('值1','值2','值3')insert into student1(`name`) values('张三')-- 插入多个字段insert into student1(`name`) values('李四'),('王五') 注意事项： 字段可以省略，但是字段和值要一一对应 可以同时插入多条数据，values后面用‘，’隔开，即values( ),( ) 3.4、修改123456789-- 修改update `student1` set name='chen' where id = 1-- 不加条件，修改所有update `stuent1` set name = 'chen'-- 语法-- update 表名 set 字段名1=值2,[字段名1=值2......] where [条件] 条件：where字句，运算符，id等于某个值，大于某个值，在某个区间内修改······· where 后面的条件为True时，对全部数据进行修改 操作符 含义 范围 结果 = 等于 5 = 6 false &lt;&gt;或!= 不等于 5 &lt;&gt; 6 true &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 between … and … 在某个范围 [2, 5] and 和 5&gt;1 and 1&gt;2 flase or 或 5&gt;1 or 1&gt;2 true 3.5、删除 delete命令 1234567-- 语法-- delete from 表名 where [条件]delete from student1 where id = 1;-- 删除student1表delete from student1 truncate 命令 作用：完全清空一个数据库表，表结构和索引约束不会变 1truncate student1 delete和truncate区别 相同点： 都能删除数据，都不会删除表结构 不同： truncate 重新设置自增列，计数器会归零， truncate 不会影响事物 delete删除问题，重启数据库，现象 InnoDB 自增列会从1开始（存在内存中，断点即失） MyISAM 继续从上一个自增量开始（存在文件中，不会丢失） 4、DQL查询数据4.1 、DQL（Data Query LANGUAGE：数据查询语言） 所有查询都使用 数据库中最核心的语言，最重要的语句 使用频率最高 4.2、select语法 select语法 12345678910SELECT [ALL | DISTINCT]{* | table.* | [table.field1[as alias1][,table.field2[as alias2]][,...]]}FROM table_name [as table_alias] [left | right | inner join table_name2] -- 联合查询 [WHERE ...] -- 指定结果需满足的条件 [GROUP BY ...] -- 指定结果按照哪几个字段来分组 [HAVING] -- 过滤分组的记录必须满足的次要条件 [ORDER BY ...] -- 指定查询记录按一个或多个条件排序 [LIMIT {[offset,]row_count | row_countOFFSET offset}]; -- 指定查询的记录从哪条至哪条 注意 : [ ] 括号代表可选的 , { }括号代表必选得 4.3、指定查询字段1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950create database if not exists `school`;-- 创建一个school数据库use `school`;-- 创建学生表drop table if exists `student`;create table `student`( `studentno` int(4) not null comment '学号', `loginpwd` varchar(20) default null, `studentname` varchar(20) default null comment '学生姓名', `sex` tinyint(1) default null comment '性别，0或1', `gradeid` int(11) default null comment '年级编号', `phone` varchar(50) null comment '联系电话，允许为空', `address` varchar(255) null comment '地址，允许为空', `borndate` datetime default null comment '出生时间', `email` varchar (50) null comment '邮箱账号允许为空', `identitycard` varchar(18) default null comment '身份证号', primary key (`studentno`), unique key `identitycard`(`identitycard`), key `email` (`email`))engine=myisam default charset=utf8;-- 创建年级表drop table if exists `grade`;create table `grade`( `gradeid` int(11) not null auto_increment comment '年级编号', `gradename` varchar(50) not null comment '年级名称', primary key (`gradeid`)) engine=innodb auto_increment = 6 default charset = utf8;-- 创建科目表drop table if exists `subject`;create table `subject`( `subjectno`int(11) not null auto_increment comment '课程编号', `subjectname` varchar(50) default null comment '课程名称', `classhour` int(4) default null comment '学时', `gradeid` int(4) default null comment '年级编号', primary key (`subjectno`))engine = innodb auto_increment = 19 default charset = utf8;-- 创建成绩表drop table if exists `result`;create table `result`( `studentno` int(4) not null comment '学号', `subjectno` int(4) not null comment '课程编号', `examdate` datetime not null comment '考试日期', `studentresult` int (4) not null comment '考试成绩', key `subjectno` (`subjectno`))engine = innodb default charset = utf8; 1234567891011-- 查询全部学生select * from student-- 查询指定字段select studentno,studentname from student-- 别名select studentno as 学号, studentname as 学生姓名 from student as s-- 函数select concat('姓名：', studentname) as 新名字 from student 去重 distinct 作用： 去除select查询出来结果中重复的数据 1select distinct studentno from result 数据库的列（表达式) 123select version() --查询系统版本（函数）select 1+5*8 -- 用来计算（表达式）select @@auto_increment_increment -- 查询自增的步长（变量） 数据库中的表达式：文本值，列，Null，函数，计算表达式，系统变量 4.4 、where条件字句作用：检索数据中符合条件的值 逻辑运算符 搜索条件可由一个或多个逻辑表达式组成 ，结果一般为真或假。 运算法 语法 描述 and &amp;&amp; a and b a&amp;&amp;b 逻辑与，两个都为真，结果为真 or || a or b a || b 逻辑或，一个为真，结果为真 not ! not a !a 逻辑非，真为假，假为真 1234567891011-- 查询分数在95到100之间的学生select studentno,studentresult from result where studentresult &gt;95 and studentresult &lt;= 100select studentno,studentresult from result where studentresult between 95 and 100-- 查询1号学生之外的同学成绩select studentno,studentresult from result where not studentno = 1select studentno,studentresult from result where studentno != 1 模糊查询：比较运算符 运算符 语法 描述 is null a is null 如果操作符为null，则结果为真 is not null a is not null 如果操作符不为null，则结果为真 between a between b and c 若a在b和c之间，则结果为真 like a like b SQL匹配，如果a匹配b，则结果为真 in a in (a1,a2,a3,…….) 若a等于a1,a2,a3中的某一个，则结果为真 12345678910111213141516171819202122232425-- 查询姓刘的同学的学号及姓名-- like结合使用的通配符 : % (代表0到任意个字符) _ (一个字符)SELECT studentno,studentname FROM studentWHERE studentname LIKE '刘%';-- 查询姓刘的同学,后面只有一个字的SELECT studentno,studentname FROM studentWHERE studentname LIKE '刘_';-- 查询姓刘的同学,后面只有两个字的SELECT studentno,studentname FROM studentWHERE studentname LIKE '刘__';-- 查询姓名中含有 文 字的SELECT studentno,studentname FROM studentWHERE studentname LIKE '%文%';-- 查询学号为1000,1001,1002的学生姓名SELECT studentno,studentname FROM studentWHERE studentno IN (1000,1001,1002);-- 查询电子邮件没有填写的同学-- 不能直接写=NULL , 这是代表错误的 , 用 is nullSELECT studentname FROM studentWHERE email IS NULL; 4.5、联表查询 join 操作 描述 Inner join 如果表中有至少一个匹配，则返回行 left join 即使右表没有匹配，也从左表中返回所有的行 right join 即使左表没有匹配，也从右表中返回所有的行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/*连接查询 如需要多张数据表的数据进行查询,则可通过连接运算符实现多个查询内连接 inner join 查询两个表中的结果集中的交集外连接 outer join 左外连接 left join (以左表作为基准,右边表来一一匹配,匹配不上的,返回左表的记录,右表以NULL填充) 右外连接 right join (以右表作为基准,左边表来一一匹配,匹配不上的,返回右表的记录,左表以NULL填充) 等值连接和非等值连接自连接*/-- 内连接SELECT s.studentno,studentname,subjectno,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentno-- 右连接(也可实现)SELECT s.studentno,studentname,subjectno,StudentResultFROM student sRIGHT JOIN result rON r.studentno = s.studentno-- 左连接 (查询了所有同学,不考试的也会查出来)SELECT s.studentno,studentname,subjectno,StudentResultFROM student sLEFT JOIN result rON r.studentno = s.studentno-- 查一下缺考的同学(左连接应用场景)SELECT s.studentno,studentname,subjectno,StudentResultFROM student sLEFT JOIN result rON r.studentno = s.studentnoWHERE StudentResult IS NULL-- 思考题:查询参加了考试的同学信息(学号,学生姓名,科目名,分数)SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON sub.subjectno = r.subjectno 自连接 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*自连接 数据表与自身进行连接需求:从一个包含栏目ID , 栏目名称和父栏目ID的表中 查询父栏目名称和其他子栏目名称*/-- 创建一个表CREATE TABLE `category` (`categoryid` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主题id',`pid` INT(10) NOT NULL COMMENT '父id',`categoryName` VARCHAR(50) NOT NULL COMMENT '主题名字',PRIMARY KEY (`categoryid`)) ENGINE=INNODB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8-- 插入数据INSERT INTO `category` (`categoryid`, `pid`, `categoryName`)VALUES('2','1','信息技术'),('3','1','软件开发'),('4','3','数据库'),('5','1','美术设计'),('6','3','web开发'),('7','5','ps技术'),('8','2','办公信息');-- 编写SQL语句,将栏目的父子关系呈现出来 (父栏目名称,子栏目名称)-- 核心思想:把一张表看成两张一模一样的表,然后将这两张表连接查询(自连接)SELECT a.categoryName AS '父栏目',b.categoryName AS '子栏目'FROM category AS a,category AS bWHERE a.`categoryid`=b.`pid`-- 思考题:查询参加了考试的同学信息(学号,学生姓名,科目名,分数)SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON sub.subjectno = r.subjectno-- 查询学员及所属的年级(学号,学生姓名,年级名)SELECT studentno AS 学号,studentname AS 学生姓名,gradename AS 年级名称FROM student sINNER JOIN grade gON s.`GradeId` = g.`GradeID`-- 查询科目及所属的年级(科目名称,年级名称)SELECT subjectname AS 科目名称,gradename AS 年级名称FROM SUBJECT subINNER JOIN grade gON sub.gradeid = g.gradeid-- 查询 操作系统 的所有考试结果(学号 学生姓名 科目名称 成绩)SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON r.subjectno = sub.subjectnoWHERE subjectname='操作系统' 4.6、分页和排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*============== 排序 ================语法 : ORDER BY ORDER BY 语句用于根据指定的列对结果集进行排序。 ORDER BY 语句默认按照ASC升序对记录进行排序。 如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。 */-- 查询 数据结构 的所有考试结果(学号 学生姓名 科目名称 成绩)-- 按成绩降序排序SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON r.subjectno = sub.subjectnoWHERE subjectname='数据结构'ORDER BY StudentResult DESC/*============== 分页 ================语法 : SELECT * FROM table LIMIT [offset,] rows | rows OFFSET offset好处 : (用户体验,网络传输,查询压力)推导: 第一页 : limit 0,5 第二页 : limit 5,5 第三页 : limit 10,5 ...... 第N页 : limit (pageNo-1)*pageSzie,pageSzie [pageNo:页码,pageSize:单页面显示条数] */-- 每页显示5条数据SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON r.subjectno = sub.subjectnoWHERE subjectname='数据结构'ORDER BY StudentResult DESC , studentnoLIMIT 0,5-- 查询 JAVA第一学年 课程成绩前10名并且分数大于80的学生信息(学号,姓名,课程名,分数)SELECT s.studentno,studentname,subjectname,StudentResultFROM student sINNER JOIN result rON r.studentno = s.studentnoINNER JOIN `subject` subON r.subjectno = sub.subjectnoWHERE subjectname='数据结构' and studentresult&gt;99ORDER BY StudentResult DESCLIMIT 0,10 4.7、子查询where（这个值是计算出来的） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/*============== 子查询 ================什么是子查询? 在查询语句中的WHERE条件子句中,又嵌套了另一个查询语句 嵌套查询可由多个子查询组成,求解的方式是由里及外; 子查询返回的结果一般都是集合,故而建议使用IN关键字;*/-- 查询 数据结构 的所有考试结果(学号,科目编号,成绩),并且成绩降序排列-- 方法一:使用连接查询SELECT studentno,r.subjectno,StudentResultFROM result rINNER JOIN `subject` subON r.`SubjectNo`=sub.`SubjectNo`WHERE subjectname = '数据结构'ORDER BY studentresult DESC;-- 方法二:使用子查询(执行顺序:由里及外)SELECT studentno,subjectno,StudentResultFROM resultWHERE subjectno=( SELECT subjectno FROM `subject` WHERE subjectname = '数据结构')ORDER BY studentresult DESC;-- 查询课程为 高等数学 且分数不小于80分的学生的学号和姓名-- 方法一:使用连接查询SELECT s.studentno,studentnameFROM student sINNER JOIN result rON s.`StudentNo` = r.`StudentNo`INNER JOIN `subject` subON sub.`SubjectNo` = r.`SubjectNo`WHERE subjectname = '高等数学' AND StudentResult&gt;=80-- 方法二:使用连接查询+子查询-- 分数不小于80分的学生的学号和姓名SELECT r.studentno,studentname FROM student sINNER JOIN result r ON s.`StudentNo`=r.`StudentNo`WHERE StudentResult&gt;=80-- 在上面SQL基础上,添加需求:课程为 高等数学SELECT r.studentno,studentname FROM student sINNER JOIN result r ON s.`StudentNo`=r.`StudentNo`WHERE StudentResult&gt;=80 AND subjectno=( SELECT subjectno FROM `subject` WHERE subjectname = '高等数学')-- 方法三:使用子查询-- 分步写简单sql语句,然后将其嵌套起来SELECT studentno,studentname FROM student WHERE studentno IN( SELECT studentno FROM result WHERE StudentResult&gt;=80 AND subjectno=( SELECT subjectno FROM `subject` WHERE subjectname = '高等数学' )) 5、MySQL函数5.1、常用函数数据函数 12345SELECT ABS(-8); /*绝对值*/SELECT CEILING(9.4); /*向上取整*/SELECT FLOOR(9.4); /*向下取整*/SELECT RAND(); /*随机数,返回一个0-1之间的随机数*/SELECT SIGN(0); /*符号函数: 负数返回-1,正数返回1,0返回0*/ 字符串函数 1234567891011121314SELECT CHAR_LENGTH('狂神说坚持就能成功'); /*返回字符串包含的字符数*/SELECT CONCAT('我','爱','程序'); /*合并字符串,参数可以有多个*/SELECT INSERT('我爱编程helloworld',1,2,'超级热爱'); /*替换字符串,从某个位置开始替换某个长度*/SELECT LOWER('KuangShen'); /*小写*/SELECT UPPER('KuangShen'); /*大写*/SELECT LEFT('hello,world',5); /*从左边截取*/SELECT RIGHT('hello,world',5); /*从右边截取*/SELECT REPLACE('狂神说坚持就能成功','坚持','努力'); /*替换字符串*/SELECT SUBSTR('狂神说坚持就能成功',4,6); /*截取字符串,开始和长度*/SELECT REVERSE('狂神说坚持就能成功'); /*反转-- 查询姓周的同学,改成邹SELECT REPLACE(studentname,'周','邹') AS 新名字FROM student WHERE studentname LIKE '周%'; 日期和时间函数 12345678910111213SELECT CURRENT_DATE(); /*获取当前日期*/SELECT CURDATE(); /*获取当前日期*/SELECT NOW(); /*获取当前日期和时间*/SELECT LOCALTIME(); /*获取当前日期和时间*/SELECT SYSDATE(); /*获取当前日期和时间*/ -- 获取年月日,时分秒SELECT YEAR(NOW());SELECT MONTH(NOW());SELECT DAY(NOW());SELECT HOUR(NOW());SELECT MINUTE(NOW());SELECT SECOND(NOW()); 系统信息函数 12SELECT VERSION(); /*版本*/SELECT USER(); /*用户*/ 5.2、聚合函数 函数名称 描述 count() 返回满足Select条件的记录总和数，如 select count(*) 【不建议使用 *，效率低】 sum() 返回数字字段或表达式列作统计，返回一列的总和。 max() 可以为数值字段，字符字段或表达式列作统计，返回最大的值 min() 可以为数值字段，字符字段或表达式列作统计，返回最小的值 avg() 通常为数值字段或表达列作统计，返回一列的平均值 123456789101112131415161718192021222324-- 聚合函数 /*COUNT:非空的*/ SELECT COUNT(studentname) FROM student; SELECT COUNT(*) FROM student; SELECT COUNT(1) FROM student; /*推荐*/ -- 从含义上讲，count(1) 与 count(*) 都表示对全部数据行的查询。 -- count(字段) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即不统计字段为null 的记录。 -- count(*) 包括了所有的列，相当于行数，在统计结果的时候，包含字段为null 的记录； -- count(1) 用1代表代码行，在统计结果的时候，包含字段为null 的记录 。 /* 很多人认为count(1)执行的效率会比count(*)高，原因是count(*)会存在全表扫描，而count(1)可以针对一个字段进行查询。其实不然，count(1)和count(*)都会对全表进行扫描，统计所有记录的条数，包括那些为null的记录，因此，它们的效率可以说是相差无几。而count(字段)则与前两者不同，它会统计该字段不为null的记录条数。 下面它们之间的一些对比： 1）在表没有主键时，count(1)比count(*)快 2）有主键时，主键作为计算条件，count(主键)效率最高； 3）若表格只有一个字段，则count(*)效率较高。 */ SELECT SUM(StudentResult) AS 总和 FROM result; SELECT AVG(StudentResult) AS 平均分 FROM result; SELECT MAX(StudentResult) AS 最高分 FROM result; SELECT MIN(StudentResult) AS 最低分 FROM result; 12345678910111213141516-- 查询不同课程的平均分,最高分,最低分-- 前提:根据不同的课程进行分组SELECT subjectname,AVG(studentresult) AS 平均分,MAX(StudentResult) AS 最高分,MIN(StudentResult) AS 最低分FROM result AS rINNER JOIN `subject` AS sON r.subjectno = s.subjectnoGROUP BY r.subjectnoHAVING 平均分&gt;80;/*where写在group by前面.要是放在分组后面的筛选要使用HAVING..因为having是从前面筛选的字段再筛选，而where是从数据表中的&gt;字段直接进行的筛选的*/ 5.3、数据库级别的MD5加密一、MD5简介 MD5即Message-Digest Algorithm 5（信息-摘要算法5），用于确保信息传输完整一致。是计算机广泛使用的杂凑算法之一（又译摘要算法、哈希算法），主流编程语言普遍已有MD5实现。将数据（如汉字）运算为另一固定长度值，是杂凑算法的基础原理，MD5的前身有MD2、MD3和MD4。 二、实现数据加密 123456CREATE TABLE `testmd5` ( `id` INT(4) NOT NULL, `name` VARCHAR(20) NOT NULL, `pwd` VARCHAR(50) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8 插入一些数据 1INSERT INTO testmd5 VALUES(1,'chen','123456'),(2,'root','456789') 如果我们要对pwd这一列数据进行加密，语法是： 1update testmd5 set pwd = md5(pwd); 如果单独对某个用户(如kuangshen)的密码加密： 12INSERT INTO testmd5 VALUES(3,'kuangshen2','123456')update testmd5 set pwd = md5(pwd) where name = 'kuangshen2'; 插入新的数据自动加密 1INSERT INTO testmd5 VALUES(4,'kuangshen3',md5('123456')); 查询登录用户信息（md5对比使用，查看用户输入加密后的密码进行比对） 1SELECT * FROM testmd5 WHERE `name`='kuangshen' AND pwd=MD5('123456'); 6、事务 什么是事物 要么都成功，要么都失败 1、SQL执行 A给B转账 A 1000 —-&gt;200 B 200 2、SQL执行 B收到A的钱 A 800 —&gt; B 400 将一组SQL放在一个批次中执行 事务原则：ACID原则 原子性，一致性，隔离性，持久性 （脏读，幻读…….） 原子性（Atomicity） 要么都成功，要么都失败 一致性（Consistency） 事物前后数据完整性要保持一致，1000 持久性（Durability） 事物一旦提交则不可逆，被持久化到数据库中 隔离性（Isolation） 事物的隔离性是多个用户并发访问数据库时，数据库为每个用户开启的事务不能被其他事务干扰。 隔离导致的问题 脏读 一个事物读取了另一个事务未提交的数据 不可重复读 在一个事物内读取表中某一行数据，多次读取结果不同 虚读（幻读） 一个事物内读取到了别的事物插入的数据，导致前后读取不一致 基本语法 123456789101112131415161718192021222324-- 使用set语句来改变自动提交模式SET autocommit = 0; /*关闭*/SET autocommit = 1; /*开启*/-- 注意:--- 1.MySQL中默认是自动提交--- 2.使用事务时应先关闭自动提交-- 开始一个事务,标记事务的起始点START TRANSACTION -- 提交一个事务给数据库COMMIT-- 将事务回滚,数据回到本次事务的初始状态ROLLBACK-- 还原MySQL数据库的自动提交SET autocommit =1;-- 保存点SAVEPOINT 保存点名称 -- 设置一个事务保存点ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点RELEASE SAVEPOINT 保存点名称 -- 删除保存点 测试 123456789101112131415161718CREATE TABLE `account` (`id` INT(11) NOT NULL AUTO_INCREMENT,`name` VARCHAR(32) NOT NULL,`cash` DECIMAL(9,2) NOT NULL,PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8INSERT INTO account (`name`,`cash`)VALUES('A',2000.00),('B',10000.00)-- 转账实现SET autocommit = 0; -- 关闭自动提交START TRANSACTION; -- 开始一个事务,标记事务的起始点UPDATE account SET cash=cash-500 WHERE `name`='A';UPDATE account SET cash=cash+500 WHERE `name`='B';COMMIT; -- 提交事务rollback;SET autocommit = 1; -- 恢复自动提交 7、索引 MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。 提取句子主干，就可以得到索引的本质：索引是数据结构。 7.1、索引的分类 主键索引（PRIMARY KEY） 唯一的标识，主键不可重复 唯一索引（UNIQUE KEY） 避免出现重复的列出现，唯一索引可以有多个 常规索引（KEY/INDEX） 默认，index/key关键字设置 全文索引（FullText） 快速定位数据 1234567891011121314-- 索引的使用-- 1、创建表示给字段添加索引-- 2、创建完毕后，增加索引-- 显示所有索引信息show index from student-- 增加一个全文索引，（索引名）列名alter table school.student add fulltext index 'studentName'(studentName);-- explain分析sql执行情况explain select * from student -- 非全文索引explain select * from student where match(studentName) against('刘') 7.2、测试索引创建表 123456789101112CREATE TABLE `app_user` (`id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,`name` varchar(50) DEFAULT '' COMMENT '用户昵称',`email` varchar(50) NOT NULL COMMENT '用户邮箱',`phone` varchar(20) DEFAULT '' COMMENT '手机号',`gender` tinyint(4) unsigned DEFAULT '0' COMMENT '性别（0:男；1：女）',`password` varchar(100) NOT NULL COMMENT '密码',`age` tinyint(4) DEFAULT '0' COMMENT '年龄',`create_time` datetime DEFAULT CURRENT_TIMESTAMP,`update_time` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='app用户表' 批量插入数据：100w 12345678910111213141516171819-- 创建函数DROP FUNCTION IF EXISTS mock_data;DELIMITER $$CREATE FUNCTION mock_data()RETURNS INTBEGINDECLARE num INT DEFAULT 1000000;DECLARE i INT DEFAULT 0;WHILE i &lt; num DO INSERT INTO app_user(`name`, `email`, `phone`, `gender`, `password`, `age`) VALUES(CONCAT('用户', i), '24736743@qq.com', CONCAT('18', FLOOR(RAND()*(999999999-100000000)+100000000)),FLOOR(RAND()*2),UUID(), FLOOR(RAND()*100)); SET i = i + 1;END WHILE;RETURN i;END;SELECT mock_data(); -- 执行函数-- 报错1418set global log_bin_trust_function_creators=TRUE; 索引效率 123-- 无索引SELECT * FROM app_user WHERE name = '用户9999'; -- 查看耗时 0.875sexplain SELECT * FROM app_user WHERE name = '用户9999'; -- 查询了99万多条数据 创建索引 1CREATE INDEX idx_app_user_name ON app_user(name); -- 耗时13s 123-- 有索引SELECT * FROM app_user WHERE name = '用户9999'; -- 查看耗时 0.015sexplain SELECT * FROM app_user WHERE name = '用户9999'; -- 查询了1条数据 7.3、索引原则 索引不是越多越好 不要对经常变动的数据加索引 小数据量的表建议不要加索引 索引一般应加在查找条件的字段","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"}]},{"title":"vue脚手架与vuex","slug":"前端/vue脚手架与vuex","date":"2021-07-31T16:00:00.000Z","updated":"2023-07-17T13:41:27.784Z","comments":true,"path":"2021/08/01/前端/vue脚手架与vuex/","link":"","permalink":"https://yichenfirst.github.io/2021/08/01/%E5%89%8D%E7%AB%AF/vue%E8%84%9A%E6%89%8B%E6%9E%B6%E4%B8%8Evuex/","excerpt":"","text":"Vue CLI 脚手架1、什么是CLI命令行界面（英语：command-line interface，缩写：CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CUI） 2 、什么是Vue CLI &lt;====&gt; (maven 项目构建工具)Vue CLI 是一个基于 Vue.js 进行快速开发的完整系统。 使用Vue 脚手架之后我们开发的页面将是一个完整系统(项目)。 前端系统 3 、Vue CLI优势 通过 vue-cli 搭建交互式的项目脚手架。 通过 @vue/cli + @vue/cli-service-global 快速开始零配置原型开发 一个运行时依赖 (@vue/cli-service)，该依赖： 可升级; 基于 webpack 构建，并带有合理的默认配置； webpack 前端打包工具 index.html vue组件 用户组件 学生组件 ….. 路由 dist目录 可以通过项目内的配置文件进行配置； cli 项目配置文件 添加 可以通过插件进行扩展。 cli 项目里 一个丰富的官方插件集合，集成了前端生态中最好的工具。 webpack打包工具===&gt;dist目录 nodejs 服务器(tomcat java) 热部署插件 npm包 一套完全图形化的创建和管理 Vue.js 项目的用户界面 4、 Vue CLI安装1、环境准备12345678910111213141516171819202122232425262728293031323334353637383940# 1.下载nodejs http://nodejs.cn/download/ windows系统: .msi 安装包(exe)指定安装位置 .zip(压缩包)直接解压缩指定目录 mac os 系统: .pkg 安装包格式自动配置环境变量 .tar.gz(压缩包)解压缩安装到指定名# 2.配置nodejs环境变量 1.windows系统: 计算上右键属性----&gt; 高级属性 ----&gt;环境变量 添加如下配置: NODE_HOME= nodejs安装目录 PATH = xxxx;%NODE_HOME% 2.macos 系统 推荐使用.pkg安装直接配置node环境# 3.验证nodejs环境是否成功 node -v # 4.npm介绍 node package mangager nodejs包管理工具 前端主流技术 npm 进行统一管理 maven 管理java后端依赖 远程仓库(中心仓库) 阿里云镜像 npm 管理前端系统依赖 远程仓库(中心仓库) 配置淘宝镜像# 5.配置淘宝镜像 npm config set registry https://registry.npm.taobao.org npm config get registry# 6.配置npm下载依赖位置 windows: npm config set cache \"D:\\nodereps\\npm-cache\" npm config set prefix \"D:\\nodereps\\npm_global\" mac os: npm config set cache \"/Users/chenyannan/dev/nodereps\" npm config set prefix \"/Users/chenyannan/dev/nodereps\"# 7.验证nodejs环境配置 npm config ls ; userconfig /Users/chenyannan/.npmrc cache = \"/Users/chenyannan/dev/nodereps\" prefix = \"/Users/chenyannan/dev/nodereps\" registry = \"https://registry.npm.taobao.org/\" 2、安装脚手架12345678910# 0.卸载脚手架 npm uninstall -g @vue/cli //卸载3.x版本脚手架 npm uninstall -g vue-cli //卸载2.x版本脚手架# 1.Vue Cli官方网站 https://cli.vuejs.org/zh/guide/# 2.安装vue Cli npm install -g vue-cli 3、第一个vue脚手架项目 Vue-cli目录结构 123456789101112131415161718192021222324252627282930313233# 1.创建vue脚手架第一个项目 vue init webpack 项目名# 2.创建第一个项目 hello -------------&gt;项目名 -build -------------&gt;用来使用webpack打包使用build依赖 构建一些依赖文件 -config -------------&gt;用来做整个项目配置目录 主要用来对 开发 测试 环境进行配置 -node_modules ------&gt;用来管理项目中使用依赖 -src ------&gt;用来书写vue的源代码[重点] +assets ------&gt;用来存放静态资源 [重点] components ------&gt;用来书写Vue组件 [重点] router ------&gt;用来配置项目中路由[重点] App.vue ------&gt;项目中根组件[重点] main.js ------&gt;项目中主入口[重点] -static ------&gt;其它静态 -.babelrc ------&gt; 将es6语法转为es5运行 -.editorconfig ------&gt; 项目编辑配置 -.gitignore ------&gt; git版本控制忽略文件 -.postcssrc.js ------&gt; 源码相关js -index.html ------&gt; 项目主页 -package.json ------&gt; 类似与pom.xml 依赖管理 jquery 不建议手动修改 -package-lock.json ----&gt; 对package.json加锁 -README.md ----&gt; 项目说明文件# 3.如何运行在项目的根目录中执行 npm run dev 运行前端系统# 4.如何访问项目 http://localhost:8080 # 5.Vue Cli中项目开发方式 注意: 一切皆组件 一个组件中 js代码 html代码 css样式 1. VueCli开发方式是在项目中开发一个一个组件对应一个业务功能模块,日后可以将多个组件组合到一起形成一个前端系统 2. 日后在使用vue Cli进行开发时不再书写html,编写的是一个个组件(组件后缀.vue结尾的文件),日后打包时vue cli会将组件编译成运行的html文件 src 目录 src目录是项目的源码目录，所有代码都会写在这里！ main.js 项目的入口文件，我们知道所有的程序都会有一个入口 12345678910111213141516// The Vue build version to load with the `import` command// (runtime-only or standalone) has been set in webpack.base.conf with an alias.import Vue from 'vue' // 引入VUE核心库import App from './App' // 引入一个当前目录下的名字为App.vue的组件import router from './router'Vue.config.productionTip = false // 是否要在生产环境当中给予提示功能。/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: '&lt;App/&gt;'}) import Vue from 'vue'：ES6 写法，会被转换成 require(“vue”); （require 是 NodeJS 提供的模块加载器）其实就相当于 &lt;script src = \"vue\"&gt;&lt;/script&gt; import App from './App' ：意思同上，但是指定了查找路径，./ 为当前目录 Vue.config.productionTip = false：关闭浏览器控制台关于环境的相关提示 new Vue({})：实例化 Vue el: '#app'：查找 index.html 中 id 为 app 的元素 template: '&lt;App/&gt;'：模板，会将 index.html 中替换为 components: { App } ：引入组件，使用的是 import App from ‘./App’ 定义的 App 组件; App.vue 12345678910111213141516171819202122232425&lt;template&gt; &lt;div id=\"app\"&gt; &lt;img src=\"./assets/logo.png\"&gt; &lt;h1&gt;自定义脚手架&lt;/h1&gt; &lt;!--展示路由--&gt; &lt;router-view/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'App'}&lt;/script&gt;&lt;style&gt;#app { font-family: 'Avenir', Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;}&lt;/style&gt; template：HTML 代码模板，会替换 &lt; App /&gt; 中的内容 export default{…}：导出 NodeJS 对象，作用是可以通过 import 关键字导入 name: ‘App’：定义组件的名称 components: { HelloWorld }：定义子组件 在hello,Vue中,关于 &lt; style scoped&gt; 的说明：CSS 样式仅在当前组件有效，声明了样式的作用域,是当前的界面私有的! 不加代表全局有效。 index.js 配置路由 12345678910111213141516import Vue from 'vue'import Router from 'vue-router'import HelloWorld from '@/components/HelloWorld'Vue.use(Router) //注册路由 new Vue({router,})export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld } ]}) HelloWorld.vue 123456789101112131415161718192021222324252627282930313233343536&lt;!-- 一个组件代表一个业务功能：html标签 js代码 css代码 --&gt;&lt;template&gt; &lt;div class=\"hello\"&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'HelloWorld', data () { return { msg: '欢迎Vue' } }}&lt;/script&gt;&lt;!-- Add \"scoped\" attribute to limit CSS to this component only --&gt;&lt;style scoped&gt;h1, h2 { font-weight: normal;}ul { list-style-type: none; padding: 0;}li { display: inline-block; margin: 0 10px;}a { color: #42b983;}&lt;/style&gt; 4、如何开发Vue脚手架注意:在Vue cli 中一切皆组件 1、我们在HelloWorld.vue的同级目录下新建两个Student和User组件。 2、Student.vue和User.vue组件 Student.vue组件 1234567891011121314151617&lt;template&gt; &lt;div class=\"hello\"&gt; &lt;h1&gt;hello world {{msg}} &lt;/h1&gt; &lt;h3&gt;我是学生管理的主页&lt;/h3&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'Student', data () { return { msg: '学生管理' } }}&lt;/script&gt; User.vue组件 1234567891011121314151617&lt;template&gt; &lt;div class=\"hello\"&gt; &lt;h1&gt;hello world {{msg}} &lt;/h1&gt; &lt;h3&gt;我是用户管理的主页&lt;/h3&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'HelloWorld', data () { return { msg: '用户管理' } }}&lt;/script&gt; 3、在App.vue组件中引入这两个组件的链接，进行路由切换 1234567891011121314&lt;template&gt; &lt;div id=\"app\"&gt; &lt;img src=\"./assets/logo.png\"&gt; &lt;h1&gt;自定义脚手架&lt;/h1&gt; &lt;router-link to=\"/\"&gt;主页&lt;/router-link&gt; &lt;router-link to=\"/user\"&gt;用户管理&lt;/router-link&gt; &lt;router-link to=\"/student\"&gt;学生管理&lt;/router-link&gt; &lt;!--展示路由--&gt; &lt;router-view/&gt; &lt;/div&gt;&lt;/template&gt; 4、启动项目，测试一下 在脚手架中使用axios1、安装axios123456789# 1.安装axios npm install axios --save# 2.配置main.js中引入axios import axios from 'axios'; Vue.prototype.$http=axios;# 3.使用axios 在需要发送异步请求的位置:this.$http.get(\"url\").then((res)=&gt;{}) this.$http.post(\"url\").then((res)=&gt;{}) Vue Cli脚手架项目部署123456# 1.在项目根目录中执行如下命令: npm run build 注意:vue脚手架打包的项目必须在服务器上运行不能直接双击运行# 2.打包之后当前项目中变化 在打包之后项目中出现dist目录,dist目录就是vue脚手架项目生产目录或者说是直接部署目录 webstorm开发vue cli项目1、简介WebStorm 是JetBrains公司旗下一款[JavaScript ](https://baike.baidu.com/item/JavaScript /321142)开发工具。已经被广大中国JS开发者誉为“Web前端开发神器”、“最强大的HTML5编辑器”、“最智能的JavaScript[ IDE](https://baike.baidu.com/item/ IDE/8232086)”等。与[IntelliJ IDEA](https://baike.baidu.com/item/IntelliJ IDEA/9548353)同源，继承了[IntelliJ IDEA](https://baike.baidu.com/item/IntelliJ IDEA/9548353)强大的JS部分的功能。 2、安装官网地址：http://www.jetbrains.com/webstorm/ 选择好安装路径 3、使用webstormwebstorm和idea同属于一家开发公司，因此使用方式基本一样。 使用webstorm初始化项目 1、我们新建一个webstromcodes项目。 2、打开Terminal终端，输入vue init webpack vue_day5 ，进行vue-day5项目的创建。 3、接下来的步骤和使用Vue Cli脚手架开发一样。 4、显示如下页面，项目初始化成功。 可以看下目录结构，和使用Vue CLI脚手架开发目录结构一致。 5、在Terminal终端输入cd vue_day5，进入当前项目根目录，项目目录中包含package.json才是根目录，可以输入dir查看当前目录结构。 6、最后我们输入npm run dev启动项目，出现如下界面： 说明项目成功运行！！！输入http://localhost:8080 ，打开浏览器查看一下，出现默认界面。 main.js 项目中主入口 12345678910111213141516//引入vue.jsimport Vue from 'vue'//引入app.vue 组件import App from './App'//引入router目录中 /index.jsimport router from './router'Vue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', //指定vue实例作用范围 router, components: { App }, template: '&lt;App/&gt;'}) 4、开发vue cli项目(一)1、Student组件 12345678910111213141516template&gt; &lt;div class=\"hello\"&gt; &lt;h1&gt;{{ msg }}&lt;/h1&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'HelloWorld', data () { return { msg: '欢迎进入我的系统！！！' } }} 2、User组件 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;template&gt; &lt;div&gt; &lt;h1&gt;用户管理&lt;/h1&gt; &lt;table border=\"1\" width=\"100%\"&gt; &lt;tr&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;工资&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr v-for=\"(user,index) in users\" :key=\"user.id\" &gt; &lt;td&gt;{{user.id}}&lt;/td&gt; &lt;td&gt;{{user.name}}&lt;/td&gt; &lt;td&gt;{{user.age }}&lt;/td&gt; &lt;td&gt;{{user.des}}&lt;/td&gt; &lt;td&gt;&lt;a href=\"\"&gt;删除&lt;/a&gt; &lt;a href=\"\"&gt;修改&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: \"User\", data(){ return { users:[],//定义数组 }; }, methods:{ }, computed:{ }, created() { //初始化阶段发送请求查询所有用户信息 this.$http.get(\"http://localhost:8990/users\").then(res=&gt;{ console.log(res.data); this.users = res.data; }); }} 3、App组件 12345678910111213141516171819202122&lt;!--组件html标签--&gt;&lt;template&gt; &lt;div id=\"app\"&gt; &lt;!--展示vue脚手架logo--&gt; &lt;img src=\"./assets/1.jpg\" class=\"aa\"&gt; &lt;br&gt; &lt;!--主页--&gt; &lt;router-link to=\"/\"&gt;主页&lt;/router-link&gt; &lt;!--用户管理--&gt; &lt;router-link to=\"/user\"&gt;用户管理&lt;/router-link&gt; &lt;!--学生管理--&gt; &lt;router-link to=\"/student\"&gt;学生管理&lt;/router-link&gt; &lt;!--展示路由组件--&gt; &lt;router-view/&gt; &lt;/div&gt;&lt;/template&gt;&lt;!--组件中js代码--&gt;&lt;script&gt;export default { name: 'App'}&lt;/script&gt; 4、index.js 1234567891011121314151617181920212223242526import Vue from 'vue'import Router from 'vue-router'//@ 代表src目录import HelloWorld from '@/components/HelloWorld'import User from '@/components/User'import Student from \"../components/Student\";Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld }, { path: '/user', name: 'user', component: User },{ path: '/student', name: 'student', component: Student } ]}) 启动项目测试一下： 成功查询到数据！！！ 5、开发vue cli项目(二)VueX 状态管理1、简介&amp;安装123456789# 1.简介- Vuex 是一个专为 Vue.js 应用程序开发的状态管理模式。它采用集中式存储管理应用的所有组件的状态，并以相应的规则保证状态以一种可预测的方式发生变化# 2.安装vuex- npm install vuex --save# 3.创建vue cli中创建store文件夹# 4.在stroe中创建index.js文件 12345678910import Vue from 'vue'import Vuex from 'vuex'//1.安装vuexVue.use(Vuex);//2.创建store对象const store = new Vuex.Store({ });//3.暴露store对象export default store; 1# 5.在main.js中引入stroe并注册到vue实例 1234567891011121314import Vue from 'vue'import App from './App'import router from './router'import store from \"./stroe\";//引入storeVue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: '&lt;App/&gt;', store,//注册状态}) 2、state属性123456789101112# 1.state属性- 作用: 用来全局定义一些共享的数据状态# 2.语法 const store = new Vuex.Store({ state:{ counter:0,//定义共享状态 }, }# 3.使用 {{$store.state.counter}} ===&gt; {{this.$store.state.counter}} 3、mutations 属性123456789101112131415161718192021222324252627282930313233343536# 1.mutations 属性- 作用: 用来定义对共享的数据修改的一系列函数# 2.语法 const store = new Vuex.Store({ state:{ counter:0,//定义共享状态 }, mutations:{ //增加 increment(state){ state.counter++ }, //减小 decrement(state){ state.counter-- } } });# 3.使用 this.$store.commit('decrement'); this.$store.commit('increment');# 4.mutations传递参数- a.定义带有参数的函数 mutations:{ //addCount 参数1:state 对象 参数2:自定义参数 addCount(state,counter){ console.log(counter); return state.counter += counter ; } }- b.调用时传递参数 this.$store.commit('addCount',11); 4、getters 属性12345678910111213141516171819202122232425262728# 1.getters 属性- 官方: 允许我们在 store 中定义“getter”（可以认为是 store 的计算属性）。就像计算属性一样，getter 的返回值会根据 它的依赖被缓存起来，且只有当它的依赖值发生了改变才会被重新计算。- 作用: 用来定义对共享的数据的计算相关的一系列函数 相当于 computed 属性 会对结果进行缓存# 2.语法 getters:{ //平方 mathSqrts(state){ console.log(\"--------\"); return state.counter*state.counter; }, //乘以一个数字 mathSqrtsNumber(state,getters){ return getters.mathSqrts*3; }, //传递参数 mathSqrtsNumbers(state,getters){ return function (number){ return number; } } }# 3.使用- 1.{{$store.getters.mathSqrts}}- 2.{{$store.getters.mathSqrtsNumber}}- 3.{{$store.getters.mathSqrtsNumbers(3)}} 5、练习index.js 123456789101112131415161718192021222324252627282930313233343536import Vue from 'vue' //vue.jsimport Vuex from 'vuex' //vuex.js//1. vue中使用vuexVue.use(Vuex);//2. 创建store对象export default new Vuex.Store({ //state: 用来定义所有组件共享的数据 state:{ counter:0, }, //mutations: 用来定义对共享数据的修改方法 mutations:{ incrCounter(state){ state.counter++; }, decrCounter(state){ state.counter--; }, incrCounterCount(state,count){ state.counter += count; }, }, //getters: 相当于store共享数据计算属性，该计算属性中方法只会执行一次，只有计算属性值发生变化才会重新计算，相当于之前的computed属性 getters:{ incrCount(state){ //二次渲染计算 return state.counter + 10; }, match(state,getters){ return getters.incrCount * 2; } }}); User.vue 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;template&gt; &lt;div&gt; &lt;h1&gt;用户管理--{{this.$store.state.counter}}--{{this.$store.getters.incrCount}}--{{this.$store.getters.match}}&lt;/h1&gt; &lt;input type=\"button\" value=\"点我修改共享数据counter++\" @click=\"changeStoreCounter\"&gt; &lt;input type=\"button\" value=\"点我修改共享数据counter--\" @click=\"decrStoreCounter\"&gt; &lt;input type=\"button\" value=\"点我修改共享数据counter+5\" @click=\"changeStoreCounterFive\"&gt; &lt;router-link to=\"/user/add\" style=\"float: left\"&gt;添加用户信息&lt;/router-link&gt; &lt;table border=\"1\" width=\"100%\"&gt; &lt;tr&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;工资&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr v-for=\"(user,index) in users\" :key=\"user.id\" &gt; &lt;td&gt;{{user.id}}&lt;/td&gt; &lt;td&gt;{{user.name}}&lt;/td&gt; &lt;td&gt;{{user.age }}&lt;/td&gt; &lt;td&gt;{{user.des}}&lt;/td&gt; &lt;td&gt;&lt;a href=\"javascript:;\" @click=\"delUser(user.id)\"&gt;删除&lt;/a&gt; &lt;router-link to=\"/user/add?id=user.id&amp;name\" &gt;修改&lt;/router-link&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;hr&gt; &lt;!--显示子组件路由--&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: \"User\", data(){ return { users:[],//定义数组 }; }, methods:{ findAll(){ this.$http.get(\"http://localhost:8990/users\").then(res=&gt;{ console.log(res.data); this.users = res.data; }); }, delUser(id){ //友情提醒删除 if(window.confirm(\"您确定要删除这条记录吗？\")){ this.$http.delete(\"http://localhost:8990/delete/\"+id).then(res=&gt;{ alert(\"用户信息删除成功!\"); this.findAll(); ////调用查询所有 }).catch(err=&gt;{ alert(\"用户信息删除失败!\"); }); } }, changeStoreCounter(){ //修改共享数据counter++ this.$store.commit('incrCounter'); }, decrStoreCounter(){ //修改共享数据counter-- this.$store.commit('decrCounter'); }, changeStoreCounterFive(){ //修改共享数据counter+5 this.$store.commit('incrCounterCount',5); } }, computed:{ }, created() { //初始化阶段发送请求查询所有用户信息 this.findAll(); }, //user /user/add /user/edit beforeRouteUpdate(to,from,next){ //当前组件中路由更新时自动触发的函数 this.findAll(); next(); //放行后路由后续操作 },}&lt;/script&gt;&lt;style scoped&gt; th{ background-color: beige; }&lt;/style&gt;","categories":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/tags/vue/"}]},{"title":"vue笔记","slug":"前端/vue笔记","date":"2021-07-29T16:00:00.000Z","updated":"2023-07-17T13:41:27.814Z","comments":true,"path":"2021/07/30/前端/vue笔记/","link":"","permalink":"https://yichenfirst.github.io/2021/07/30/%E5%89%8D%E7%AB%AF/vue%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Vue实战1、Vue 引言 渐进式 JavaScript 框架 –摘自官网 123456789101112131415161718192021# 渐进式 1. 易用 html css javascript 2. 高效 开发前端页面 非常高效 3. 灵活 开发灵活 多样性# 总结 Vue 是一个javascript 框架 js 简化页面js操作 bootstrap 是一个css框架 封装css# 后端服务端开发人员: 页面标签 dom jquery js document.getElementById(\"xxx\") Vue 渐进式javascript框架: 让我们通过操作很少的DOM,甚至不需要操作页面中任何DOM元素,就很容易的完成数据和视图绑定 ====&gt; 双向绑定 MVVM 注意：日后在使用vue过程中页面不要再引入Jquery框架 html css---&gt;javascript(document.getElementById()...) ----&gt;jquery($(\"#xx\")) ---&gt; angularjs ---&gt;vue(前后端分离架构核心) vue 前端系统 &lt;-----JSON-----&gt; 后台系统springcloud# Vue 作者 尤雨溪 国内的 2、Vue入门2.1、下载Vuejs1234567//开发版本: &lt;!-- 开发环境版本，包含了有帮助的命令行警告 --&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt;//生产版本: &lt;!-- 生产环境版本，优化了尺寸和速度 --&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue\"&gt;&lt;/script&gt; 2.2、Vue第一个入门应用1、vue第一个入门应用 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; {{msg}} &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;span&gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;span&gt; &lt;span&gt;{{msg}}&lt;/span&gt; &lt;/span&gt; &lt;/span&gt; &lt;h3&gt;用户名：{{username}}&lt;/h3&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app= new Vue({ el:\"#app\", //element：元素 作用：用来指定vue实例作用范围 日后在el指定的作用范围内可以直接使用{{属性名}}获取data中的属性 data:{ //data：数据 作用：用来给vue实例对象绑定一系列数据 msg: \"Vue欢迎您！\", username: \"小陈!!\", } });&lt;/script&gt; 2、vue实例中定义对象，数组等相关数据 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;h2&gt;{{age}}&lt;/h2&gt; &lt;h2&gt;姓名：{{user.name}} 描述：{{user.des}}&lt;/h2&gt; &lt;h2&gt;{{schools[0]}}-{{schools[1]}}-{{schools[2]}}-{{schools[3]}}&lt;/h2&gt; &lt;h2&gt;姓名：{{users[0].name}} 年龄：{{users[0].age}} 生日：{{users[0].bir}}&lt;/h2&gt; &lt;h2&gt;姓名：{{users[1].name}} 年龄：{{users[1].age}} 生日：{{users[1].bir}}&lt;/h2&gt; &lt;h2&gt;姓名：{{users[2].name}} 年龄：{{users[2].age}} 生日：{{users[2].bir}}&lt;/h2&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"百知欢迎您!!!\", age:\"23\", user:{name:\"小陈\",des:\"他在百知,百知等你！！\"}, //定义对象 schools:[\"河南校区\",\"北京校区\",\"天津校区\",\"山西校区\"], //定义一个数组 users:[ {name:\"小王\",age:23,bir:\"2012-12-01\"}, {name:\"小李\",age:24,bir:\"2013-12-01\"}, {name:\"小赵\",age:25,bir:\"2014-12-01\"}, ] } });&lt;/script&gt; 3、使用获取data数据时,使用表达式 运算符等相关操作 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!--{{属性名}}：使用这种方式获取数据时，可以进行相关的运算（算数，逻辑），调用获取值类型相关js方法--&gt; &lt;h2&gt;{{msg + '您好'}}&lt;/h2&gt; &lt;h2&gt;{{msg == 'hello vue'}}&lt;/h2&gt; &lt;h2&gt;{{msg.toUpperCase()}}&lt;/h2&gt; &lt;h2&gt;{{age + 1}}&lt;/h2&gt; &lt;h2&gt;{{age == 23}}&lt;/h2&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", age:23, } });&lt;/script&gt; 4、使用vue时el属性指定 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" class=\"aa\"&gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue作用范围 // 书写格式：使用css选择器 id选择器 html选择器 类选择器 推荐使用id选择器 id选择器具有唯一性 // 注意事项：不要将el指向body或html标签 Do not mount Vue to &lt;html&gt; or &lt;body&gt; - mount to normal elements instead. data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", } });&lt;/script&gt; 5、总结 12345# 总结: 1.vue实例(对象)中el属性: 代表Vue的作用范围 日后在Vue的作用范围内都可以使用Vue的语法 2.vue实例(对象)中data属性: 用来给Vue实例绑定一些相关数据, 绑定的数据可以通过{{变量名}}在Vue作用范围内取出 3.在使用{{}}进行获取data中数据时,可以在{{}}中书写表达式,运算符,调用相关方法,以及逻辑运算等 4.el属性中可以书写任意的CSS选择器[jquery选择器],但是在使用Vue开发是推荐使用 id选择器 注意: el属性值不能指定body或html标签 3、v-text和v-html3.1、v-text v-text:用来获取data中数据将数据以文本的形式渲染到指定标签内部 类似于javascript 中 innerText 123456789101112131415&lt;div id=\"app\" class=\"aa\"&gt; &lt;span &gt;{{ message }}&lt;/span&gt; &lt;span v-text=\"message\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;!--引入vue.js--&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt; &lt;script&gt; const app = new Vue({ el:\"#app\", data:{ message:\"百知欢迎您\" } }) &lt;/script&gt; 1234# 总结 1.{{}}(插值表达式)和v-text获取数据的区别在于 a.使用v-text取值会将标签中原有的数据覆盖 使用插值表达式的形式不会覆盖标签原有的数据 b.使用v-text可以避免在网络环境较差的情况下出现插值闪烁 3.2、v-html v-html:用来获取data中数据将数据中含有的html标签先解析在渲染到指定标签的内部 类似于javascript中 innerHTML 12345678910111213141516171819&lt;div id=\"app\" class=\"aa\"&gt; &lt;span&gt;{{message}}&lt;/span&gt; &lt;br&gt; &lt;span v-text=\"message\"&gt;&lt;/span&gt; &lt;br&gt; &lt;span v-html=\"message\"&gt;xxxxxx&lt;/span&gt; &lt;/div&gt; &lt;!--引入vue.js--&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt; &lt;script&gt; const app = new Vue({ el:\"#app\", data:{ message:\"&lt;a href=''&gt;百知欢迎您&lt;/a&gt;\" } }) &lt;/script&gt; 3.3、v-text和v-html对比代码： 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" class=\"aa\"&gt; &lt;h2&gt;{{msg}} 您好&lt;/h2&gt; &lt;!--vue提供两个指令： v-text v-html 都可以直接根据属性名获取data数据渲染到指定标签内--&gt; &lt;!--v-text--&gt; &lt;h2&gt;&lt;span v-text=\"msg\"&gt;&lt;/span&gt; 您好&lt;/h2&gt; &lt;!-- v-text: {{}} 取值区别： 1.使用{{}}取值不会将标签原始数据覆盖 使用v-text获取数据会将标签中原始内容覆盖 2.v-text获取数据时不会出现插值闪烁 {{属性名}} ===&gt; 插值表达式：容易出现插值闪烁 插值闪烁：当网络不好条件情况下使用{{}}方式获取数据 --&gt; &lt;h2 v-html=\"msg\"&gt;&lt;/h2&gt; &lt;!-- 共同点：都可以直接根据data中数据名，将数据渲染到标签内部 v-text: v-text将获取数据直接以文本形式渲染到标签内部 innerText v-html: v-html将获取数据中含有html标签解析之后渲染到对应标签内部 innerHtml --&gt; &lt;h1 &gt;{{content}}&lt;/h1&gt; &lt;h1 v-text=\"content\"&gt;&lt;/h1&gt; &lt;h1 v-html=\"content\"&gt;&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", content : \"欢迎来到&lt;a href='http://www.baidu.com'&gt;百度&lt;/a&gt;\", } });&lt;/script&gt; 运行结果： 4、vue中事件绑定(v-on)4.1、绑定事件基本语法1、vue事件绑定（一） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;h2 v-text=\"msg\"&gt;&lt;/h2&gt; &lt;h2 v-html=\"msg\"&gt;&lt;/h2&gt; &lt;!-- js 事件三要素 1.事件源：发生事件源头称之为事件源，一般指的是html标签 2.事件：发生特定动作 onclick单击 dbclick 双击 onkeyup ...... 3.监听器：事件处理器程序 事件处理函数 function(){} vue 事件：v-on 1.在vue中给对应标签绑定事件可以通过vue提供v-on指令进行事件绑定 ==&gt; v-on:事件名 2.在vue中事件处理函数统一声明在vue实例中methods属性 --&gt; &lt;!--给button按钮绑定多个事件--&gt; &lt;input type=\"button\" value=\"点我\" v-on:click=\"aaa\" v-on:mouseover=\"bbb\" v-on:mouseout=\"ccc\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", }, methods:{ //用来给当前vue实例对象，声明一系列函数 aaa: function () { alert(\"aaa\"); }, bbb: function () { console.log(\"mouse over\"); }, ccc: function () { console.log(\"mouse out\"); } } });&lt;/script&gt; 2、vue事件绑定（二） 给一个按钮绑定点击+1事件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;h1&gt;{{age}}&lt;/h1&gt; &lt;!-- vue事件： 1. 使用v-on：事件名 2. 函数名统一定义在vue实例中 methods 属性中 --&gt; &lt;input type=\"button\" value=\"点我给年龄+1\" v-on:click=\"incrmentAge\"&gt; &lt;input type=\"button\" value=\"点我给年龄-1\" v-on:click=\"decrmentAge\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", age:23, }, methods:{ //用来给当前vue实例对象，声明一系列函数 incrmentAge:function () { //this对象代表当前vue实例对象 console.log(this); console.log(this.age) if(this.age &gt;= 120) return; this.age ++; //vue实例中data数据age发生变化 }, decrmentAge:function () { //定义函数 if(this.age &lt; 2) return ; this.age --; } } });&lt;/script&gt; 3、总结 ​ 事件源: 发生事件dom元素 事件: 发生特定的动作 click.... 监听器 发生特定动作之后的事件处理程序 通常是js中函数 在vue中绑定事件是通过v-on指令来完成的 v-on:事件名 如 v-on:click 在v-on:事件名的赋值语句中是当前事件触发调用的函数名 在vue中事件的函数统一定义在Vue实例的methods属性中 在vue定义的事件中this指的就是当前的Vue实例,日后可以在事件中通过使用this获取Vue实例中相关数据 调用methods中相关方法 4.2、Vue中事件的简化语法12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;h1&gt;{{age}}&lt;/h1&gt; &lt;!-- vue事件绑定 v-on：事件名 简化写法===&gt; @事件名 --&gt; &lt;input type=\"button\" value=\"点我改变年龄\" @click=\"changeAge\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", age:23, }, methods:{ changeAge:function () { this.age ++ ; } } });&lt;/script&gt; 12# 总结: 1.日后在vue中绑定事件时可以通过@符号形式 简化 v-on 的事件绑定 4.3、Vue事件函数两种写法1234567891011121314151617&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", age:23, }, methods:{ /*changeAge:function () { //定义事件 简化写法 this.age ++ ; }*/ changeAge(){ //es6语法 ecmascript 6版本 this.age ++ ; } } });&lt;/script&gt; 1234# 总结: 1.在Vue中事件定义存在两种写法 一种是 函数名:function(){} 一种是 函数名(){} 推荐 4.4、Vue事件参数传递我们还可以给vue事件中传递参数。 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;h1&gt;{{age}}&lt;/h1&gt; &lt;!-- vue事件绑定 v-on：事件名 简化 @事件名=\"事件函数名(参数......)\" --&gt; &lt;!--多个参数使用','隔开--&gt; &lt;input type=\"button\" value=\"点我改变年龄的值\" @click=\"changeAge(10,'xiaohei')\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", age:23, }, methods:{ changeAge(number,name){ //定义事件 console.log(number); console.log(name); this.age += number ; } } });&lt;/script&gt; 12# 总结: 1.在使用事件时,可以直接在事件调用处给事件进行参数传递,在事件定义处通过定义对应变量接收传递的参数 5、v-show v-if v-bind5.1、v-show、v-if使用 v-show: 用来控制页面中某个标签元素是否展示 v-if: 用来控制页面元素是否展示 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!-- v-if、v-show ： 作用：都是用来控制页面中标签是否展示和隐藏 使用：标签：v-if=\"true|false\" v-show=\"true|false\" 区别： v-show: 底层在控制页面标签是否展示时底层使用的是css 中 display 属性来标签展示和隐藏 推荐使用：v-show 数据量比较大 控制显示状态切换频繁 v-if : 底层在控制页面标签是否展示时底层是直接操作dom元素，通过对dom元素删除和添加来控制标签的展示和隐藏 --&gt; &lt;!--v-show--&gt; &lt;h1 v-show=\"isShow\"&gt;{{content}}&lt;/h1&gt; &lt;!--v-if--&gt; &lt;h1 v-if=\"isShow\"&gt;{{content}}&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", content: \"vue学习\", isShow:true }, methods:{ //用来给vue实例定义事件处理函数 } });&lt;/script&gt; 总结： 在使用v-show时可以直接书写boolean值控制元素展示,也可以通过变量控制标签展示和隐藏。 在v-show中可以通过boolean表达式控制标签的展示和隐藏。 v-if、v-show ： 作用：都是用来控制页面中标签是否展示和隐藏 使用：标签：v-if=\"true|false\" ,v-show=\"true|false\" 区别： v-show: 底层在控制页面标签是否展示时底层使用的是css 中 display 属性来标签展示和隐藏 。推荐使用：v-show 在数据量比较大和控制显示状态切换频繁时。 v-if : 底层在控制页面标签是否展示时底层是直接操作dom元素，通过对dom元素删除和添加来控制标签的展示和隐藏。 5.2、v-show、v-if小案例1、v-show、v-if显示隐藏案例(一) 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;!-- v-show v-if --&gt; &lt;h2 v-show=\"isShow\"&gt;{{msg}}&lt;/h2&gt; &lt;!--绑定事件 单击事件 @click--&gt; &lt;input type=\"button\" value=\"显示\" @click=\"show\"&gt; &lt;input type=\"button\" value=\"隐藏\" @click=\"hidden\"&gt; &lt;input type=\"button\" value=\"切换显示状态\" @click=\"changeState\"&gt; &lt;input type=\"button\" value=\"切换显示状态,另一种写法\" @click=\"isShow=!isShow\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", isShow:true }, methods:{ //用来给vue实例定义事件处理函数 show(){ //用来显示 this.isShow = true; }, hidden(){//用来隐藏 this.isShow = false; }, changeState(){ //切换显示状态 this.isShow = !this.isShow; }, } });&lt;/script&gt; 2、v-show、v-if显示隐藏案例(二) 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;!-- v-show v-if --&gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!--图片添加鼠标移入事件--&gt; &lt;img width=\"200\" v-show=\"isShow\" @mouseover=\"hide\" style=\"border: 5px red solid\" src=\"https://img0.baidu.com/it/u=384452397,1089369801&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\" alt=\"这是图片\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", isShow:true }, methods:{ //用来给vue实例定义事件处理函数 hide(){ this.isShow = false; } } });&lt;/script&gt; 5.3、v-bind v-bind: 用来绑定标签的属性从而通过vue动态修改标签的属性 1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!-- v-bind: 绑定 作用：用来将html标签中相关属性绑定到vue实例中，日后通过对vue实例中数据修改，影响到对应标签中属性变化 语法：v-bind:属性名 --&gt; &lt;img v-bind:width=\"width\" v-bind:src=\"imgSrc\" v-bind:alt=\"tip\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", imgSrc:\"https://img0.baidu.com/it/u=384452397,1089369801&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\", width:200, tip:\"这是图片\" }, methods:{ //用来给vue实例定义事件处理函数 } });&lt;/script&gt; 5.4、v-bind 简化写法 ​ vue为了方便我们日后绑定标签的属性提供了对属性绑定的简化写法如 v-bind:属性名 简化之后 :属性名 12345&lt;!-- v-bind: 绑定 作用：用来将html标签中相关属性绑定到vue实例中，日后通过对vue实例中数据修改，影响到对应标签中属性变化 语法：v-bind:属性名 =====&gt; 简化写法：属性名--&gt;&lt;img :width=\"width\" :src=\"imgSrc\" :alt=\"tip\"&gt; 扩展v-bind使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt; &lt;style&gt; .aa{ border: 5px red solid; } .bb{ border: 5px darkorange solid; } &lt;/style&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!-- v-bind: 绑定 作用：用来将html标签中相关属性绑定到vue实例中，日后通过对vue实例中数据修改，影响到对应标签中属性变化 语法：v-bind:属性名 =====&gt; 简化写法：属性名 --&gt; &lt;img :width=\"width\" :src=\"imgSrc\" :alt=\"tip\" :class=\"isClass?'aa':'bb'\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", imgSrc:\"https://img0.baidu.com/it/u=384452397,1089369801&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\", width:200, tip:\"这是图片\", isClass:true, //ture 显示red false 显示orange }, methods:{ //用来给vue实例定义事件处理函数 } });&lt;/script&gt; 5.5、v-bind案例实现鼠标移入和移出对图片和边框的切换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt; &lt;style&gt; .aa{ border: 5px red solid; } .bb{ border: 5px darkorange solid; } &lt;/style&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!--属性绑定--&gt; &lt;img width=\"200\" @mou=\"change\" :src=\"src\" :class=\"cls\" @mouseover=\"change\" @mouseout=\"recover\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", src:\"https://img2.baidu.com/it/u=1077360284,2857506492&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\", cls:\"aa\" }, methods:{ //用来给vue实例定义事件处理函数 change(){ this.src = \"https://img1.baidu.com/it/u=3229045480,3780302107&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\"; this.cls = \"bb\"; }, recover(){ this.src = \"https://img2.baidu.com/it/u=1077360284,2857506492&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg\"; this.cls = \"aa\"; } } });&lt;/script&gt; 6、v-for的使用 v-for: 作用就是用来对对象进行遍历的(数组也是对象的一种) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!-- v-for : 作用：用来给vue实例中数据进行遍历 --&gt; &lt;h1&gt;遍历对象&lt;/h1&gt; &lt;h2 v-for=\"(value,key,index) in user\"&gt; index: {{index}} key:{{key}} value:{{value}} &lt;/h2&gt; &lt;h1&gt;遍历数组&lt;/h1&gt; &lt;h2 v-for=\"(school,index) in schools\"&gt; index:{{index}} schools:{{school}} &lt;/h2&gt; &lt;h1&gt;遍历数组中含有对象&lt;/h1&gt; &lt;h2 v-for=\"(user,index) in users\" :key = \"user.id\"&gt; index: {{index}} name:{{user.name}} age:{{user.age}} bir:{{user.bir}} &lt;/h2&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", user:{name:\"小王\",age:23,bir:\"2012-02-06\"}, schools:[\"北京\",\"重庆\",\"天津\"], users:[ {id:\"1\",name:\"小王\",age:23,bir:\"2012-02-06\"}, {id:\"2\",name:\"小李\",age:34,bir:\"2016-02-06\"}, {id:\"3\",name:\"小赵\",age:12,bir:\"2014-02-06\"}, ] }, methods:{ //用来给vue实例定义事件处理函数 } });&lt;/script&gt; 12# 总结 1.在使用v-for的时候一定要注意加入:key 用来给vue内部提供重用和排序的唯一key 7、v-model 双向绑定7.1、v-model v-model: 作用用来绑定标签元素的值与vue实例对象中data数据保持一致,从而实现双向的数据绑定机制 代码： 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body &gt; &lt;div id=\"app\" &gt; &lt;h2&gt;{{msg}}&lt;/h2&gt; &lt;!-- v-model : 作用：用来绑定from表单标签中的value属性交给vue实例进行管理 input select checxbox button ... --&gt; &lt;input type=\"text\" v-model=\"msg\"&gt; &lt;input type=\"button\" value=\"改变data数据\" @click=\"change\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue.js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //指定vue实例作用范围 data:{ //用来给vue实例绑定一系列数据 msg:\"hello vue\", }, methods:{ //用来给vue实例定义事件处理函数 change(){ this.msg = \"vue学习\" } } });&lt;/script&gt; 总结： 12345678910# 总结 1.使用v-model指令可以实现数据的双向绑定 2.所谓双向绑定 表单中数据变化导致vue实例data数据变化 vue实例中data数据的变化导致表单中数据变化 称之为双向绑定# MVVM架构 双向绑定机制 Model: 数据 Vue实例中绑定数据 VM: ViewModel 监听器 View: 页面 页面展示的数据 8、计算属性 计算属性：computed: vue官方提供一个计算属性 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!-- computed: vue官方提供一个计算属性 作用：在完成某种业务时，往往页面结果需要经过多次计算才能获取，computed属性就是用来完成页面结果多次计算 好处：在完成计算同时也会将本次计算结果进行缓存，如果数据没有发生变化，在页面中多次使用，计算方法仅执行一次 使用：{{ 属性名}} 属性名即方法名称 --&gt; &lt;table border=\"1\"&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;名称&lt;/th&gt; &lt;th&gt;价格&lt;/th&gt; &lt;th&gt;数量&lt;/th&gt; &lt;th&gt;小计&lt;/th&gt; &lt;/tr&gt; &lt;!--v-for--&gt; &lt;tr v-for=\"(item,index) in items\" :key=\"item.id\"&gt; &lt;td&gt;{{item.id}}&lt;/td&gt; &lt;td&gt;{{item.name}}&lt;/td&gt; &lt;td&gt;{{item.price}}&lt;/td&gt; &lt;td&gt;&lt;input type=\"button\" value=\"-\" @click=\"decrCount(index)\"&gt;{{item.count}}&lt;input type=\"button\" value=\"+\" @click=\"incrCount(index)\"&gt;&lt;/td&gt; &lt;td&gt;{{(item.price * item.count).toFixed(2)}}&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;!--使用methods方法完成计算业务： 缺点： 1.只要调用了一次计算方法，整个计算方法就会执行一次，如果在一个页面中多次使用到计算结果，可能会导致造成重复计算，导致页面加载性能变低 --&gt; &lt;h3&gt;总价格：{{totalPrice}}&lt;/h3&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue的js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //代表vue实例作用范围 data:{ //在vue实例中定义一系列数据 msg:\"购物车功能实现之methods方法实现总价格\", items:[ {id:1,name:\"苹果iphone12\",count:1,price:28.28}, {id:2,name:\"华为mate40 pro\",count:1,price:30.28}, ] }, methods:{ //在vue实例中定义相关函数 incrCount(index){ //数量增加的方法 console.log(this.items[index].count); this.items[index].count++; }, decrCount(index){ //数量减少的方法 console.log(this.items[index].count); if( this.items[index].count &gt;= 1){ this.items[index].count--; }else{ alert(\"不能在少了!\"); return ; } }, }, computed:{ //用来书写计算相关方法 计算属性 totalPrice(){ //计算方法 好处：只进行一次计算，多次使用时直接使用第一次计算之后缓存结果 var totalPrice = 0; for(var i = 0; i &lt; this.items.length; i++){ totalPrice += this.items[i].count * this.items[i].price; } return totalPrice.toFixed(2); } } });&lt;/script&gt; 总结： 作用：在完成某种业务时，往往页面结果需要经过多次计算才能获取，computed属性就是用来完成页面结果多次计算 好处：在完成计算同时也会将本次计算结果进行缓存，如果数据没有发生变化，在页面中多次使用，计算方法仅执行一次 使用： 属性名即方法名称 9、事件修饰符 修饰符: 用来和事件连用,用来决定事件触发条件或者是阻止事件的触发机制 123456# 1.常用的事件修饰符 .stop 停止 .prevent 阻止 .self 独自 .once 一次 9.1 stop事件修饰符 用来阻止事件冒泡 12345678&lt;h2&gt;stop事件修饰符&lt;/h2&gt;&lt;!--.stop 事件修饰符 作用：用来阻止事件的冒泡--&gt;&lt;div style=\"width: 200px;height: 200px;background: red\" @click=\"parent\"&gt; &lt;!--对孩子中单击事件进行修饰：不进行事件冒泡处理 .stop--&gt; &lt;div style=\"width: 100px;height: 100px;background: green\" @click.stop=\"child\"&gt;&lt;/div&gt;&lt;/div&gt; 9.2 prevent 事件修饰符 用来阻止标签的默认行为 12345&lt;h2&gt;prevent事件修饰符&lt;/h2&gt;&lt;!--默认行为：根据href连接自动跳转 .prevent 阻止事件默认行为--&gt;&lt;a href=\"http://www.baidu.com\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt;&lt;a href=\"javascript:void(0);\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt;&lt;a href=\"javascript:;\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt; 9.3 self 事件修饰符 用来针对于当前标签的事件触发 ===========&gt; 只触发自己标签的上特定动作的事件 只关心自己标签上触发的事件 不监听事件冒泡 12345678&lt;h2&gt;slef事件修饰符&lt;/h2&gt;&lt;!--self: 只监听自身标签触发的对应事件--&gt;&lt;div style=\"width: 200px;height: 200px;background: aqua\" @click.self=\"parent\"&gt; &lt;div style=\"width: 100px;height: 100px;background: green\" @click=\"child\"&gt;&lt;/div&gt; &lt;div style=\"width: 100px;height: 100px;background: brown\" @click=\"child\"&gt;&lt;/div&gt;&lt;/div&gt; 9.4 once 事件修饰符 once 一次作用: 就是让指定事件只触发一次 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!-- 事件修饰符 作用：用来和事件连用，用来决定事件触发条件和决定事件触发机制 .stop 停止事件冒泡 .prevent 阻止默认行为 .slef 只触发自身行为 .once 一次事件 注意：事件修饰符可以多个连用 --&gt; &lt;h2&gt;stop事件修饰符&lt;/h2&gt; &lt;!--.stop 事件修饰符 作用：用来阻止事件的冒泡--&gt; &lt;div style=\"width: 200px;height: 200px;background: red\" @click=\"parent\"&gt; &lt;!--对孩子中单击事件进行修饰：不进行事件冒泡处理 .stop--&gt; &lt;div style=\"width: 100px;height: 100px;background: green\" @click.stop.once=\"child\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;h2&gt;prevent事件修饰符&lt;/h2&gt; &lt;!--默认行为：根据href连接自动跳转 .prevent 阻止事件默认行为--&gt; &lt;a href=\"http://www.baidu.com\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt; &lt;a href=\"javascript:void(0);\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt; &lt;a href=\"javascript:;\" @click.prevent=\"search\"&gt;百度一下&lt;/a&gt; &lt;h2&gt;slef事件修饰符&lt;/h2&gt; &lt;!--self: 只监听自身标签触发的对应事件--&gt; &lt;div style=\"width: 200px;height: 200px;background: aqua\" @click.self=\"parent\"&gt; &lt;div style=\"width: 100px;height: 100px;background: green\" @click=\"child\"&gt;&lt;/div&gt; &lt;div style=\"width: 100px;height: 100px;background: brown\" @click=\"child\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;h2&gt;once事件修饰符&lt;/h2&gt; &lt;!--once事件修饰符：作用：只能让标签上的对应事件执行一次--&gt; &lt;input type=\"button\" value=\"点我\" @click.once=\"clickMe\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue的js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //代表vue实例作用范围 data:{ //在vue实例中定义一系列数据 msg:\"事件修饰符\", }, methods:{ //在vue实例中定义相关函数 parent(){ alert(\"parent div event\"); }, child(){ alert(\"child div event\"); }, search(){ alert(\"a click event\"); }, clickMe(){ alert(\"click me!!!\") } }, computed:{ //用来书写计算相关方法 计算属性 } });&lt;/script&gt; 10、按键修饰符 作用: 用来与键盘中按键事件绑定在一起,用来修饰特定的按键事件的修饰符 12345678910# 按键修饰符 .enter .tab .delete (捕获“删除”和“退格”键) .esc .space .up .down .left .right 10.1 enter 回车键 用来在触发回车按键之后触发的事件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{msg}}&lt;/h1&gt; &lt;!-- 按键修饰符：作用:用来和键盘上事件(keyup,keydown......)进行连用，用来修饰键盘上特定的按键来触发对应的事件 .enter .tab .delete (捕获“删除”和“退格”键) .esc .space .up .down .left .right --&gt; &lt;!--.enter 回车按键修饰符--&gt; &lt;input type=\"text\" v-model=\"msg\" @keyup.enter=\"test\"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入vue的js文件--&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el:\"#app\", //代表vue实例作用范围 data:{ //在vue实例中定义一系列数据 msg:\"按键修饰符\", }, methods:{ //在vue实例中定义相关函数 test(){ console.log(\"test\"); } }, computed:{ //用来书写计算相关方法 计算属性 } });&lt;/script&gt; 10.2 tab 键 用来捕获到tab键执行到当前标签是才会触发 1&lt;input type=\"text\" @keyup.tab=\"test\"&gt; 11、Axios 基本使用11.1、引言 Axios 是一个异步请求技术,核心作用就是用来在页面中发送异步请求,并获取对应数据在页面中渲染 页面局部更新技术 Ajax 11.2、Axios 第一个程序中文网站:https://www.kancloud.cn/yunye/axios/234845 安装: https://unpkg.com/axios/dist/axios.min.js 11.2.1、GET方式的请求后端代码： 123456789101112131415161718192021222324252627282930313233343536373839package com.xiao.controller;import com.xiao.entity.User;import org.springframework.web.bind.annotation.*;import javax.websocket.server.PathParam;import java.util.Date;@RestController //代表接口中返回的都是json格式数据@CrossOrigin //运行所有的请求 所有域访问 解决：跨域问题public class AdminController { //user接口 //rest接口 url/11/ @GetMapping(\"user/{id}\") public User FindUserById(@PathVariable(\"id\") Integer id){ System.out.println(\"id: \"+id); System.out.println(\"user...\"); return new User(id,\"小李\",23,new Date()); } //queryString接口 url?id=11 @GetMapping(\"user\") public User user(@RequestParam(\"id\") Integer id){ System.out.println(\"id: \"+id); System.out.println(\"user...\"); return new User(id,\"小陈\",23,new Date()); } //测试接口 @GetMapping(\"demo\") public String demo(){ System.out.println(\"demo...\"); return \"demo ok\"; }} 前端代码： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id = \"app\"&gt; &lt;h1&gt;axios的GET方式请求&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入axios异步请求核心js文件--&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;!--测试异步请求--&gt;&lt;script&gt; //发送axios 的GET方式请求 /*axios.get(\"http://localhost:8989/user?id=11\").then(function (res){ console.log(res.data); console.log(res.data.id); console.log(res.data.name); console.log(res.data.age); console.log(res.data.bir); });*/ //es6中简化写法function(){}简化写法java中lambada表达式 ()=&gt; axios.get(\"http://localhost:8989/user/11\").then((res)=&gt;{ console.log(res.data); console.log(res.data.id); console.log(res.data.name); console.log(res.data.age); console.log(res.data.bir); }); /*axios.get(\"http://localhost:8989/demo\").then(function (resonse){ //then 正确请求返回处理结果 console.log(resonse.data); }).catch(function (error){ //请求地址值出错的处理结果 console.log(error) }); //发送异步请求方式*/&lt;/script&gt; 11.2.2 POST方式请求后端代码： 12345678910//定义post接口@PostMapping(\"user\")public Map&lt;String,Object&gt; save(@RequestBody User user){ //@RequestBody 将json格式数据转换成java对象 System.out.println(\"user:\" + user); HashMap&lt;String,Object&gt; result = new HashMap&lt;&gt;(); result.put(\"success\",true); result.put(\"msg\",\"添加成功~~\"); return result;} 前端代码： 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id = \"app\"&gt; &lt;h1&gt;axios的POST方式请求&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--引入axios异步请求核心js文件--&gt;&lt;script src=\"js/axios.min.js\"&gt;&lt;/script&gt;&lt;!--测试异步请求--&gt;&lt;script&gt; //发送post方式请求 //参数1: url地址 参数2: 请求数据 axios.post(\"http://localhost:8989/user\",{ name:\"小李\", age:23, bir:\"2012-02-05\" }).then((res)=&gt;{ console.log(res.data); });&lt;/script&gt; 11.2.3 axios并发请求 并发请求: 将多个请求在同一时刻发送到后端服务接口,最后在集中处理每个请求的响应结果 12345678910111213//axios并发请求//定义demo请求function demo(){ return axios.get(\"http://localhost:8989/demo\");}//定义user请求function user(){ return axios.get(\"http://localhost:8989/user?id=11\");}axios.all([demo(), user()]).then(axios.spread((demoRes,useRes)=&gt;{ //并发请求 console.log(demoRes); console.log(useRes);})); 11.2.4 拦截器 12、Vue 生命周期 Vue 实例生命周期 ===&gt; java 对象生命周期(初始化阶段 运行阶段 销毁阶段) 生命周期钩子 ====&gt; 生命周期函数 Vue实例从创建到销毁过程中自动触发一系列函数 ====&gt; Vue生命周期函数(钩子) Vue生命周期总结 12345678910111213141516171819202122232425262728293031# - 1.初始化阶段 beforeCreate(){ //1.生命周期中第一个函数,该函数在执行时Vue实例仅仅完成了自身事件的绑定和生命周期函数的初始化工作,Vue实例中还没有 Data el methods相关属性 console.log(\"beforeCreate: \"+this.msg); }, created(){ //2.生命周期中第二个函数,该函数在执行时Vue实例已经初始化了data属性和methods中相关方法 console.log(\"created: \"+this.msg); }, beforeMount(){//3.生命周期中第三个函数,该函数在执行时Vue将El中指定作用范围作为模板编译 console.log(\"beforeMount: \"+document.getElementById(\"sp\").innerText); }, mounted(){//4.生命周期中第四个函数,该函数在执行过程中,已经将数据渲染到界面中并且已经更新页面 console.log(\"Mounted: \"+document.getElementById(\"sp\").innerText); }- 2.运行阶段 beforeUpdate(){//5.生命周期中第五个函数,该函数是data中数据发生变化时执行 这个事件执行时仅仅是Vue实例中data数据变化页面显示的依然是原始数据 console.log(\"beforeUpdate:\"+this.msg); console.log(\"beforeUpdate:\"+document.getElementById(\"sp\").innerText); }, updated(){ //6.生命周期中第六个函数,该函数执行时data中数据发生变化,页面中数据也发生了变化 页面中数据已经和data中数据一致 console.log(\"updated:\"+this.msg); console.log(\"updated:\"+document.getElementById(\"sp\").innerText); },- 3.销毁阶段 beforeDestory(){//7.生命周期第七个函数,该函数执行时,Vue中所有数据 methods componet 都没销毁 }, destoryed(){ //8.生命周期的第八个函数,该函数执行时,Vue实例彻底销毁 } 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;vue系列课程&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id = \"app\"&gt; &lt;h1 id=\"sp\"&gt;{{msg}}&lt;/h1&gt; &lt;input type=\"button\" value=\"修改数据\" @click=\"changeData\"&gt; &lt;!-- vue生命周期分为三个阶段： 1.初始化阶段 beforeCreate(){ //1.生命周期中第一个函数,该函数在执行时Vue实例仅仅完成了自身事件的绑定和生命周期函数的初始化工作,Vue实例中还没有 Data el methods相关属性 console.log(\"beforeCreate: \"+this.msg); }, created(){ //2.生命周期中第二个函数,该函数在执行时Vue实例已经初始化了data属性和methods中相关方法 console.log(\"created: \"+this.msg); }, beforeMount(){//3.生命周期中第三个函数,该函数在执行时Vue将El中指定作用范围作为模板编译 console.log(\"beforeMount: \"+document.getElementById(\"sp\").innerText); }, mounted(){ //4.生命周期中第四个函数,该函数在执行过程中,已经将数据渲染到界面中并且已经更新页面 console.log(\"Mounted: \"+document.getElementById(\"sp\").innerText); } 2.运行阶段 beforeUpdate(){//5.生命周期中第五个函数,该函数是data中数据发生变化时执行 这个事件执行时仅仅是Vue实例中data数据变化页面显示的依然是原始数据 console.log(\"beforeUpdate:\"+this.msg); console.log(\"beforeUpdate:\"+document.getElementById(\"sp\").innerText); }, updated(){ //6.生命周期中第六个函数,该函数执行时data中数据发生变化,页面中数据也发生了变化 页面中数据已经和data中数据一致 console.log(\"updated:\"+this.msg); console.log(\"updated:\"+document.getElementById(\"sp\").innerText); }, 3.销毁阶段 beforeDestory(){//7.生命周期第七个函数,该函数执行时,Vue中所有数据 methods componet 都没销毁 }, destoryed(){ //8.生命周期的第八个函数,该函数执行时,Vue实例彻底销毁 } --&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script src=\"js/vue.js\"&gt;&lt;/script&gt;&lt;script&gt; var app = new Vue({ el: \"#app\", data:{ msg:\"vue 生命周期\", }, methods:{ changeData(){ this.msg=\"vue 生命周期讲解\"; } }, computed:{}, beforeCreate(){ //1.生命周期中第一个函数,该函数在执行时Vue实例仅仅完成了自身事件的绑定和生命周期函数的初始化工作,Vue实例中还没有 Data el methods相关属性 console.log(\"beforeCreate: \"+this.msg); }, created(){ //2.生命周期中第二个函数,该函数在执行时Vue实例已经初始化了data属性和methods中相关方法 console.log(\"created: \"+this.msg); }, beforeMount(){//3.生命周期中第三个函数,该函数在执行时Vue将El中指定作用范围作为模板编译 console.log(\"beforeMount: \"+document.getElementById(\"sp\").innerText); }, mounted(){ //4.生命周期中第四个函数,该函数在执行过程中,已经将数据渲染到界面中并且已经更新页面 console.log(\"Mounted: \"+document.getElementById(\"sp\").innerText); }, beforeUpdate(){//5.生命周期中第五个函数,该函数是data中数据发生变化时执行 这个事件执行时仅仅是Vue实例中data数据变化页面显示的依然是原始数据 console.log(\"beforeUpdate:\"+this.msg); console.log(\"beforeUpdate:\"+document.getElementById(\"sp\").innerText); }, updated(){ //6.生命周期中第六个函数,该函数执行时data中数据发生变化,页面中数据也发生了变化 页面中数据已经和data中数据一致 console.log(\"updated:\"+this.msg); console.log(\"updated:\"+document.getElementById(\"sp\").innerText); }, beforeDestory(){//7.生命周期第七个函数,该函数执行时,Vue中所有数据 methods componet 都没销毁 }, destoryed(){ //8.生命周期的第八个函数,该函数执行时,Vue实例彻底销毁 } });&lt;/script&gt;","categories":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/tags/vue/"}]},{"title":"JavaScript","slug":"前端/JavaScript","date":"2021-06-07T16:00:00.000Z","updated":"2023-07-17T13:41:27.764Z","comments":true,"path":"2021/06/08/前端/JavaScript/","link":"","permalink":"https://yichenfirst.github.io/2021/06/08/%E5%89%8D%E7%AB%AF/JavaScript/","excerpt":"","text":"1、什么是JavaScriptJavaScript是一门世界上最流行的脚本语言 ECMAScript是JavaScript的一个标准 最新版本已经到es6，但是大部分浏览器只支持es5 2、快速入门引入JavaScript1、内部标签 123&lt;script&gt; //&lt;/script&gt; 2、外部引入 demo1.js 1&lt;script src=\"js/demo1.js\"&gt;&lt;/script&gt; 语法入门12345678910111213&lt;script&gt; // 1. 定义变量 变量类型 变量名 = 变量值 var score = 1; //条件控制 if (score &gt; 60 &amp;&amp; score &lt; 70){ alert(\"60-70\"); }else if(score &gt; 70 &amp;&amp; score &lt; 80){ alert(\"70-80\"); }else{ alert(\"other\") }&lt;/script&gt; 数据类型 Number js不区分小数和整数 12345123 //整数123.1 //浮点数1.123e3 //科学计数法-99 //复数NaN // not a Number 字符串 ‘abc’, “abc” 布尔值 true, false 逻辑运算 123&amp;&amp;||! 比较运算符 123=== 等于 类型不一样，但值一样，也会判断为true 如 1 “1”=== 绝对等于，类型一样，值也一样，结果为true 须知： NaN === NaN（false），这个与所有的数值都不相等，包括自己 只能通过isNaN(NaN)，来判断是否是NaN 浮点数问题 12console.log((1/3) === (1-2/3))console.log(Math.abs(1/3 - (1 - 2/3) &lt; 0.0000001)) 存在精度问题 null和undefined null 空 undefined 未定义 数组 数组中元素类型不需要相同 12345var arr = [1, 2, 3, 4, \"hello\", null]new Array(1, 2, 3, 4, \"hello\", null)console.log(arr[1])console.log(arr[100]) #数组越界，输出undefined 对象 对象是大括号，数组是中括号 12345var person = { name: \"chen\", age:3, tags:['java','js','c']} 严格检查模式12'use strict'; //严格检查模式，预防JavaScript的随意性导致产生的一些问题，必须放在第一行，es6语法i = 1 // 全局变量 3、数据类型字符串1、正常字符串使用，单引号，双引号包裹 2、转义字符 \\ 3、多行字符串，是用tab上面``包裹 123let str = `abc abc cbd` 4、模板字符串 1234let name = \"chen\"let msg = `你好，${name}`console.log(msg) 5、字符串长度 1str.length 6、字符串的可变性，不可变 7、大小写转换 12student.toUpperCase()student.toLowerCase() 8、indexOf() 9、substring 12student.substring(1) // 从第一个字符截取到最后一个字符student.substring(1,3) // [1，3) 数组Array可以包含任意类型 1var arr = [1, 2, 3, 4, 5] 1、长度 1arr.length 给arr.length复制，数组大小会发生变化，扩大数组，以undefined填充，缩小数组，截断 2、indexOf 通过元素获得下标索引（第一个） 1234var arr = [1, 2, 1, 2, 3, 4, 5, 6, \"1\", \"2\"]console.log(arr.indexOf(1))# 0 字符串“1”，和数字1不同 3、slice()截取Array的一部分，返回一个新的数组,类似substring 4、push、pop、unshift、shift 1234arr.push(\"a\") # 压入到尾部arr.pop() # 弹出尾部的一个元素arr.unshift(\"a\") # 压入到头部arr.shift() # 弹出头部的一个元素 5、排序 sort 123(8)&nbsp;[1, 2, 1, 2, 3, 4, 5, 6]arr.sort()(8)&nbsp;[1, 1, 2, 2, 3, 4, 5, 6] 6、元素反转 reverse 123(8)&nbsp;[1, 1, 2, 2, 3, 4, 5, 6]arr.reverse()(8)&nbsp;[6, 5, 4, 3, 2, 2, 1, 1] 7、concat() 数组拼接，返回一个新的数组，并不会修改数组 12345(8)&nbsp;[6, 5, 4, 3, 2, 2, 1, 1]arr.concat([\"q\",\"d\"])(10)&nbsp;[6, 5, 4, 3, 2, 2, 1, 1, \"q\", \"d\"]arr(8) [6, 5, 4, 3, 2, 2, 1, 1] 8、连接符join 打印拼接数组，使用特定的字符从连接 12arr.join(\"-\")\"6-5-4-3-2-2-1-1\" 对象若干键值对 12345var person = { name: \"chen\", age:3, tags:['java','js','c']} JavaScript中所有的键都是字符串，值是任意对象 1、对象复制 1234person.name = \"yichen\"\"yichen\"person.name\"yichen\" 2、使用一个不存在的对象，不会报错，undefined 12person.name2undefined 3、动态删减属性, delete 1234delete person.nametrueperson{age: 3, tags: Array(3)} 4、添加属性，直接给新的属性赋值 1234person.name = \"yichen\"\"yichen\"person{age: 3, tags: Array(3), name: \"yichen\"} 5、判断属性值是否在对象中 1234'age' in persontrue\"toString\" in persontrue 6、判断属性是否是对象自身拥有的，hasOwnProperty（） 123trueperson.hasOwnProperty(\"toString\")false 流程控制if else 12345678let age = 3;if(age&gt;3){ alert(\"haha\")}else if (age &lt;5){ alert(\"wua\")}else{ alert(\"wwwww\")} while 1234while(age&lt;100){ age += 1; console.log(age);} for 123for (let i = 0; i &lt; 100; i++) { console.log(i);} 数组循环 1234var arr = [1,2,3,4,5,6,6,7,7,81,9,9]for(var num in arr){ console.log(arr[num]);} Map12345&gt; var map = new Map([['tom', 100],['jack', 90],['haha', 80]]);&gt; var name = map.get('tom')&gt; console.log(name)&gt; map.set('admin', 123)&lt; Map(4)&nbsp;{\"tom\" =&gt; 100, \"jack\" =&gt; 90, \"haha\" =&gt; 80, \"admin\" =&gt; 123} Set无序不重复集合 123456var set = new Set([3, 1, 1, 1, 2])Set(3) {3, 1, 2}set.add(5)set.delete(1)set.has(2) # 集合中是否有该元素 iterator遍历数组 1234var arr = [1, 2, 3]for (var x of arr){ console.log(x)} 遍历map 12345678var map = new Map([ ['tom', 100], ['jack', 90], ['haha', 80]]);for (var x of map) { console.log(x)} 遍历set 1234var set = new Set([3, 1, 1, 1, 2])for (var x of map) { console.log(x)} 4、函数自定义函数绝对值函数 1234567function abs(x){ if(x &gt;= 0){ return x; }else{ return -x; }} 一旦执行到return，代表函数结束，返回结果 没有执行return， 函数执行完也会返回结果，结果就是undefined 定义方式二 1234567var abs = function(x){ if(x &gt;= 0){ return x; }else{ return -x; }} 参数问题：javascript可以传任意个参数，也可以不传递参数 手动抛出异常： 123456789var abs = function(x){ if (typeof(x) !== 'number') throw 'Not a Number'; if(x &gt;= 0){ return x; }else{ return -x; }} rest 以前： 12345if(arguments.length&gt;2){ for(var i = 2; i&lt;arguments.length;i++){ //.............. }} ES6引入新特性，获取除了已经定义的参数之外的所有参数 12345function aaa(a,b,...rest){ console.log(\"a=&gt;\"+a); console.log(\"b=&gt;\"+a); console.log(rest);} 变量的作用域 全局函数 123456//全局变量x = 1;function f(){ console.log(x);}console.log(x); 全局对象window 123var x = 'xxx';alert(x);alert(window.x);//默认所有全局变量绑定在window对象下 规范 由于所有全局变量都会绑定到window上，如果不同的js文件，使用了相同的全局变量，就会发生冲突。 12345678//唯一全局变量var app = {};//定义全局变量app.name = 'chen';app.add = function(a, b){ return a + b;} 把自己的代码全部放入做自己定义的唯一空间名字中，降低全局命名冲突。 局部作用域 let 123456function fun() { for (var i = 0; i &lt; 100; i++) { console.log(i); } console.log(i + 1); // ? 出了作用域还可以使用} ES6 let关键字，解决局部作用域冲突问题 123456function fun() { for (let i = 0; i &lt; 100; i++) { console.log(i); } console.log(i + 1); //Uncaught ReferenceError: i is not defined} 常量const 在ES6之前，定义常量：只有用全部大写字母命名的变量就是常量，建议不要修改 在ES6中引入了const 方法 定义方法 放在对象内部 123456789101112var person = { name:\"chen\", birth:1999, age:function(){ var now = new Date().getFullYear(); return now - this.birth; }}//属性person.name//方法person.age() this是无法指向的，默认指向调用它的那个对象的 12345678910111213function getAge(){ var now = new Date().getFullYear(); return now - this.birth;}var person = { name:\"chen\", birth:1999, age:getAge}person.nameperson.age() apply 在js中可以控制this的指向 1getAge.apply(person) 5、内部对象 标准对象 12345678910111213141516typeof 123\"number\"typeof \"123\"\"string\"typeof true\"boolean\"typeof NaN\"number\"typeof []\"object\"typeof {}\"object\"typeof Math.abs\"function\"typeof undefined\"undefined\" Date123456789var now = new Date();now.getFullYear(); //年now.getMonth(); //月 0~11now.getDate(); //日now.getDay(); //星期几now.getHours(); // 时now.getMinutes(); // 分now.getSeconds(); // 秒now.getTime(); //时间戳 转换 12345678now.toDateString()\"Mon Jun 07 2021\"now.toLocaleDateString()\"2021/6/7\"now.toGMTString()\"Mon, 07 Jun 2021 07:36:10 GMT\"now.toLocaleTimeString()\"下午3:36:10\" Json json 是一种轻量级的数据交换格式 简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率 在JavaScript一切皆为对象、任何js支持的类型都可以用Json来表示； 格式： 对象使用{} 数组用[] 所有的键值对都是key value 123456789var person = { name: \"chen\", birth: 1999, age: 23}//对象转化为jsonvar jsonUser = JSON.stringify(person);// json转换成对象var obj = JSON.parse(\"{\\\"name\\\":\\\"chen\\\",\\\"birth\\\":1999,\\\"age\\\":23}\") 6、面向对象之前： 12345678910111213var xiaoming = { name: 'xiaoming', age: 3, run: function() { console.log(this.name + \" run....\"); }}var zhangsan = { name: '张三'}zhangsan.__proto__ = xiaoming;xiaoming.run() class继承 class关键字，ES6引入 定义一个类 12345678class Student{ constructor(name){ this.name = name; } hello(){ alert(\"hello\") }} 继承 12345678910class Student2 extends Student{ constructor(name, grade) { super(name); this.grade = grade; } myGrade(){ alert(\"我是一名小学生\"); }} 7、操作BOM对象JavaScript诞生就是为了能够让他在浏览器中运行 BOM：浏览器对象模型 IE Chrome Safari FireFox window window代表浏览器窗口 1234window.innerHeight708window.innerWidth150 Navigator Navigator，封装了浏览器的信息 12345678navigator.appCodeName\"Mozilla\"navigator.appName\"Netscape\"navigator.userAgent\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"navigator.platform\"Win32\" 一般不会使用navigator，因为会被人为修改 screen 屏幕尺寸 1234screen.width1536screen.height864 location location代表当前页面URL 1234567891011host: \"github.com\"hostname: \"github.com\"href: \"https://github.com/\"origin: \"https://github.com\"pathname: \"/\"port: \"\"protocol: \"https:\"reload: f reload() // 刷新网页//设置新的地址location.assign(\"https://www.baidu.com/\") document document代表当前页面，HTML DOM文档树 history distory代表浏览器的历史记录 12history.back()history.forward() 8、操作DOM对象 核心 浏览器网页就是一个Dom树形结构！ 更新：更新DOM节点 遍历 删除 添加 获得dom节点 1234567&lt;body&gt;&lt;div id=\"father\"&gt; &lt;h1&gt;标题&lt;/h1&gt; &lt;p id=\"p1\"&gt;p1&lt;/p&gt; &lt;p class=\"p2\"&gt;p2&lt;/p&gt;&lt;/div&gt;&lt;/body&gt; 123456789var h1 = document.getElementsByTagName('h1');var p1 = document.getElementById('p1');var p2 = document.getElementsByClassName('p2');var father = document.getElementById('father');var childern = father.children//father.firstchild//father.lastChild 更新节点 操作文本 id1.innerText = ‘213’修改文本的值 id.innerHTML = “ 123“可以解析HTML文本标签 操作css 123id1.style.color = 'yellow'id1.style.fontSize = '20px'id1.style.padding = '2em' 删除节点 删除节点步骤：先获取父节点，在通过父节点删除自己 12345&lt;div id=\"father\"&gt; &lt;h1&gt;标题&lt;/h1&gt; &lt;p id=\"p1\"&gt;p1&lt;/p&gt; &lt;p class=\"p2\"&gt;p2&lt;/p&gt;&lt;/div&gt; 12345&lt;script&gt; var self = document.getElementById('p1'); var father = p1.parentElement; father.removeChild(self)&lt;/script&gt; 插入节点 如果DOM节点是空的，可以通过innerText添加元素，但是存在元素是会覆盖。 123456&lt;p id=\"js\"&gt;JavaScript&lt;/p&gt;&lt;div id=\"list\"&gt; &lt;p id=\"se\"&gt;JavaSE&lt;/p&gt; &lt;p id=\"ee\"&gt;JavaEE&lt;/p&gt; &lt;p id=\"me\"&gt;JavaME&lt;/p&gt;&lt;/div&gt; 123var js = document.getElementById('js');var list = document.getElementById('list');list.appendChild(js); // 追加 创建一个新标签 1234var newP = document.createElement('p');newP.id = 'newP';newP.innerText = 'hello'list.appendChild('newP') 9、操作表单 表单 文本框 text 下拉框 单选框 &lt;&gt; 多选框 隐藏域 密码框 ………… 10、jQueryJavaScript jQuery库，javascript的函数库 获取jQuery https://jquery.com/download/ 1&lt;script src=\"js/jQuery-3.6.0.js\"&gt;&lt;/script&gt; jQuery公式：$(selector).action() 1234567&lt;a href=\"\" id=\"test-jquery\"&gt;点我&lt;/a&gt;&lt;script&gt; //选择器就是css选择器 $('#test-jquery').click(function(){ alert('hello'); })&lt;/script&gt;","categories":[],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://yichenfirst.github.io/tags/javascript/"}]},{"title":"ArrayList源码分析","slug":"java/ArrayList源码分析","date":"2021-03-07T16:00:00.000Z","updated":"2023-07-17T13:41:27.714Z","comments":true,"path":"2021/03/08/java/ArrayList源码分析/","link":"","permalink":"https://yichenfirst.github.io/2021/03/08/java/ArrayList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"构造函数1234567891011121314151617181920212223242526272829303132333435363738// 默认初始容量private static final int DEFAULT_CAPACITY = 10;// 用于空实例的共享数组实例private static final Object[] EMPTY_ELEMENTDATA = {};// 用于默认大小的空实例的共享空数组实例,与EMPTY_ELEMNTDATA区分开,以了解添加第一个元素时要膨胀多少private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};// 使用对象数组存储ArrayList中的元素transient Object[] elementData;// ArrayList中元素数量private int size;// 带初始容量的构造函数, 可以再创建ArrayList时指定集合初始大小public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); }}public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;}// 按照集合的迭代器返回的顺序构造包含指定集合的元素列表public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; }} 以JDK8为例，ArrayList中有三个构造函数， 从无参构造函数中可以看出，ArrayList对象创建后容量是0，之后当添加第一个元素后， 才会真正分配空间，此时容量为10（之后会在add函数中说明）。 ArrayList的扩容机制通过add(E e)向ArrayList对象中添加元素时，首先会调用ensureCapacityInternal(int minCapacity)函数判断是否需要扩容，如果需要扩容则进行扩容，然后将需要元素添加到elementData中。 123456// 向ArrayList中添加元素public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;} 首先通过calculateCapacity(elementData, minCapacity)函数判断出最小扩容量，如果ArrayList对象是由无参构造器创建的，则最小扩容量为DEFAULT_CAPACITY(10)，否则是size + 1。再调用ensureExplicitCapacity(int minCapacity)判断是否需要扩容，当elementData数组长度小于最小扩容量时调用grow(int minCapacity)进行扩容。需要注意的是elementData.length和size的区别，elementData.length表示当前集合扩容前的最大容量，size表示集合中元素数量，所以判断是否需要扩容时，使用elementData.length进行比较，而不是size。 12345678910111213141516171819// 获取最小扩容量,扩容private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));}// 计算所需要的最小容量, 如果ArrayListprivate static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity;}//判断是否需要扩容private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);} 实际实现扩容的方法是grow(int minCapacity)，通常情况会扩大到原先容量的1.5倍，当扩大1.5倍还小于最小扩容量时，则按照最小扩容量进行扩容（使用无参构造函数创建的集合对象在添加以第一个元素时会按照最小扩容量进行扩容，即默认容量10）。当扩大到1.5倍后，大于MAX_ARRAY_SIZE时，如果MAX_ARRAY_SIZE大于最小扩容量，则按照MAX_ARRAY_SIZE扩容，否则按照Integer.MAX_VALUE扩容。 需要注意的是ArrayList对象的最大容量是Integer.MAX_VALUE而不是MAX_ARRAY_SIZE(Integer.MAX_VALUE - 8) 之所以要将MAX_ARRAY_SIZE设置为Integer.MAX_VALUE - 8而不是Integer.MAX_VALUE，是因为一些虚拟机栈(VMs)会在数组中保留一些header，分配更大的空间可能会导致OutOfMemoryError。ArrayList容量达到2/3的Integer.MAX_VALUE时，如果按照1.5倍进行扩容，一些VMs会直接报错，而有了MAX_ARRAY_SIZE后，这些VMs在数组长度达到MAX_ARRAY_SIZE之后，再添加元素才会报错，而其他VMs会正常扩容到Integer.MAX_VALUE，这样大大地减少了这些VMs报错的几率，并给这个VMs增加了近1/3的实例存储容量。 1234567891011121314151617181920212223242526private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;// 实现扩容的核心方法private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; // 新容量约等于就容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 当新容量小于最小扩容量,则使用最小扩容量进行扩容 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 新容量大于MAX_ARRAY_SIZE时 // 如果最小扩容量也大于MAX_ARRAY_SIZE,新扩容量为Integer.MAX_VALUE // 否则新扩容量为MAX_ARRAY_SIZE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);}private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;} Arrays.copyOf()在扩容方法glow中使用了Arrays.copyOf()进行数组扩容。代码如下，首先根据长度创建新的对象数组，然后使用System.arraycopy进行数组中元素的拷贝（浅拷贝）。 123456789public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) { @SuppressWarnings(\"unchecked\") T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;} System.arraycopy放是被native修饰，表示此方法是本地方法。 12345678910/** * src：源对象 * srcPos：源数组中的起始位置 * dest：目标数组对象 * destPos：目标数据中的起始位置 * length：要拷贝的数组元素的数量 */public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); ensureCapacity方法大量向ArrayList中插入对象时，ArrayList会频繁扩容以及移动元素，导致效率较低。可以使用ensureCapacity方法在大量插入之前动态修改ArrayList的容量，减少扩容次数，提高效率。 123456789ArrayList&lt;Object&gt; testList = new ArrayList&lt;Object&gt;();final int N = 1000000;long startTime = System.currentTimeMillis();for (int i = 0; i &lt; N; i++) { testList.add(i);}long endTime = System.currentTimeMillis();System.out.println(\"使用ensureCapacity方法前：\"+(endTime - startTime));// 使用ensureCapacity方法前：31 12345678910ArrayList&lt;Object&gt; testList = new ArrayList&lt;Object&gt;();final int N = 1000000;long startTime = System.currentTimeMillis();testList.ensureCapacity(N);for (int i = 0; i &lt; N; i++) { testList.add(i);}long endTime = System.currentTimeMillis();System.out.println(\"使用ensureCapacity方法后：\"+(endTime - startTime));// 使用ensureCapacity方法后：19 浅拷贝与深拷贝ArrayList常见的基本都是浅拷贝。 通过构造函数方法拷贝 1List&lt;Integer&gt; newList = new ArrayList&lt;&gt;(list); addAll方法 12List&lt;Integer&gt; newList = new ArrayList&lt;&gt;();newList.addAll(list); Collection.copy() 123List&lt;Integer&gt; newList = new ArrayList&lt;&gt;();newList.addAll(list);Collections.copy(newList, list) stream方法 1List&lt;Integer&gt; newList = list.stream().collect(toList()); clone()方法 12List&lt;Integer&gt; newList = new ArrayList&lt;&gt;(); newList = (List&lt;Integer&gt;) list.clone(); 可以通过序列化的方法实现深拷贝 1234567891011121314151617181920212223242526public class CloneUtil { @SuppressWarnings(\"unchecked\") public static &lt;T extends Serializable&gt; T clone(T obj){ T cloneObj = null; //写入字节流 try { ByteArrayOutputStream out = new ByteArrayOutputStream(); ObjectOutputStream obs = new ObjectOutputStream(out); obs.writeObject(obj); obs.close(); //分配内存，写入原始对象，生成新对象 ByteArrayInputStream ios = new ByteArrayInputStream(out.toByteArray()); ObjectInputStream ois = new ObjectInputStream(ios); //返回生成的新对象 cloneObj = (T) ois.readObject(); ois.close(); }catch(IOException e){ e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } return cloneObj; }} 使用: 1234for(Shard shard: list) { Shard newShard = CloneUtil.clone(shard); newList.add(newShard);} 常见面试问题问：ArrayList 如何进行扩容？答： ArrayList 在添加元素时，首先检查容量大小判断是否需要扩容，如果需要扩容，会重新定义一个容量为原来的1.5倍的数组，然后将原来的数组复制到新数组，再把指向原数组的地址指向新数组。 问：ArrayList 与LinkedList 的区别？答：①ArrayList 和 LinkedList 都是线程不安全的。 ②ArrayList 其底层用数组实现所以查找元素速度快，但新增和删除由于要在数组中移动元素，所以效率低。而 LinkedList 的查找元素速度慢，但新增和删除速度快。 ③ArrayList需要一份连续的内存空间，LinkedList不需要连续的内存空间。 问：ArrayList 是线程安全的吗？ 答：不是，线程安全的数组容器是Vector，Vector是在所有方法上都加上了 synchronized 进行修饰 问：既然线程不安全，为啥使用频率这么高？ 答： ArrayList 一般用于查询数据，实际情况下并不会对 ArrayList 进行频繁的增删。频繁增删用 LinkedList，线程安全用 Vector。 问：ArrayList 频繁扩容导致性能下降，如何处理？ 答：扩容的原因就是容量不够用了，那我们可以直接通过 ArrayList（int initialCapacity）来指定一个较大的容量，减少扩容次数。当然，数据量大了扩容是不可避免的，我们只能减少扩容次数。","categories":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"}]},{"title":"mybatis笔记","slug":"mybatis/mybatis笔记","date":"2021-03-05T16:00:00.000Z","updated":"2023-07-17T13:41:27.804Z","comments":true,"path":"2021/03/06/mybatis/mybatis笔记/","link":"","permalink":"https://yichenfirst.github.io/2021/03/06/mybatis/mybatis%E7%AC%94%E8%AE%B0/","excerpt":"","text":"框架​ 框架相当于是一个脚手架，内部已经写好了很多代码，我们只要其基础上进行开发就可以提高我们的开发效率。 ​ 框架阶段学习： ①先去学习如何使用框架 ②然后再使用熟练的情况下去猜测内部的原理 ③通过源码去验证自己的猜测。 Mybatis介绍 MyBatis 是一款优秀的持久层框架。 MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。 官网：https://mybatis.org/mybatis-3/zh/# 快速入门①数据准备 1234567891011CREATE DATABASE /*!32312 IF NOT EXISTS*/`mybatis_db` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `mybatis_db`;DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(50) DEFAULT NULL, `age` int(11) DEFAULT NULL, `address` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;insert into `user`(`id`,`username`,`age`,`address`) values (1,'UZI',19,'上海'),(2,'PDD',25,'上海'); ②导入依赖 123456789101112&lt;!--mybatis依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt;&lt;!--mysql驱动--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt; ③编写核心配置 在资源目录下创建：mybatis-config.xml 内容如下： 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"com/sangeng/dao/UserDao.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; ④定义接口及对应的xml映射文件 com.sangeng.dao.UserDao: 1234public interface UserDao { List&lt;User&gt; findAll();} 资源目录下：com/sangeng/dao/UserDao.xml 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.sangeng.dao.UserDao\"&gt; &lt;select id=\"findAll\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;/select&gt;&lt;/mapper&gt; ⑤编写测试类 获取SqlSession,通过SqlSession获取UserDao调用对应的方法 123456789101112131415public static void main(String[] args) throws IOException { //定义mybatis配置文件的路径 String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //获取Sqlsession对象 SqlSession sqlSession = sqlSessionFactory.openSession(); //获取UserDao实现类对象 UserDao userDao = sqlSession.getMapper(UserDao.class); //调用方法测试 List&lt;User&gt; userList = userDao.findAll(); System.out.println(userList); //释放资源 sqlSession.close();} 高效编程1 配置代码模板 2 Mybatis插件​ 下载安装Free Mybatis plugin，安装完后重启IDEA 参数获取1 一个参数1.1 基本参数​ 我们可以使用#{}直接来取值，写任意名字都可以获取到参数。但是一般用方法的参数名来取。 例如： 接口中方法定义如下 1User findById(Integer id); xml中内容如下: 1&lt;select id=\"findById\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id}&lt;/select&gt; 1.2 POJO​ 我们可以使用POJO中的属性名来获取对应的值。 例如： 接口中方法定义如下 1User findByUser(User user); xml中内容如下： 123&lt;select id=\"findByUser\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id} and username = #{username} and age = #{age} and address = #{address}&lt;/select&gt; 1.3 Map​ 我们可以使用map中的key来获取对应的值。 例如： 接口中方法定义如下 1User findByMap(Map map); xml中内容如下： 123&lt;select id=\"findByMap\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id} and username = #{username} and age = #{age} and address = #{address}&lt;/select&gt; 方法调用： 123456Map map = new HashMap();map.put(\"id\",2);map.put(\"username\",\"PDD\");map.put(\"age\",25);map.put(\"address\",\"上海\");userDao.findByMap(map); 2 多个参数​ Mybatis会把多个参数放入一个Map集合中，默认的key是argx和paramx这种格式。 例如： 接口中方法定义如下 1User findByCondition(Integer id,String username); 最终map中的键值对如下： 1{arg1=PDD, arg0=2, param1=2, param2=PDD} ​ 我们虽然可以使用对应的默认key来获取值，但是这种方式可读性不好。我们一般在方法参数前使用@Param来设置参数名。 例如： 接口中方法定义 1User findByCondition(@Param(\"id\") Integer id,@Param(\"username\") String username); 最终map中的键值对如下： 1{id=2, param1=2, username=PDD, param2=PDD} 所以我们就可以使用如下方式来获取参数 123&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id} and username = #{username}&lt;/select&gt; 3 总结​ 建议如果只有一个参数的时候不用做什么特殊处理。如果是有多个参数的情况下一定要加上@Param来设置参数名。 核心类1 SqlSessionFactory​ SqlSessionFactory是一个SqlSession的工厂类。主要用来获取SqlSession对象。、 成员方法如下： 123SqlSession openSession();//获取SqlSession对象，传入的参数代表创建的SqlSession是否自动提交SqlSession openSession(boolean autoCommit); 2 SqlSession​ SqlSession 提供了在数据库执行 SQL 命令所需的所有方法 。它还提供了事务的相关操作。 成员方法如下： 1234T getMapper(Class&lt;T&gt; type);//获取mapper对象void commit();//提交事务void rollback();//回滚事务void close();//释放资源 Mybatis实现增删改查1 新增①接口中增加相关方法 1void insertUser(User user); ②映射文件UserDao.xml增加响应的标签 123&lt;insert id=\"insertUser\"&gt; insert into user values(null,#{username},#{age},#{address})&lt;/insert&gt; 注意：要记得提交事务。 2 删除①接口中增加相关方法 1void deleteById(Integer id); ②映射文件UserDao.xml增加响应的标签 123&lt;delete id=\"deleteById\"&gt; delete from user where id = #{id}&lt;/delete&gt; 注意：要记得提交事务。 3 修改①接口中增加相关方法 1void updateUser(User user); ②映射文件UserDao.xml增加响应的标签 1234&lt;!--更新用户--&gt;&lt;update id=\"updateUser\"&gt; UPDATE USER SET age = #{age} , username = #{username},address = #{address} WHERE id = #{id}&lt;/update&gt; 注意：要记得提交事务。 4 根据id查询①接口中增加相关方法 1User findById(Integer id); ②映射文件UserDao.xml增加响应的标签 123&lt;select id=\"findById\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id} &lt;/select&gt; 5 查询所有①接口中增加相关方法 1List&lt;User&gt; findAll(); ②映射文件UserDao.xml增加响应的标签 123&lt;select id=\"findAll\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user&lt;/select&gt; 配置文件详解1 properties​ 可以使用properties读取properties配置文件。使用其中的resource属性来设置配置文件的路径。 ​ 然后使用${key}来获取配置文件中的值 例如： 在resources目录下有jdbc.properties文件，内容如下： 1234jdbc.url=jdbc:mysql://localhost:3306/mybatis_dbjdbc.driver=com.mysql.jdbc.Driverjdbc.username=rootjdbc.password=root 在mybatis-config.xml中： 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!--设置配置文件所在的路径--&gt; &lt;properties resource=\"jdbc.properties\"&gt;&lt;/properties&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;!--获取配置文件中配置的对应的值来设置连接相关参数--&gt; &lt;property name=\"driver\" value=\"${jdbc.driver}\"/&gt; &lt;property name=\"url\" value=\"${jdbc.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.username}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 2 settings​ 可以使用该标签来设置进行一些设置 例如： 1234&lt;settings&gt; &lt;!--开启自动驼峰命名映射--&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/&gt;&lt;/settings&gt; 具体的设置参考：https://mybatis.org/mybatis-3/zh/configuration.html#settings 3 typeAliases​ 可以用来设置给全类名设置别名，简化书写。一般设置一个包下的类全部具有默认别名。默认别名是类目首字母小写。例如：com.sangeng.pojo.User别名为user 123&lt;typeAliases&gt; &lt;package name=\"com.sangeng.dao\"&gt;&lt;/package&gt; &lt;/typeAliases&gt; 4 environments​ 配置数据库相关的环境，例如事物管理器，连接池相关参数等。 123456789101112131415 &lt;!--设置默认环境--&gt;&lt;environments default=\"development\"&gt; &lt;!--设置该环境的唯一标识--&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;!--获取配置文件中配置的对应的值来设置连接相关参数--&gt; &lt;property name=\"driver\" value=\"${jdbc.driver}\"/&gt; &lt;property name=\"url\" value=\"${jdbc.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.username}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 5 mappers​ 该标签的作用是加载映射的，加载方式有如下几种(主要使用第四种)： ​ ①使用相对于类路径的资源引用，例如： 123456&lt;!-- 使用相对于类路径的资源引用 --&gt;&lt;mappers&gt; &lt;mapper resource=\"org/mybatis/builder/AuthorMapper.xml\"/&gt; &lt;mapper resource=\"org/mybatis/builder/BlogMapper.xml\"/&gt; &lt;mapper resource=\"org/mybatis/builder/PostMapper.xml\"/&gt;&lt;/mappers&gt; ​ ②使用完全限定资源定位符（URL），例如： 123456&lt;!-- 使用完全限定资源定位符（URL） --&gt;&lt;mappers&gt; &lt;mapper url=\"file:///var/mappers/AuthorMapper.xml\"/&gt; &lt;mapper url=\"file:///var/mappers/BlogMapper.xml\"/&gt; &lt;mapper url=\"file:///var/mappers/PostMapper.xml\"/&gt;&lt;/mappers&gt; ​ ③使用映射器接口实现类的完全限定类名，例如： 123456&lt;!-- 使用映射器接口实现类的完全限定类名 --&gt;&lt;mappers&gt; &lt;mapper class=\"org.mybatis.builder.AuthorMapper\"/&gt; &lt;mapper class=\"org.mybatis.builder.BlogMapper\"/&gt; &lt;mapper class=\"org.mybatis.builder.PostMapper\"/&gt;&lt;/mappers&gt; ​ ④将包内的映射器接口实现全部注册为映射器，例如： 1234&lt;!-- 定义dao接口所在的包。要求xml文件存放的路径和dao接口的包名要对应 --&gt;&lt;mappers&gt; &lt;package name=\"org.mybatis.builder\"/&gt;&lt;/mappers&gt; ​ 打印日志①log4j配置 在resources目录下创建log4j.properties文件，内容如下： 12345678910111213141516### direct log messages to stdout ###log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target=System.outlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %5p %c{1}:%L - %m%n### direct messages to file mylog.log ###log4j.appender.file=org.apache.log4j.FileAppenderlog4j.appender.file.File=c:/mylog.loglog4j.appender.file.layout=org.apache.log4j.PatternLayoutlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p %c{1}:%L - %m%n### set log levels - for more verbose logging change 'info' to 'debug' ###log4j.rootLogger=debug, stdout ②引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 获取参数时 #{}和${}的区别​ 如果使用#{}.他是预编译的sql可以防止SQL注入攻击​ 如果使用${}他是直接把参数值拿来进行拼接，这样会有SQL注入的危险 如果使用的是#{}来获取参数值日志如下：Preparing: select * from user where id = ? and username = ? and age = ? and address = ?Parameters: 2(Integer), 快乐风男(String), 29(Integer), 北京(String) 如果使用${}来获取参数值日志如下：Preparing: select * from user where id = 2 and username = 快乐风男 and age = 29 and address = 北京 注解开发​ 我们也可以使用注解的形式来进行开发，用注解来替换掉xml。 使用注解来映射简单语句会使代码显得更加简洁，但对于稍微复杂一点的语句，Java 注解不仅力不从心，还会让你本就复杂的 SQL 语句更加混乱不堪。 所以我们在实际企业开发中一般都是使用XML的形式。 ​ 1 步骤①在核心配置文件中配置mapper接口所在的包名 123&lt;mappers&gt; &lt;package name=\"com.sangeng.dao\"&gt;&lt;/package&gt;&lt;/mappers&gt; ②在接口对应方法上使用注解来配置需要执行的sql 123456789101112131415public interface UserDao { @Select(\"select * from user\") List&lt;User&gt; findAll(); @Insert(\"insert into user values(null,#{username},#{age},#{address})\") void insertUser(User user); @Update(\"UPDATE USER SET age = #{age} , username = #{username},address = #{address} WHERE id = #{id}\") void updateUser(User user); @Delete(\"delete from user where id = #{id}\") void deleteById(Integer id);} ③和之前的一样获取Mapper调用方法即可 123456789101112131415public static void main(String[] args) throws IOException { //定义mybatis配置文件的路径 String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //获取Sqlsession对象 SqlSession sqlSession = sqlSessionFactory.openSession(); //获取UserDao实现类对象 UserDao userDao = sqlSession.getMapper(UserDao.class); //调用方法测试 List&lt;User&gt; userList = userDao.findAll(); System.out.println(userList); //释放资源 sqlSession.close();} 动态SQL​ 在实际开发中的SQL语句没有之前的这么简单，很多时候需要根据传入的参数情况动态的生成SQL语句。Mybatis提供了动态SQL相关的标签让我们使用。 1 if​ 可以使用if标签进行条件判断，条件成立才会把if标签中的内容拼接进sql语句中。 例如： 1234567&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user where id = #{id} &lt;if test=\"username!=null\"&gt; and username = #{username} &lt;/if&gt;&lt;/select&gt; 如果参数username为null则执行的sql为：select * from user where id = ? 如果参数username不为null则执行的sql为：select * from user where id = ? and username = ? 注意：在test属性中表示参数的时候不需要写#{}，写了会出问题。 2 trim​ 可以使用该标签动态的添加前缀或后缀，也可以使用该标签动态的消除前缀。 2.1 prefixOverrides属性​ 用来设置需要被清除的前缀,多个值可以用|分隔，注意|前后不要有空格。例如： and|or 例如： 123456&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;trim prefixOverrides=\"and|or\" &gt; and &lt;/trim&gt;&lt;/select&gt; 最终执行的sql为： select * from user 2.2 suffixOverrides属性​ 用来设置需要被清除的后缀,多个值可以用|分隔，注意|前后不要有空格。例如： and|or 例如： 123456&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;trim suffixOverrides=\"like|and\" &gt; where 1=1 like &lt;/trim&gt;&lt;/select&gt; 最终执行的sql为： select * from user 去掉了后缀like 2.3 prefix属性​ 用来设置动态添加的前缀，如果标签中有内容就会添加上设置的前缀 例如： 123456&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;trim prefix=\"where\" &gt; 1=1 &lt;/trim&gt;&lt;/select&gt; 最终执行的sql为：select * from user where 1=1 动态增加了前缀where 2.4 suffix属性​ 用来设置动态添加的后缀，如果标签中有内容就会添加上设置的后缀 123456&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;trim suffix=\"1=1\" &gt; where &lt;/trim&gt;&lt;/select&gt; 最终执行的sql为：select * from user where 1=1 动态增加了后缀1=1 2.5 动态添加前缀where 并且消除前缀and或者or1User findByCondition(@Param(\"id\") Integer id,@Param(\"username\") String username); 1234567891011&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;trim prefix=\"where\" prefixOverrides=\"and|or\" &gt; &lt;if test=\"id!=null\"&gt; id = #{id} &lt;/if&gt; &lt;if test=\"username!=null\"&gt; and username = #{username} &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 调用方法时如果传入的id和username为null则执行的SQL为：select * from user 调用方法时如果传入的id为null，username不为null，则执行的SQL为：select * from user where username = ? 3 where​ where标签等价于： 1&lt;trim prefix=\"where\" prefixOverrides=\"and|or\" &gt;&lt;/trim&gt; ​ 可以使用where标签动态的拼接where并且去除前缀的and或者or。 例如： 1234567891011&lt;select id=\"findByCondition\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;where&gt; &lt;if test=\"id!=null\"&gt; id = #{id} &lt;/if&gt; &lt;if test=\"username!=null\"&gt; and username = #{username} &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 如果id和username都为null，则执行的sql为：**select * from user ** 如果id为null，username不为null，则执行的sql为：**select * from user where username = ? ** 4 set​ set标签等价于 1&lt;trim prefix=\"set\" suffixOverrides=\",\" &gt;&lt;/trim&gt; ​ 可以使用set标签动态的拼接set并且去除后缀的逗号。 例如： 123456789101112131415&lt;update id=\"updateUser\"&gt; UPDATE USER &lt;set&gt; &lt;if test=\"username!=null\"&gt; username = #{username}, &lt;/if&gt; &lt;if test=\"age!=null\"&gt; age = #{age}, &lt;/if&gt; &lt;if test=\"address!=null\"&gt; address = #{address}, &lt;/if&gt; &lt;/set&gt; where id = #{id}&lt;/update&gt; 如果调用方法时传入的User对象的id为2，username不为null，其他属性都为null则最终执行的sql为：UPDATE USER SET username = ? where id = ? 5 foreach​ 可以使用foreach标签遍历集合或者数组类型的参数，获取其中的元素拿来动态的拼接SQL语句。 例如： 方法定义如下 1List&lt;User&gt; findByIds(@Param(\"ids\") Integer[] ids); 如果期望动态的根据实际传入的数组的长度拼接SQL语句。例如传入长度为4个数组最终执行的SQL为： 1select * from User WHERE id in( ? , ? , ? , ?, ? ) 则在xml映射文件中可以使用以下写法 12345678&lt;select id=\"findByIds\" resultType=\"com.sangeng.pojo.User\"&gt; select * from User &lt;where&gt; &lt;foreach collection=\"ids\" open=\"id in(\" close=\")\" item=\"id\" separator=\",\"&gt; #{id} &lt;/foreach&gt; &lt;/where&gt; &lt;/select&gt; collection：表示要遍历的参数。 open:表示遍历开始时拼接的语句 item：表示给当前遍历到的元素的取的名字 separator：表示每遍历完一次拼接的分隔符 close：表示最后一次遍历完拼接的语句 注意：如果方法参数是数组类型，默认的参数名是array，如果方法参数是list集合默认的参数名是list。建议遇到数组或者集合类型的参数统一使用@Param注解进行命名。 6 choose、when、otherwise​ 当我们不想使用所有的条件，而只是想从多个条件中选择一个使用时。可以使用choose系列标签。类似于java中的switch。 例如: 接口中方法定义如下 1List&lt;User&gt; selectChose(User user); 期望： ​ 如果user对象的id不为空时就通过id查询。 ​ 如果id为null,username不为null就通过username查询。 ​ 如果id和username都会null就查询id为3的用户 xml映射文件如下 12345678910111213141516&lt;select id=\"selectChose\" resultType=\"com.sangeng.pojo.User\"&gt; select * from user &lt;where&gt; &lt;choose&gt; &lt;when test=\"id!=null\"&gt; id = #{id} &lt;/when&gt; &lt;when test=\"username!=null\"&gt; username = #{username} &lt;/when&gt; &lt;otherwise&gt; id = 3 &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt; &lt;/select&gt; choose类似于java中的switch when类似于java中的case otherwise类似于java中的dufault ​ 一个choose标签中最多只会有一个when中的判断成立。从上到下去进行判断。如果成立了就把标签体的内容拼接到sql中，并且不会进行其它when的判断和拼接。如果所有的when都不成立则拼接otherwise中的语句。 SQL片段抽取​ 我们在xml映射文件中编写SQL语句的时候可能会遇到重复的SQL片段。这种SQL片段我们可以使用sql标签来进行抽取。然后在需要使用的时候使用include标签进行使用。 例如： 1234&lt;sql id=\"baseSelect\" &gt;id,username,age,address&lt;/sql&gt;&lt;select id=\"findAll\" resultType=\"com.sangeng.pojo.User\"&gt; select &lt;include refid=\"baseSelect\"/&gt; from user&lt;/select&gt; 最终执行的sql为： select id,username,age,address from user 案例环境1 案例数据初始化sql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354CREATE DATABASE /*!32312 IF NOT EXISTS*/`mybatis_db` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `mybatis_db`;DROP TABLE IF EXISTS `orders`;CREATE TABLE `orders` ( `id` int(11) NOT NULL AUTO_INCREMENT, `createtime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '创建时间', `price` int(11) DEFAULT NULL COMMENT '价格', `remark` varchar(100) DEFAULT NULL COMMENT '备注', `user_id` int(11) DEFAULT NULL COMMENT '用户id', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;insert into `orders`(`id`,`createtime`,`price`,`remark`,`user_id`) values (1,'2014-06-26 16:55:43',2000,'无',2),(2,'2021-02-23 16:55:57',3000,'无',3),(3,'2021-02-23 16:56:21',4000,'无',2);DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(100) DEFAULT NULL COMMENT '角色名', `desc` varchar(100) DEFAULT NULL COMMENT '角色描述', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;/*Data for the table `role` */insert into `role`(`id`,`name`,`desc`) values (1,'总经理','一人之下'),(2,'CFO',NULL);/*Table structure for table `user` */DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(50) DEFAULT NULL, `age` int(11) DEFAULT NULL, `address` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=33 DEFAULT CHARSET=utf8;/*Data for the table `user` */insert into `user`(`id`,`username`,`age`,`address`) values (2,'pdd',26,NULL),(3,'UZI',19,'上海11'),(4,'RF',19,NULL);/*Table structure for table `user_role` */DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `user_id` int(11) DEFAULT NULL, `role_id` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*Data for the table `user_role` */insert into `user_role`(`user_id`,`role_id`) values (2,2),(2,1),(3,1); 2 实体类2.1 User.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class User { private Integer id; private String username; private Integer age; private String address; @Override public String toString() { return \"User{\" + \"id=\" + id + \", username='\" + username + '\\'' + \", age=\" + age + \", address='\" + address + '\\'' + '}'; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public String getAddress() { return address; } public void setAddress(String address) { this.address = address; } public User() { } public User(Integer id, String username, Integer age, String address) { this.id = id; this.username = username; this.age = age; this.address = address; }} 2.2 Order.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Order { private Integer id; private Date createtime; private Integer price; private String remark; private Integer userId; @Override public String toString() { return \"Order{\" + \"id=\" + id + \", createtime=\" + createtime + \", price=\" + price + \", remark='\" + remark + '\\'' + \", userId=\" + userId + '}'; } public Order() { } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public Date getCreatetime() { return createtime; } public void setCreatetime(Date createtime) { this.createtime = createtime; } public Integer getPrice() { return price; } public void setPrice(Integer price) { this.price = price; } public String getRemark() { return remark; } public void setRemark(String remark) { this.remark = remark; } public Integer getUserId() { return userId; } public void setUserId(Integer userId) { this.userId = userId; } public Order(Integer id, Date createtime, Integer price, String remark, Integer userId) { this.id = id; this.createtime = createtime; this.price = price; this.remark = remark; this.userId = userId; }} 2.3 Role.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Role { private Integer id; private String name; private String desc; @Override public String toString() { return \"Role{\" + \"id=\" + id + \", name='\" + name + '\\'' + \", desc='\" + desc + '\\'' + '}'; } public Role() { } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getDesc() { return desc; } public void setDesc(String desc) { this.desc = desc; } public Role(Integer id, String name, String desc) { this.id = id; this.name = name; this.desc = desc; }} ResultMap1 基本使用​ 我们可以使用resultMap标签自定义结果集和实体类属性的映射规则。 123456789101112131415161718192021222324252627 &lt;!-- resultMap 用来自定义结果集和实体类的映射 属性： id 相当于这个resultMap的唯一标识 type 用来指定映射到哪个实体类 id标签 用来指定主键列的映射规则 属性： property 要映射的属性名 column 对应的列名 result标签 用来指定普通列的映射规则 属性： property 要映射的属性名 column 对应的列名 --&gt; &lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" &gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"createtime\" property=\"createtime\"&gt;&lt;/result&gt; &lt;result column=\"price\" property=\"price\"&gt;&lt;/result&gt; &lt;result column=\"remark\" property=\"remark\"&gt;&lt;/result&gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt; &lt;/resultMap&gt;&lt;!--使用我们自定义的映射规则--&gt; &lt;select id=\"findAll\" resultMap=\"orderMap\"&gt; SELECT id,createtime,price,remark,user_id FROM ORDERS &lt;/select&gt; 2 自动映射​ 我们定义resultMap时默认情况下自动映射是开启状态的。也就是如果结果集的列名和我们的属性名相同是会自动映射的我们只需要写特殊情况的映射关系即可。 例如： 下面这种写法和上面的写法会有相同的效果，因为其他属性的属性名和结果集的列名都是相同的会自动映射。 1234567 &lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" &gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt; &lt;/resultMap&gt;&lt;!--使用我们自定义的映射规则--&gt; &lt;select id=\"findAll\" resultMap=\"orderMap\"&gt; SELECT id,createtime,price,remark,user_id FROM ORDERS &lt;/select&gt; ​ 如有需要可以选择关闭自动映射可以把resultMap的autoMapping属性设置为false。 例如： 1234567&lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"createtime\" property=\"createtime\"&gt;&lt;/result&gt; &lt;result column=\"price\" property=\"price\"&gt;&lt;/result&gt; &lt;result column=\"remark\" property=\"remark\"&gt;&lt;/result&gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt;&lt;/resultMap&gt; 3 继承映射关系​ 我们可以使用resultMap 的extends属性来指定一个resultMap，从而复用重复的映射关系配置。 例如： 1234567891011 &lt;!--定义个父映射，供其他resultMap继承--&gt;&lt;resultMap id=\"baseOrderMap\" type=\"com.sangeng.pojo.Order\" &gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"createtime\" property=\"createtime\"&gt;&lt;/result&gt; &lt;result column=\"price\" property=\"price\"&gt;&lt;/result&gt; &lt;result column=\"remark\" property=\"remark\"&gt;&lt;/result&gt; &lt;/resultMap&gt;&lt;!--继承baseOrderMap，然后只需要写自己特有的映射关系即可--&gt; &lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\" extends=\"baseOrderMap\"&gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt; &lt;/resultMap&gt; 多表查询​ 有的时候我们需要查询多张表的数据才可以得到我们要的结果。 ​ 我们可以直接写一个多表关联的SQL进行查询。也可以分步进行多次的查询来拿到我们需要的结果。 ​ Mybatis就提供了对应的配置，可以让我们去更方便的进行相应的查询和对应的结果集处理。 1 多表关联查询1.1 一对一关系​ 两个实体之间是一对一的关系。(例如我们需要查询订单，要求还需要下单用户的数据。这里的订单相对于用户是一对一。) 例如： 方法定义如下 12//根据订单id查询订单，要求把下单用户的信息也查询出来Order findById(Integer id); 因为期望Order中还能包含下单用户的数据，所以可以再Order中增加一个属性 1private User user; SQL语句如下 1234567SELECT o.id,o.`createtime`,o.`price`,o.`remark`,o.`user_id`,u.`id` uid,u.`username`,u.`age`,u.`address`FROM orders o,USER uWHERE o.`user_id` = u.`id` AND o.id = 2 我们可以使用如下两种方式封装结果集。 1.1.1 使用ResultMap对所有字段进行映射​ 可以使用ResultMap设置user对象的属性的映射规则。 ①resultMap定义，主要是对user对象的属性设置映射规则 123456789101112131415161718&lt;resultMap id=\"baseOrderMap\" type=\"com.sangeng.pojo.Order\" &gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"createtime\" property=\"createtime\"&gt;&lt;/result&gt; &lt;result column=\"price\" property=\"price\"&gt;&lt;/result&gt; &lt;result column=\"remark\" property=\"remark\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\" extends=\"baseOrderMap\"&gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;!--Order和User关联的映射--&gt; &lt;resultMap id=\"orderUserMap\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\" extends=\"orderMap\"&gt; &lt;result property=\"user.id\" column=\"uid\"&gt;&lt;/result&gt; &lt;result property=\"user.username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"user.age\" column=\"age\"&gt;&lt;/result&gt; &lt;result property=\"user.address\" column=\"address\"&gt;&lt;/result&gt; &lt;/resultMap&gt; ②使用定义好的resultMap 12345678910&lt;!--根据订单id查询订单，要求把下单用户的信息也查询出来--&gt; &lt;select id=\"findById\" resultMap=\"orderUserMap\"&gt; SELECT o.`id`,o.`createtime`,o.`price`,o.`remark`,o.`user_id`,u.`id` uid,u.`username`,u.`age`,u.`address` FROM orders o,`user` u WHERE o.id = #{id} AND o.`user_id`=u.`id` &lt;/select&gt; 1.1.2 使用ResultMap中的association​ 可以使用ResultMap中的子标签association 来设置关联实体类的映射规则. ①定义resultMap 1234567891011121314151617181920&lt;resultMap id=\"baseOrderMap\" type=\"com.sangeng.pojo.Order\" &gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"createtime\" property=\"createtime\"&gt;&lt;/result&gt; &lt;result column=\"price\" property=\"price\"&gt;&lt;/result&gt; &lt;result column=\"remark\" property=\"remark\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;resultMap id=\"orderMap\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\" extends=\"baseOrderMap\"&gt; &lt;result column=\"user_id\" property=\"userId\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;!--Order和User关联的映射（使用association）--&gt; &lt;resultMap id=\"orderUserMapUseAssociation\" type=\"com.sangeng.pojo.Order\" autoMapping=\"false\" extends=\"orderMap\"&gt; &lt;association property=\"user\" javaType=\"com.sangeng.pojo.User\"&gt; &lt;id property=\"id\" column=\"uid\"&gt;&lt;/id&gt; &lt;result property=\"username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"age\" column=\"age\"&gt;&lt;/result&gt; &lt;result property=\"address\" column=\"address\"&gt;&lt;/result&gt; &lt;/association&gt; &lt;/resultMap&gt; ②使用resultMap 12345678910&lt;!--根据订单id查询订单，要求把下单用户的信息也查询出来--&gt; &lt;select id=\"findById\" resultMap=\"orderUserMapUseAssociation\"&gt; SELECT o.`id`,o.`createtime`,o.`price`,o.`remark`,o.`user_id`,u.`id` uid,u.`username`,u.`age`,u.`address` FROM orders o,`user` u WHERE o.id = #{id} AND o.`user_id`=u.`id` &lt;/select&gt; 1.2 一对多关系​ 两个实体之间是一对多的关系。(例如我们需要查询用户，要求还需要该用户所具有的角色信息。这里的用户相对于角色是一对多的。) 例如： 方法定义如下 12//根据id查询用户，并且要求把该用户所具有的角色信息也查询出来User findById(Integer id); 因为期望User中还能包含该用户所具有的角色信息，所以可以在User中增加一个属性 12// 该用户所具有的角色 private List&lt;Role&gt; roles; SQL语句如下 1234567SELECT u.`id`,u.`username`,u.`age`,u.`address`,r.id rid,r.name,r.descFROM USER u,user_role ur,role rWHERE u.id=ur.user_id AND ur.role_id = r.id AND u.id = 2 结果集 我们可以使用如下的方式封装结果集。 1.2.1 使用ResultMap中的collection​ 可以使用ResultMap中的子标签association 来设置关联实体类的映射规则. ①定义ResultMap 12345678910111213141516&lt;!--定义User基本属性映射规则--&gt;&lt;resultMap id=\"userMap\" type=\"com.sangeng.pojo.User\"&gt; &lt;id property=\"id\" column=\"id\"&gt;&lt;/id&gt; &lt;result property=\"username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"age\" column=\"age\"&gt;&lt;/result&gt; &lt;result property=\"address\" column=\"address\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;resultMap id=\"userRoleMap\" type=\"com.sangeng.pojo.User\" extends=\"userMap\"&gt; &lt;collection property=\"roles\" ofType=\"com.sangeng.pojo.Role\" &gt; &lt;id property=\"id\" column=\"rid\"&gt;&lt;/id&gt; &lt;result property=\"name\" column=\"name\"&gt;&lt;/result&gt; &lt;result property=\"desc\" column=\"desc\"&gt;&lt;/result&gt; &lt;/collection&gt; &lt;/resultMap&gt; ②使用ResultMap 12345678910&lt;select id=\"findById\" resultMap=\"userRoleMap\" &gt; SELECT u.`id`,u.`username`,u.`age`,u.`address`,r.id rid,r.name,r.desc FROM USER u,user_role ur,role r WHERE u.id=ur.user_id AND ur.role_id = r.id AND u.id = #{id}&lt;/select&gt; 最终封装完的结果如下： 2 分步查询​ 如果有需要多表查询的需求我们也可以选择用多次查询的方式来查询出我们想要的数据。Mybatis也提供了对应的配置。 ​ 例如我们需要查询用户，要求还需要查询出该用户所具有的角色信息。我们可以选择先查询User表查询用户信息。然后在去查询关联的角色信息。 2.1实现步骤​ 具体步骤如下： ①定义查询方法​ 因为我们要分两步查询: 1.查询User 2.根据用户的id查询Role 所以我们需要定义下面两个方法，并且把对应的标签也先写好 1.查询User 12//根据用户名查询用户，并且要求把该用户所具有的角色信息也查询出来User findByUsername(String username); 1234&lt;!--根据用户名查询用户--&gt;&lt;select id=\"findByUsername\" resultType=\"com.sangeng.pojo.User\"&gt; select id,username,age,address from user where username = #{username}&lt;/select&gt; 2.根据user_id查询Role 12345public interface RoleDao { //根据userId查询所具有的角色 List&lt;Role&gt; findRoleByUserId(Integer userId);} 12345678910&lt;!--根据userId查询所具有的角色--&gt;&lt;select id=\"findRoleByUserId\" resultType=\"com.sangeng.pojo.Role\"&gt; select r.id,r.name,r.desc from role r,user_role ur where ur.role_id = r.id and ur.user_id = #{userId}&lt;/select&gt; ②配置分步查询​ 我们期望的效果是调用findByUsername方法查询出来的结果中就包含角色的信息。所以我们可以设置findByUsername方法的RestltMap，指定分步查询 1234567891011121314151617 &lt;resultMap id=\"userMap\" type=\"com.sangeng.pojo.User\"&gt; &lt;id property=\"id\" column=\"id\"&gt;&lt;/id&gt; &lt;result property=\"username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"age\" column=\"age\"&gt;&lt;/result&gt; &lt;result property=\"address\" column=\"address\"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;!-- select属性：指定用哪个查询来查询当前属性的数据 写法：包名.接口名.方法名 column属性：设置当前结果集中哪列的数据作为select属性指定的查询方法需要参数 --&gt;&lt;resultMap id=\"userRoleMapBySelect\" type=\"com.sangeng.pojo.User\" extends=\"userMap\"&gt; &lt;collection property=\"roles\" ofType=\"com.sangeng.pojo.Role\" select=\"com.sangeng.dao.RoleDao.findRoleByUserId\" column=\"id\"&gt; &lt;/collection&gt; &lt;/resultMap&gt; ​ 指定findByUsername使用我们刚刚创建的resultMap 1234&lt;!--根据用户名查询用户--&gt;&lt;select id=\"findByUsername\" resultMap=\"userRoleMapBySelect\"&gt; select id,username,age,address from user where username = #{username}&lt;/select&gt; 2.2 设置按需加载​ 我们可以设置按需加载，这样在我们代码中需要用到关联数据的时候才会去查询关联数据。 ​ 有两种方式可以配置分别是全局配置和局部配置 局部配置 设置fetchType属性为lazy 1234567&lt;resultMap id=\"userRoleMapBySelect\" type=\"com.sangeng.pojo.User\" extends=\"userMap\"&gt; &lt;collection property=\"roles\" ofType=\"com.sangeng.pojo.Role\" select=\"com.sangeng.dao.RoleDao.findRoleByUserId\" column=\"id\" fetchType=\"lazy\"&gt; &lt;/collection&gt; &lt;/resultMap&gt; 全局配置 设置lazyLoadingEnabled为true 123&lt;settings&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 分页查询-PageHelper​ 我们可以使用PageHelper非常方便的帮我们实现分页查询的需求。不需要自己在SQL中拼接SQL相关参数，并且能非常方便的获取的总页数总条数等分页相关数据。 1 实现步骤①定义方法查询方法以及生成对应标签1List&lt;User&gt; findAll(); 123&lt;select id=\"findAll\" resultType=\"com.sangeng.pojo.User\"&gt; select id,username,age,address from user&lt;/select&gt; ② 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt;&lt;/dependency&gt; ③ 配置Mybatis核心配置文件使用分页插件1234567&lt;plugins&gt; &lt;!-- 注意：分页助手的插件 配置在通用馆mapper之前 --&gt; &lt;plugin interceptor=\"com.github.pagehelper.PageHelper\"&gt; &lt;!-- 指定方言 --&gt; &lt;property name=\"dialect\" value=\"mysql\"/&gt; &lt;/plugin&gt; &lt;/plugins&gt; ④ 开始分页查询我们只需要在使用查询方法前设置分页参数即可 123456 //设置分页参数UserDao userDao = session.getMapper(UserDao.class); //设置分页查询参数 PageHelper.startPage(1,1); List&lt;User&gt; users = userDao.findAll(); System.out.println(users.get(0)); 如果需要获取总页数总条数等分页相关数据，只需要创建一个PageInfo对象，把刚刚查询出的返回值做为构造方法参数传入。然后使用pageInfo对象获取即可。 12345PageInfo&lt;User&gt; pageInfo = new PageInfo&lt;User&gt;(users); System.out.println(\"总条数：\"+pageInfo.getTotal());System.out.println(\"总页数：\"+pageInfo.getPages());System.out.println(\"当前页：\"+pageInfo.getPageNum());System.out.println(\"每页显示长度：\"+pageInfo.getPageSize()); 2 一对多多表查询分页问题​ 我们在进行一对多的多表查询时，如果使用了PageHelper进行分页。会出现关联数据不全的情况。我们可以使用分步查询的方式解决该问题。 Mybatis缓存​ Mybatis的缓存其实就是把之前查到的数据存入内存（map）,下次如果还是查相同的东西，就可以直接从缓存中取，从而提高效率。 ​ Mybatis有一级缓存和二级缓存之分，一级缓存（默认开启）是sqlsession级别的缓存。二级缓存相当于mapper级别的缓存。 1 一级缓存几种不会使用一级缓存的情况 1.调用相同方法但是传入的参数不同 2.调用相同方法参数也相同，但是使用的是另外一个SqlSession 3.如果查询完后，对同一个表进行了增，删改的操作，都会清空这sqlSession上的缓存 4.如果手动调用SqlSession的clearCache方法清除缓存了，后面也使用不了缓存 2 二级缓存​ 注意：只在sqlsession调用了close或者commit后的数据才会进入二级缓存。 2.1 开启二级缓存①全局开启 在Mybatis核心配置文件中配置 123&lt;settings&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; ②局部开启 在要开启二级缓存的mapper映射文件中设置 cache标签 12345&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.sangeng.dao.RoleDao\"&gt; &lt;cache&gt;&lt;/cache&gt;&lt;/mapper&gt; 2.2 使用建议​ 二级缓存在实际开发中基本不会使用。","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://yichenfirst.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://yichenfirst.github.io/tags/mybatis/"}]}],"categories":[{"name":"分布式","slug":"分布式","permalink":"https://yichenfirst.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/categories/java/"},{"name":"go","slug":"go","permalink":"https://yichenfirst.github.io/categories/go/"},{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/categories/redis/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://yichenfirst.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/categories/spring/"},{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/categories/%E9%9D%A2%E8%AF%95/"},{"name":"docker","slug":"docker","permalink":"https://yichenfirst.github.io/categories/docker/"},{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/categories/mysql/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://yichenfirst.github.io/categories/ElasticSearch/"},{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/categories/vue/"},{"name":"mybatis","slug":"mybatis","permalink":"https://yichenfirst.github.io/categories/mybatis/"}],"tags":[{"name":"java","slug":"java","permalink":"https://yichenfirst.github.io/tags/java/"},{"name":"go","slug":"go","permalink":"https://yichenfirst.github.io/tags/go/"},{"name":"redis","slug":"redis","permalink":"https://yichenfirst.github.io/tags/redis/"},{"name":"tcp","slug":"tcp","permalink":"https://yichenfirst.github.io/tags/tcp/"},{"name":"操作系统","slug":"操作系统","permalink":"https://yichenfirst.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"spring","slug":"spring","permalink":"https://yichenfirst.github.io/tags/spring/"},{"name":"总结","slug":"总结","permalink":"https://yichenfirst.github.io/tags/%E6%80%BB%E7%BB%93/"},{"name":"面试","slug":"面试","permalink":"https://yichenfirst.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"docker","slug":"docker","permalink":"https://yichenfirst.github.io/tags/docker/"},{"name":"springcloud","slug":"springcloud","permalink":"https://yichenfirst.github.io/tags/springcloud/"},{"name":"mysql","slug":"mysql","permalink":"https://yichenfirst.github.io/tags/mysql/"},{"name":"token","slug":"token","permalink":"https://yichenfirst.github.io/tags/token/"},{"name":"springsecurity","slug":"springsecurity","permalink":"https://yichenfirst.github.io/tags/springsecurity/"},{"name":"aop","slug":"aop","permalink":"https://yichenfirst.github.io/tags/aop/"},{"name":"vue","slug":"vue","permalink":"https://yichenfirst.github.io/tags/vue/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://yichenfirst.github.io/tags/ElasticSearch/"},{"name":"javascript","slug":"javascript","permalink":"https://yichenfirst.github.io/tags/javascript/"},{"name":"mybatis","slug":"mybatis","permalink":"https://yichenfirst.github.io/tags/mybatis/"}]}
